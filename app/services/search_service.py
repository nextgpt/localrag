"""
æœç´¢æœåŠ¡
è´Ÿè´£æ–‡æ¡£çš„è¯­ä¹‰æ£€ç´¢ã€å‘é‡æ£€ç´¢å’Œæ··åˆæ£€ç´¢ï¼Œä½¿ç”¨åˆ†å¸ƒå¼å­˜å‚¨æ¶æ„
"""

import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
import json

from app.core.config import settings
from app.models.responses import ErrorCode
from app.core.exceptions import create_service_exception
from app.services.vector_service import get_vector_service
from app.services.cache_service import get_cache_service

logger = logging.getLogger("rag-anything")


class SearchService:
    """æœç´¢æœåŠ¡"""
    
    def __init__(self):
        self.vector_service = None
        self.cache_service = None
        
    async def _get_services(self):
        """è·å–ä¾èµ–æœåŠ¡"""
        if self.vector_service is None:
            self.vector_service = await get_vector_service()
        if self.cache_service is None:
            self.cache_service = await get_cache_service()
    
    async def _get_query_embedding(self, query: str) -> List[float]:
        """è·å–æŸ¥è¯¢çš„embeddingå‘é‡"""
        try:
            # æ£€æŸ¥APIé…ç½®
            if not settings.EMBEDDING_API_BASE or not settings.EMBEDDING_API_KEY:
                logger.warning("Embedding APIé…ç½®ä¸å®Œæ•´ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°æ–¹æ¡ˆ")
                return await self._get_local_embedding(query)
                
            import httpx
            
            # ç¡®ä¿API_KEYä¸ä¸ºç©º
            api_key = str(settings.EMBEDDING_API_KEY).strip()
            if not api_key or api_key == "None":
                logger.warning("EMBEDDING_API_KEYä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°æ–¹æ¡ˆ")
                return await self._get_local_embedding(query)
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{settings.EMBEDDING_API_BASE}/embeddings",
                    json={
                        "model": settings.EMBEDDING_MODEL_NAME,
                        "input": query
                    },
                    headers={
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json"
                    },
                    timeout=30
                )
                response.raise_for_status()
                
                data = response.json()
                embedding = data["data"][0]["embedding"]
                return embedding
                
        except Exception as e:
            logger.error(f"è·å–æŸ¥è¯¢embeddingå¤±è´¥: {e}")
            # å°è¯•ä½¿ç”¨æœ¬åœ°embeddingä½œä¸ºfallback
            try:
                logger.info("å°è¯•ä½¿ç”¨æœ¬åœ°embeddingä½œä¸ºfallback")
                return await self._get_local_embedding(query)
            except Exception as fallback_error:
                logger.error(f"æœ¬åœ°embeddingä¹Ÿå¤±è´¥: {fallback_error}")
                raise create_service_exception(
                    ErrorCode.EMBEDDING_CONNECTION_ERROR,
                    f"è·å–æŸ¥è¯¢å‘é‡å¤±è´¥: {str(e)}"
                )
    
    async def _get_local_embedding(self, text: str) -> List[float]:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆembedding - fallbackæ–¹æ¡ˆ"""
        try:
            # ä½¿ç”¨é…ç½®ä¸­çš„å‘é‡ç»´åº¦ï¼ˆQwen3-Embedding-8Bï¼š3072ç»´ï¼‰
            import hashlib
            import struct
            from app.core.config import settings
            
            vector_dim = settings.EMBEDDING_DIMENSION
            
            # ä½¿ç”¨æ–‡æœ¬å“ˆå¸Œç”Ÿæˆç¡®å®šæ€§å‘é‡
            text_hash = hashlib.md5(text.encode('utf-8')).digest()
            
            # ç”ŸæˆæŒ‡å®šç»´åº¦çš„å‘é‡
            embedding = []
            for i in range(vector_dim):
                # ä½¿ç”¨å“ˆå¸Œå€¼çš„ä¸åŒéƒ¨åˆ†ç”Ÿæˆæµ®ç‚¹æ•°
                byte_offset = (i * 2) % len(text_hash)
                if byte_offset + 1 < len(text_hash):
                    value = struct.unpack('!h', text_hash[byte_offset:byte_offset+2])[0]
                    # å½’ä¸€åŒ–åˆ°[-1, 1]
                    normalized_value = value / 32768.0
                    embedding.append(normalized_value)
                else:
                    embedding.append(0.0)
            
            # å‘é‡å½’ä¸€åŒ–åˆ°å•ä½é•¿åº¦ï¼ˆæ¨¡é•¿ä¸º1ï¼‰
            magnitude = sum(x*x for x in embedding)**0.5
            if magnitude > 0:
                embedding = [x / magnitude for x in embedding]
            
            # éªŒè¯å½’ä¸€åŒ–åçš„æ¨¡é•¿
            final_magnitude = sum(x*x for x in embedding)**0.5
            
            logger.info(f"ä½¿ç”¨æœ¬åœ°embeddingç”Ÿæˆå‘é‡: {len(embedding)}ç»´")
            logger.info(f"å‘é‡å‰5ä¸ªå€¼: {embedding[:5]}")
            logger.info(f"å½’ä¸€åŒ–å‰æ¨¡é•¿: {magnitude:.4f}")
            logger.info(f"å½’ä¸€åŒ–åæ¨¡é•¿: {final_magnitude:.4f}")
            return embedding
            
        except Exception as e:
            logger.error(f"æœ¬åœ°embeddingç”Ÿæˆå¤±è´¥: {e}")
            # å¦‚æœæ‰€æœ‰æ–¹æ¡ˆéƒ½å¤±è´¥ï¼Œè¿”å›é›¶å‘é‡
            from app.core.config import settings
            return [0.0] * settings.EMBEDDING_DIMENSION
    
    async def vector_search(
        self,
        query: str,
        limit: int = 10,
        score_threshold: float = 0.5,  # ğŸ”§ é™ä½é˜ˆå€¼ä»¥è·å¾—æ›´å¤šç›¸å…³ç»“æœ
        file_ids: Optional[List[str]] = None,
        collection_name: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """å‘é‡è¯­ä¹‰æ£€ç´¢"""
        await self._get_services()
        
        try:
            # è·å–æŸ¥è¯¢å‘é‡
            query_vector = await self._get_query_embedding(query)
            
            # æ‰§è¡Œå‘é‡æœç´¢
            search_results = await self.vector_service.search_documents(
                query_vector=query_vector,
                file_ids=file_ids,
                limit=limit,
                score_threshold=score_threshold,
                collection_name=collection_name
            )
            
            # å¢å¼ºæœç´¢ç»“æœï¼Œæ·»åŠ æ–‡ä»¶å…ƒæ•°æ®
            enriched_results = []
            for result in search_results:
                payload = result.get("payload", {})
                file_id = payload.get("file_id")
                
                # è·å–æ–‡ä»¶å…ƒæ•°æ®
                file_metadata = None
                if file_id:
                    file_metadata = await self.cache_service.get_file_metadata(file_id)
                
                enriched_result = {
                    "score": result.get("score", 0.0),
                    "chunk_id": payload.get("chunk_id"),
                    "file_id": file_id,
                    "text": payload.get("text", ""),
                    "chunk_index": payload.get("chunk_index", 0),
                    "source_file": payload.get("source_file"),
                    "block_type": payload.get("block_type"),
                    "file_metadata": {
                        "filename": file_metadata.get("filename") if file_metadata else None,
                        "upload_date": file_metadata.get("upload_date") if file_metadata else None,
                        "file_size": file_metadata.get("file_size") if file_metadata else None,
                        "content_type": file_metadata.get("content_type") if file_metadata else None
                    } if file_metadata else None
                }
                enriched_results.append(enriched_result)
            
            logger.info(f"å‘é‡æ£€ç´¢å®Œæˆ: æŸ¥è¯¢='{query}' æ‰¾åˆ°{len(enriched_results)}ä¸ªç»“æœ")
            return enriched_results
            
        except Exception as e:
            logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {query} - {e}")
            raise create_service_exception(
                ErrorCode.SEARCH_FAILED,
                f"å‘é‡æ£€ç´¢å¤±è´¥: {str(e)}"
            )
    
    async def text_search(
        self,
        query: str,
        limit: int = 10,
        file_ids: Optional[List[str]] = None,
        collection_name: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """æ–‡æœ¬å…³é”®è¯æ£€ç´¢"""
        await self._get_services()
        
        try:
            # ä½¿ç”¨å‘é‡æ•°æ®åº“çš„æ–‡æœ¬æœç´¢åŠŸèƒ½
            search_results = await self.vector_service.search_by_text_filter(
                collection_name=collection_name or settings.QDRANT_COLLECTION_NAME,
                text_field="text",
                search_text=query,
                limit=limit
            )
            
            # è¿‡æ»¤æŒ‡å®šæ–‡ä»¶
            if file_ids:
                search_results = [
                    result for result in search_results
                    if result.get("payload", {}).get("file_id") in file_ids
                ]
            
            # å¢å¼ºæœç´¢ç»“æœ
            enriched_results = []
            for result in search_results[:limit]:
                payload = result.get("payload", {})
                file_id = payload.get("file_id")
                
                # è·å–æ–‡ä»¶å…ƒæ•°æ®
                file_metadata = None
                if file_id:
                    file_metadata = await self.cache_service.get_file_metadata(file_id)
                
                enriched_result = {
                    "score": 1.0,  # æ–‡æœ¬æœç´¢æ²¡æœ‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œä½¿ç”¨1.0
                    "chunk_id": payload.get("chunk_id"),
                    "file_id": file_id,
                    "text": payload.get("text", ""),
                    "chunk_index": payload.get("chunk_index", 0),
                    "source_file": payload.get("source_file"),
                    "block_type": payload.get("block_type"),
                    "file_metadata": {
                        "filename": file_metadata.get("filename") if file_metadata else None,
                        "upload_date": file_metadata.get("upload_date") if file_metadata else None,
                        "file_size": file_metadata.get("file_size") if file_metadata else None,
                        "content_type": file_metadata.get("content_type") if file_metadata else None
                    } if file_metadata else None
                }
                enriched_results.append(enriched_result)
            
            logger.info(f"æ–‡æœ¬æ£€ç´¢å®Œæˆ: æŸ¥è¯¢='{query}' æ‰¾åˆ°{len(enriched_results)}ä¸ªç»“æœ")
            return enriched_results
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬æ£€ç´¢å¤±è´¥: {query} - {e}")
            raise create_service_exception(
                ErrorCode.SEARCH_FAILED,
                f"æ–‡æœ¬æ£€ç´¢å¤±è´¥: {str(e)}"
            )
    
    async def hybrid_search(
        self,
        query: str,
        limit: int = 10,
        vector_weight: float = 0.7,
        text_weight: float = 0.3,
        score_threshold: float = 0.5,
        file_ids: Optional[List[str]] = None,
        collection_name: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """æ··åˆæ£€ç´¢ï¼ˆå‘é‡æ£€ç´¢+æ–‡æœ¬æ£€ç´¢ï¼‰"""
        await self._get_services()
        
        try:
            # å¹¶è¡Œæ‰§è¡Œå‘é‡æ£€ç´¢å’Œæ–‡æœ¬æ£€ç´¢
            import asyncio
            
            vector_task = asyncio.create_task(
                self.vector_search(
                    query=query,
                    limit=limit * 2,  # è·å–æ›´å¤šç»“æœç”¨äºèåˆ
                    score_threshold=score_threshold,
                    file_ids=file_ids,
                    collection_name=collection_name
                )
            )
            
            text_task = asyncio.create_task(
                self.text_search(
                    query=query,
                    limit=limit * 2,
                    file_ids=file_ids,
                    collection_name=collection_name
                )
            )
            
            vector_results, text_results = await asyncio.gather(vector_task, text_task)
            
            # åˆå¹¶ç»“æœå¹¶é‡æ–°è¯„åˆ†
            chunk_scores = {}
            
            # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
            for result in vector_results:
                chunk_id = result.get("chunk_id")
                if chunk_id:
                    chunk_scores[chunk_id] = {
                        "vector_score": result.get("score", 0.0),
                        "text_score": 0.0,
                        "result": result
                    }
            
            # å¤„ç†æ–‡æœ¬æ£€ç´¢ç»“æœ
            for result in text_results:
                chunk_id = result.get("chunk_id")
                if chunk_id:
                    if chunk_id in chunk_scores:
                        chunk_scores[chunk_id]["text_score"] = 1.0
                    else:
                        chunk_scores[chunk_id] = {
                            "vector_score": 0.0,
                            "text_score": 1.0,
                            "result": result
                        }
            
            # è®¡ç®—æ··åˆåˆ†æ•°å¹¶æ’åº
            hybrid_results = []
            for chunk_id, scores in chunk_scores.items():
                hybrid_score = (
                    scores["vector_score"] * vector_weight +
                    scores["text_score"] * text_weight
                )
                
                result = scores["result"].copy()
                result["hybrid_score"] = hybrid_score
                result["vector_score"] = scores["vector_score"]
                result["text_score"] = scores["text_score"]
                
                hybrid_results.append(result)
            
            # æŒ‰æ··åˆåˆ†æ•°æ’åº
            hybrid_results.sort(key=lambda x: x["hybrid_score"], reverse=True)
            
            # è¿”å›å‰Nä¸ªç»“æœ
            final_results = hybrid_results[:limit]
            
            logger.info(f"æ··åˆæ£€ç´¢å®Œæˆ: æŸ¥è¯¢='{query}' æ‰¾åˆ°{len(final_results)}ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"æ··åˆæ£€ç´¢å¤±è´¥: {query} - {e}")
            raise create_service_exception(
                ErrorCode.SEARCH_FAILED,
                f"æ··åˆæ£€ç´¢å¤±è´¥: {str(e)}"
            )
    
    async def generate_answer(
        self,
        query: str,
        search_results: List[Dict[str, Any]],
        max_context_length: int = 4000
    ) -> Dict[str, Any]:
        """åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”"""
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context_texts = []
            sources = []
            total_length = 0
            
            for result in search_results:
                text = result.get("text", "")
                if text and total_length + len(text) <= max_context_length:
                    context_texts.append(text)
                    sources.append({
                        "chunk_id": result.get("chunk_id"),
                        "file_id": result.get("file_id"),
                        "filename": result.get("file_metadata", {}).get("filename"),
                        "score": result.get("score", 0.0)
                    })
                    total_length += len(text)
                
                if total_length >= max_context_length:
                    break
            
            if not context_texts:
                return {
                    "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    "sources": [],
                    "context_used": ""
                }
            
            context = "\n\n".join(context_texts)
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·ç¡®ä¿ç­”æ¡ˆå‡†ç¡®ã€å®Œæ•´ï¼Œå¹¶åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æä¾›ä¸€ä¸ªè¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼š"""
            
            # è°ƒç”¨LLMç”Ÿæˆå›ç­”
            answer = await self._call_llm(prompt)
            
            result = {
                "answer": answer,
                "sources": sources,
                "context_used": context,
                "query": query,
                "generated_at": datetime.now().isoformat()
            }
            
            logger.info(f"ç­”æ¡ˆç”Ÿæˆå®Œæˆ: æŸ¥è¯¢='{query}' ä½¿ç”¨äº†{len(sources)}ä¸ªæ¥æº")
            return result
            
        except Exception as e:
            logger.error(f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {query} - {e}")
            raise create_service_exception(
                ErrorCode.LLM_CONNECTION_ERROR,
                f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}"
            )
    
    async def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLMç”Ÿæˆå›ç­”"""
        try:
            import httpx
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{settings.LLM_API_BASE}/chat/completions",
                    json={
                        "model": settings.LLM_MODEL_NAME,
                        "messages": [
                            {"role": "user", "content": prompt}
                        ],
                        "max_tokens": 1000,
                        "temperature": 0.7
                    },
                    headers={
                        "Authorization": f"Bearer {settings.LLM_API_KEY}",
                        "Content-Type": "application/json"
                    },
                    timeout=60
                )
                response.raise_for_status()
                
                data = response.json()
                answer = data["choices"][0]["message"]["content"]
                return answer.strip()
                
        except Exception as e:
            logger.error(f"è°ƒç”¨LLMå¤±è´¥: {e}")
            raise
    
    async def search_and_answer(
        self,
        query: str,
        search_type: str = "hybrid",
        limit: int = 10,
        score_threshold: float = 0.5,
        file_ids: Optional[List[str]] = None,
        generate_answer: bool = True
    ) -> Dict[str, Any]:
        """æ£€ç´¢å¹¶ç”Ÿæˆå›ç­”"""
        await self._get_services()
        
        try:
            # æ ¹æ®æ£€ç´¢ç±»å‹æ‰§è¡Œæœç´¢
            if search_type == "vector":
                search_results = await self.vector_search(
                    query=query,
                    limit=limit,
                    score_threshold=score_threshold,
                    file_ids=file_ids
                )
            elif search_type == "text":
                search_results = await self.text_search(
                    query=query,
                    limit=limit,
                    file_ids=file_ids
                )
            elif search_type == "hybrid":
                search_results = await self.hybrid_search(
                    query=query,
                    limit=limit,
                    score_threshold=score_threshold,
                    file_ids=file_ids
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢ç±»å‹: {search_type}")
            
            result = {
                "query": query,
                "search_type": search_type,
                "search_results": search_results,
                "search_count": len(search_results),
                "searched_at": datetime.now().isoformat()
            }
            
            # ç”Ÿæˆå›ç­”
            if generate_answer and search_results:
                answer_result = await self.generate_answer(query, search_results)
                result["answer"] = answer_result["answer"]
                result["sources"] = answer_result["sources"]
                result["context_used"] = answer_result["context_used"]
            elif generate_answer:
                result["answer"] = "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
                result["sources"] = []
                result["context_used"] = ""
            
            # ä¿å­˜æœç´¢å†å²åˆ°ç¼“å­˜
            await self._save_search_history(query, result)
            
            return result
            
        except Exception as e:
            logger.error(f"æ£€ç´¢å’Œå›ç­”å¤±è´¥: {query} - {e}")
            raise create_service_exception(
                ErrorCode.SEARCH_FAILED,
                f"æ£€ç´¢å’Œå›ç­”å¤±è´¥: {str(e)}"
            )
    
    async def _save_search_history(self, query: str, result: Dict[str, Any]):
        """ä¿å­˜æœç´¢å†å²"""
        try:
            # ç®€åŒ–ç»“æœç”¨äºå­˜å‚¨
            simplified_result = {
                "query": query,
                "search_type": result.get("search_type"),
                "search_count": result.get("search_count"),
                "searched_at": result.get("searched_at"),
                "has_answer": "answer" in result
            }
            
            # ç”Ÿæˆæœç´¢å†å²key
            search_key = f"search_history:{datetime.now().strftime('%Y%m%d')}"
            
            # æ·»åŠ åˆ°æœç´¢å†å²åˆ—è¡¨
            await self.cache_service.rpush(search_key, simplified_result)
            
            # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆ7å¤©ï¼‰
            await self.cache_service.expire(search_key, 7 * 24 * 3600)
            
        except Exception as e:
            logger.warning(f"ä¿å­˜æœç´¢å†å²å¤±è´¥: {e}")
    
    async def get_search_suggestions(self, query: str, limit: int = 5) -> List[str]:
        """è·å–æœç´¢å»ºè®®"""
        await self._get_services()
        
        try:
            # ç®€å•çš„æœç´¢å»ºè®®å®ç°ï¼ŒåŸºäºå·²æœ‰çš„æ–‡æœ¬å†…å®¹
            # å¯ä»¥åç»­ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•ä¼˜åŒ–
            
            # ä»å‘é‡æ•°æ®åº“ä¸­è·å–ç›¸å…³æ–‡æœ¬ç‰‡æ®µ
            search_results = await self.text_search(
                query=query,
                limit=20
            )
            
            suggestions = set()
            query_words = set(query.lower().split())
            
            for result in search_results:
                text = result.get("text", "")
                words = text.lower().split()
                
                # æå–åŒ…å«æŸ¥è¯¢è¯çš„çŸ­è¯­
                for i, word in enumerate(words):
                    if any(query_word in word for query_word in query_words):
                        # æå–å‰åå‡ ä¸ªè¯ä½œä¸ºå»ºè®®
                        start = max(0, i - 2)
                        end = min(len(words), i + 3)
                        phrase = " ".join(words[start:end])
                        if len(phrase) > len(query) and len(phrase) < 50:
                            suggestions.add(phrase)
                        
                        if len(suggestions) >= limit * 2:
                            break
                
                if len(suggestions) >= limit * 2:
                    break
            
            # è¿”å›æœ€ç›¸å…³çš„å»ºè®®
            suggestion_list = list(suggestions)[:limit]
            
            logger.debug(f"ç”Ÿæˆæœç´¢å»ºè®®: æŸ¥è¯¢='{query}' å»ºè®®æ•°={len(suggestion_list)}")
            return suggestion_list
            
        except Exception as e:
            logger.error(f"è·å–æœç´¢å»ºè®®å¤±è´¥: {query} - {e}")
            return []
    
    async def get_search_statistics(self) -> Dict[str, Any]:
        """è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        await self._get_services()
        
        try:
            # è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡
            collection_info = await self.vector_service.get_collection_info(
                settings.QDRANT_COLLECTION_NAME
            )
            
            # è·å–ä»Šæ—¥æœç´¢å†å²
            today_key = f"search_history:{datetime.now().strftime('%Y%m%d')}"
            today_searches = await self.cache_service.llen(today_key)
            
            # è·å–æ–‡ä»¶ç»Ÿè®¡
            file_keys = []
            cursor = 0
            while True:
                cursor, new_keys = await self.cache_service.redis.scan(
                    cursor, match="file:*", count=100
                )
                file_keys.extend(new_keys)
                if cursor == 0:
                    break
            
            statistics = {
                "vector_database": {
                    "total_points": collection_info.get("points_count", 0) if collection_info else 0,
                    "collection_status": collection_info.get("status") if collection_info else "unknown"
                },
                "search_activity": {
                    "today_searches": today_searches,
                },
                "document_library": {
                    "total_files": len(file_keys),
                },
                "updated_at": datetime.now().isoformat()
            }
            
            return statistics
            
        except Exception as e:
            logger.error(f"è·å–æœç´¢ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                "error": str(e),
                "updated_at": datetime.now().isoformat()
            }
    
    async def search(
        self,
        query: str,
        search_type: str = "hybrid",
        limit: int = 10,
        score_threshold: float = 0.5,  # ğŸ”§ é™ä½é»˜è®¤é˜ˆå€¼
        file_ids: Optional[List[str]] = None,
        collection_name: Optional[str] = None,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """é€šç”¨æœç´¢æ–¹æ³• - æ ¹æ®search_typeè°ƒç”¨ä¸åŒçš„æœç´¢ç­–ç•¥"""
        if search_type == "vector" or search_type == "semantic":
            return await self.vector_search(
                query=query,
                limit=limit,
                score_threshold=score_threshold,
                file_ids=file_ids,
                collection_name=collection_name
            )
        elif search_type == "text":
            return await self.text_search(
                query=query,
                limit=limit,
                file_ids=file_ids,
                collection_name=collection_name
            )
        elif search_type == "hybrid":
            vector_weight = kwargs.get("vector_weight", 0.7)
            semantic_weight = kwargs.get("semantic_weight", 0.3)
            return await self.hybrid_search(
                query=query,
                limit=limit,
                vector_weight=vector_weight,
                text_weight=semantic_weight,  # è½¬æ¢å‚æ•°å
                score_threshold=score_threshold,
                file_ids=file_ids,
                collection_name=collection_name
            )
        else:
            raise create_service_exception(
                ErrorCode.INVALID_REQUEST,
                f"ä¸æ”¯æŒçš„æœç´¢ç±»å‹: {search_type}"
            )
    
    async def semantic_search(
        self,
        query: str,
        limit: int = 10,
        score_threshold: float = 0.5,  # ğŸ”§ é™ä½é»˜è®¤é˜ˆå€¼
        file_ids: Optional[List[str]] = None,
        collection_name: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æ£€ç´¢ - ä½¿ç”¨å‘é‡æœç´¢"""
        return await self.vector_search(
            query=query,
            limit=limit,
            score_threshold=score_threshold,
            file_ids=file_ids,
            collection_name=collection_name
        )
    
    async def search_in_knowledge_base(
        self,
        kb_id: str,
        collection_name: str,
        query: str,
        top_k: int = 10,
        score_threshold: float = 0.5,  # ğŸ”§ é™ä½é»˜è®¤é˜ˆå€¼
        return_images: bool = True,
        return_metadata: bool = True,
        file_types: Optional[List[str]] = None,
        date_range: Optional[Dict[str, str]] = None
    ) -> List[Dict[str, Any]]:
        """
        åœ¨çŸ¥è¯†åº“ä¸­è¿›è¡Œä¸“ç”¨æ£€ç´¢
        
        æ”¯æŒè¿”å›æ–‡æœ¬å’Œå›¾ç‰‡ï¼Œé’ˆå¯¹çŸ¥è¯†åº“ä¼˜åŒ–çš„æ£€ç´¢ç®—æ³•
        """
        await self._get_services()
        
        try:
            # è·å–çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨ï¼ˆç”¨äºè¿‡æ»¤ï¼‰
            from app.services.knowledge_base_service import get_knowledge_base_service
            kb_service = await get_knowledge_base_service()
            kb_file_ids = await kb_service.get_knowledge_base_files(kb_id)
            
            if not kb_file_ids:
                logger.warning(f"çŸ¥è¯†åº“ {kb_id} ä¸­æ²¡æœ‰æ–‡ä»¶")
                return []
            
            # åº”ç”¨æ–‡ä»¶ç±»å‹è¿‡æ»¤
            filtered_file_ids = kb_file_ids
            if file_types:
                filtered_file_ids = []
                for file_id in kb_file_ids:
                    file_metadata = await self.cache_service.get_file_metadata(file_id)
                    if file_metadata:
                        file_ext = file_metadata.get("filename", "").lower().split(".")[-1]
                        if f".{file_ext}" in file_types:
                            filtered_file_ids.append(file_id)
            
            # åº”ç”¨æ—¥æœŸèŒƒå›´è¿‡æ»¤
            if date_range and ("start_date" in date_range or "end_date" in date_range):
                date_filtered_ids = []
                for file_id in filtered_file_ids:
                    file_metadata = await self.cache_service.get_file_metadata(file_id)
                    if file_metadata:
                        upload_date = file_metadata.get("upload_date")
                        if upload_date:
                            # ç®€å•çš„æ—¥æœŸæ¯”è¾ƒï¼ˆè¿™é‡Œå¯ä»¥ä¼˜åŒ–ï¼‰
                            if date_range.get("start_date"):
                                if upload_date >= date_range["start_date"]:
                                    date_filtered_ids.append(file_id)
                            elif date_range.get("end_date"):
                                if upload_date <= date_range["end_date"]:
                                    date_filtered_ids.append(file_id)
                            else:
                                date_filtered_ids.append(file_id)
                filtered_file_ids = date_filtered_ids
            
            # æ‰§è¡Œå‘é‡æ£€ç´¢
            search_results = await self.vector_search(
                query=query,
                limit=top_k,
                score_threshold=score_threshold,
                file_ids=filtered_file_ids,
                collection_name=collection_name
            )
            
            # å¢å¼ºç»“æœ - æ·»åŠ å›¾ç‰‡å’Œå…ƒæ•°æ®
            enhanced_results = []
            for result in search_results:
                enhanced_result = {
                    "score": result.get("score", 0.0),
                    "text": result.get("text", ""),
                    "chunk_id": result.get("chunk_id"),
                    "file_id": result.get("file_id"),
                    "chunk_index": result.get("chunk_index", 0),
                    "block_type": result.get("block_type", "text"),
                    "source_file": result.get("source_file")
                }
                
                # æ·»åŠ æ–‡ä»¶å…ƒæ•°æ®
                if return_metadata and result.get("file_metadata"):
                    enhanced_result["file_metadata"] = result["file_metadata"]
                
                # æŸ¥æ‰¾å’Œæ·»åŠ ç›¸å…³å›¾ç‰‡
                if return_images:
                    images = await self._get_related_images(
                        result.get("file_id"),
                        result.get("chunk_index", 0)
                    )
                    enhanced_result["images"] = images
                
                enhanced_results.append(enhanced_result)
            
            logger.info(f"çŸ¥è¯†åº“ {kb_id} æ£€ç´¢å®Œæˆ: æŸ¥è¯¢='{query}' æ‰¾åˆ°{len(enhanced_results)}ä¸ªç»“æœ")
            return enhanced_results
            
        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {kb_id} - {query} - {e}")
            raise create_service_exception(
                ErrorCode.SEARCH_FAILED,
                f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {str(e)}"
            )
    
    async def _get_related_images(
        self, 
        file_id: str, 
        chunk_index: int = 0
    ) -> List[Dict[str, Any]]:
        """
        è·å–ä¸æ–‡æœ¬å—ç›¸å…³çš„å›¾ç‰‡
        
        ä»MinIOä¸­æŸ¥æ‰¾è§£æç»“æœä¸­çš„å›¾ç‰‡
        """
        try:
            from app.services.storage_service import get_minio_service
            minio_service = await get_minio_service()
            
            # è·å–æ–‡ä»¶çš„è§£æç»“æœè·¯å¾„
            file_metadata = await self.cache_service.get_file_metadata(file_id)
            if not file_metadata:
                return []
            
            # æ„å»ºå›¾ç‰‡è·¯å¾„å‰ç¼€
            image_prefix = f"parsed/{file_id}/"
            
            # åˆ—å‡ºæ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
            image_files = []
            try:
                objects = minio_service.client.list_objects(
                    settings.MINIO_BUCKET_NAME,
                    prefix=image_prefix,
                    recursive=True
                )
                
                for obj in objects:
                    if obj.object_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp')):
                        # ç”Ÿæˆé¢„ç­¾åURLç”¨äºå›¾ç‰‡è®¿é—®
                        image_url = minio_service.client.presigned_get_object(
                            settings.MINIO_BUCKET_NAME,
                            obj.object_name,
                            expires=timedelta(hours=24)  # 24å°æ—¶æœ‰æ•ˆæœŸ
                        )
                        
                        image_info = {
                            "filename": obj.object_name.split("/")[-1],
                            "path": obj.object_name,
                            "url": image_url,
                            "size": obj.size,
                            "last_modified": obj.last_modified.isoformat() if obj.last_modified else None
                        }
                        image_files.append(image_info)
                        
            except Exception as e:
                logger.warning(f"è·å–å›¾ç‰‡åˆ—è¡¨å¤±è´¥: {file_id} - {e}")
            
            # æ ¹æ®chunk_indexè¿›è¡Œç®€å•çš„å›¾ç‰‡å…³è”ï¼ˆå¯ä»¥ä¼˜åŒ–ç®—æ³•ï¼‰
            # è¿™é‡Œè¿”å›å‰3å¼ å›¾ç‰‡ä½œä¸ºç›¸å…³å›¾ç‰‡
            related_images = image_files[:3] if len(image_files) > 3 else image_files
            
            logger.info(f"ä¸ºæ–‡ä»¶ {file_id} æ‰¾åˆ° {len(related_images)} å¼ ç›¸å…³å›¾ç‰‡")
            return related_images
            
        except Exception as e:
            logger.error(f"è·å–ç›¸å…³å›¾ç‰‡å¤±è´¥: {file_id} - {e}")
            return []
    
    async def get_knowledge_base_image_gallery(
        self,
        kb_id: str,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """
        è·å–çŸ¥è¯†åº“çš„å›¾ç‰‡ç”»å»Š
        
        è¿”å›çŸ¥è¯†åº“ä¸­æ‰€æœ‰æ–‡ä»¶çš„å›¾ç‰‡é›†åˆ
        """
        try:
            await self._get_services()
            
            # è·å–çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨
            from app.services.knowledge_base_service import get_knowledge_base_service
            kb_service = await get_knowledge_base_service()
            file_ids = await kb_service.get_knowledge_base_files(kb_id)
            
            all_images = []
            for file_id in file_ids:
                images = await self._get_related_images(file_id)
                for image in images:
                    image["file_id"] = file_id
                    all_images.append(image)
            
            # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„å›¾ç‰‡
            all_images.sort(key=lambda x: x.get("last_modified", ""), reverse=True)
            
            return all_images[:limit]
            
        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†åº“å›¾ç‰‡ç”»å»Šå¤±è´¥: {kb_id} - {e}")
            return []


# å…¨å±€æœç´¢æœåŠ¡å®ä¾‹
search_service = SearchService()


async def get_search_service() -> SearchService:
    """è·å–æœç´¢æœåŠ¡å®ä¾‹"""
    return search_service 