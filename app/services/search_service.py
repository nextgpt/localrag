"""
æœç´¢æœåŠ¡
è´Ÿè´£æ–‡æ¡£çš„è¯­ä¹‰æ£€ç´¢ã€å‘é‡æ£€ç´¢å’Œæ··åˆæ£€ç´¢ï¼Œä½¿ç”¨åˆ†å¸ƒå¼å­˜å‚¨æ¶æ„
"""

import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
import json

from app.core.config import settings
from app.models.responses import ErrorCode
from app.core.exceptions import create_service_exception
from app.services.vector_service import get_vector_service
from app.services.cache_service import get_cache_service

logger = logging.getLogger("rag-anything")


class SearchService:
    """æœç´¢æœåŠ¡"""
    
    def __init__(self):
        self.vector_service = None
        self.cache_service = None
        
    async def _get_services(self):
        """è·å–ä¾èµ–æœåŠ¡"""
        if self.vector_service is None:
            self.vector_service = await get_vector_service()
        if self.cache_service is None:
            self.cache_service = await get_cache_service()
    
    async def _get_query_embedding(self, query: str) -> List[float]:
        """è·å–æŸ¥è¯¢çš„embeddingå‘é‡"""
        try:
            # æ£€æŸ¥APIé…ç½®
            if not settings.EMBEDDING_API_BASE or not settings.EMBEDDING_API_KEY:
                logger.warning("Embedding APIé…ç½®ä¸å®Œæ•´ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°æ–¹æ¡ˆ")
                return await self._get_local_embedding(query)
                
            import httpx
            
            # ç¡®ä¿API_KEYä¸ä¸ºç©º
            api_key = str(settings.EMBEDDING_API_KEY).strip()
            if not api_key or api_key == "None":
                logger.warning("EMBEDDING_API_KEYä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°æ–¹æ¡ˆ")
                return await self._get_local_embedding(query)
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{settings.EMBEDDING_API_BASE}/embeddings",
                    json={
                        "model": settings.EMBEDDING_MODEL_NAME,
                        "input": query
                    },
                    headers={
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json"
                    },
                    timeout=30
                )
                response.raise_for_status()
                
                data = response.json()
                embedding = data["data"][0]["embedding"]
                return embedding
                
        except Exception as e:
            logger.error(f"è·å–æŸ¥è¯¢embeddingå¤±è´¥: {e}")
            # å°è¯•ä½¿ç”¨æœ¬åœ°embeddingä½œä¸ºfallback
            try:
                logger.info("å°è¯•ä½¿ç”¨æœ¬åœ°embeddingä½œä¸ºfallback")
                return await self._get_local_embedding(query)
            except Exception as fallback_error:
                logger.error(f"æœ¬åœ°embeddingä¹Ÿå¤±è´¥: {fallback_error}")
                raise create_service_exception(
                    ErrorCode.EMBEDDING_CONNECTION_ERROR,
                    f"è·å–æŸ¥è¯¢å‘é‡å¤±è´¥: {str(e)}"
                )
    
    async def _get_local_embedding(self, text: str) -> List[float]:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆembedding - fallbackæ–¹æ¡ˆ"""
        try:
            # ä½¿ç”¨é…ç½®ä¸­çš„å‘é‡ç»´åº¦ï¼ˆQwen3-Embedding-8Bï¼š3072ç»´ï¼‰
            import hashlib
            import struct
            from app.core.config import settings
            
            vector_dim = settings.EMBEDDING_DIMENSION
            
            # ä½¿ç”¨æ–‡æœ¬å“ˆå¸Œç”Ÿæˆç¡®å®šæ€§å‘é‡
            text_hash = hashlib.md5(text.encode('utf-8')).digest()
            
            # ç”ŸæˆæŒ‡å®šç»´åº¦çš„å‘é‡
            embedding = []
            for i in range(vector_dim):
                # ä½¿ç”¨å“ˆå¸Œå€¼çš„ä¸åŒéƒ¨åˆ†ç”Ÿæˆæµ®ç‚¹æ•°
                byte_offset = (i * 2) % len(text_hash)
                if byte_offset + 1 < len(text_hash):
                    value = struct.unpack('!h', text_hash[byte_offset:byte_offset+2])[0]
                    # å½’ä¸€åŒ–åˆ°[-1, 1]
                    normalized_value = value / 32768.0
                    embedding.append(normalized_value)
                else:
                    embedding.append(0.0)
            
            # å‘é‡å½’ä¸€åŒ–åˆ°å•ä½é•¿åº¦ï¼ˆæ¨¡é•¿ä¸º1ï¼‰
            magnitude = sum(x*x for x in embedding)**0.5
            if magnitude > 0:
                embedding = [x / magnitude for x in embedding]
            
            # éªŒè¯å½’ä¸€åŒ–åçš„æ¨¡é•¿
            final_magnitude = sum(x*x for x in embedding)**0.5
            
            logger.info(f"ä½¿ç”¨æœ¬åœ°embeddingç”Ÿæˆå‘é‡: {len(embedding)}ç»´")
            logger.info(f"å‘é‡å‰5ä¸ªå€¼: {embedding[:5]}")
            logger.info(f"å½’ä¸€åŒ–å‰æ¨¡é•¿: {magnitude:.4f}")
            logger.info(f"å½’ä¸€åŒ–åæ¨¡é•¿: {final_magnitude:.4f}")
            return embedding
            
        except Exception as e:
            logger.error(f"æœ¬åœ°embeddingç”Ÿæˆå¤±è´¥: {e}")
            # å¦‚æœæ‰€æœ‰æ–¹æ¡ˆéƒ½å¤±è´¥ï¼Œè¿”å›é›¶å‘é‡
            from app.core.config import settings
            return [0.0] * settings.EMBEDDING_DIMENSION
    
    async def vector_search(
        self,
        query: str,
        limit: int = 10,
        score_threshold: float = 0.5,  # ğŸ”§ é™ä½é˜ˆå€¼ä»¥è·å¾—æ›´å¤šç›¸å…³ç»“æœ
        file_ids: Optional[List[str]] = None,
        collection_name: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """å‘é‡è¯­ä¹‰æ£€ç´¢"""
        await self._get_services()
        
        try:
            # è·å–æŸ¥è¯¢å‘é‡
            query_vector = await self._get_query_embedding(query)
            
            # æ‰§è¡Œå‘é‡æœç´¢
            search_results = await self.vector_service.search_documents(
                query_vector=query_vector,
                file_ids=file_ids,
                limit=limit,
                score_threshold=score_threshold,
                collection_name=collection_name
            )
            
            # å¢å¼ºæœç´¢ç»“æœï¼Œæ·»åŠ æ–‡ä»¶å…ƒæ•°æ®
            enriched_results = []
            for result in search_results:
                payload = result.get("payload", {})
                file_id = payload.get("file_id")
                
                # è·å–æ–‡ä»¶å…ƒæ•°æ®
                file_metadata = None
                if file_id:
                    file_metadata = await self.cache_service.get_file_metadata(file_id)
                
                enriched_result = {
                    "score": result.get("score", 0.0),
                    "chunk_id": payload.get("chunk_id"),
                    "file_id": file_id,
                    "text": payload.get("text", ""),
                    "chunk_index": payload.get("chunk_index", 0),
                    "source_file": payload.get("source_file"),
                    "block_type": payload.get("block_type"),
                    "file_metadata": {
                        "filename": file_metadata.get("filename") if file_metadata else None,
                        "upload_date": file_metadata.get("upload_date") if file_metadata else None,
                        "file_size": file_metadata.get("file_size") if file_metadata else None,
                        "content_type": file_metadata.get("content_type") if file_metadata else None
                    } if file_metadata else None
                }
                enriched_results.append(enriched_result)
            
            logger.info(f"å‘é‡æ£€ç´¢å®Œæˆ: æŸ¥è¯¢='{query}' æ‰¾åˆ°{len(enriched_results)}ä¸ªç»“æœ")
            return enriched_results
            
        except Exception as e:
            logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {query} - {e}")
            raise create_service_exception(
                ErrorCode.SEARCH_FAILED,
                f"å‘é‡æ£€ç´¢å¤±è´¥: {str(e)}"
            )
    
    async def text_search(
        self,
        query: str,
        limit: int = 10,
        file_ids: Optional[List[str]] = None,
        collection_name: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """æ–‡æœ¬å…³é”®è¯æ£€ç´¢"""
        await self._get_services()
        
        try:
            # ä½¿ç”¨å‘é‡æ•°æ®åº“çš„æ–‡æœ¬æœç´¢åŠŸèƒ½
            search_results = await self.vector_service.search_by_text_filter(
                collection_name=collection_name or settings.QDRANT_COLLECTION_NAME,
                text_field="text",
                search_text=query,
                limit=limit
            )
            
            # è¿‡æ»¤æŒ‡å®šæ–‡ä»¶
            if file_ids:
                search_results = [
                    result for result in search_results
                    if result.get("payload", {}).get("file_id") in file_ids
                ]
            
            # å¢å¼ºæœç´¢ç»“æœ
            enriched_results = []
            for result in search_results[:limit]:
                payload = result.get("payload", {})
                file_id = payload.get("file_id")
                
                # è·å–æ–‡ä»¶å…ƒæ•°æ®
                file_metadata = None
                if file_id:
                    file_metadata = await self.cache_service.get_file_metadata(file_id)
                
                enriched_result = {
                    "score": 1.0,  # æ–‡æœ¬æœç´¢æ²¡æœ‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œä½¿ç”¨1.0
                    "chunk_id": payload.get("chunk_id"),
                    "file_id": file_id,
                    "text": payload.get("text", ""),
                    "chunk_index": payload.get("chunk_index", 0),
                    "source_file": payload.get("source_file"),
                    "block_type": payload.get("block_type"),
                    "file_metadata": {
                        "filename": file_metadata.get("filename") if file_metadata else None,
                        "upload_date": file_metadata.get("upload_date") if file_metadata else None,
                        "file_size": file_metadata.get("file_size") if file_metadata else None,
                        "content_type": file_metadata.get("content_type") if file_metadata else None
                    } if file_metadata else None
                }
                enriched_results.append(enriched_result)
            
            logger.info(f"æ–‡æœ¬æ£€ç´¢å®Œæˆ: æŸ¥è¯¢='{query}' æ‰¾åˆ°{len(enriched_results)}ä¸ªç»“æœ")
            return enriched_results
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬æ£€ç´¢å¤±è´¥: {query} - {e}")
            raise create_service_exception(
                ErrorCode.SEARCH_FAILED,
                f"æ–‡æœ¬æ£€ç´¢å¤±è´¥: {str(e)}"
            )
    
    async def hybrid_search(
        self,
        query: str,
        limit: int = 10,
        vector_weight: float = 0.7,
        text_weight: float = 0.3,
        score_threshold: float = 0.5,
        file_ids: Optional[List[str]] = None,
        collection_name: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """æ··åˆæ£€ç´¢ï¼ˆå‘é‡æ£€ç´¢+æ–‡æœ¬æ£€ç´¢ï¼‰"""
        await self._get_services()
        
        try:
            # å¹¶è¡Œæ‰§è¡Œå‘é‡æ£€ç´¢å’Œæ–‡æœ¬æ£€ç´¢
            import asyncio
            
            vector_task = asyncio.create_task(
                self.vector_search(
                    query=query,
                    limit=limit * 2,  # è·å–æ›´å¤šç»“æœç”¨äºèåˆ
                    score_threshold=score_threshold,
                    file_ids=file_ids,
                    collection_name=collection_name
                )
            )
            
            text_task = asyncio.create_task(
                self.text_search(
                    query=query,
                    limit=limit * 2,
                    file_ids=file_ids,
                    collection_name=collection_name
                )
            )
            
            vector_results, text_results = await asyncio.gather(vector_task, text_task)
            
            # åˆå¹¶ç»“æœå¹¶é‡æ–°è¯„åˆ†
            chunk_scores = {}
            
            # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
            for result in vector_results:
                chunk_id = result.get("chunk_id")
                if chunk_id:
                    chunk_scores[chunk_id] = {
                        "vector_score": result.get("score", 0.0),
                        "text_score": 0.0,
                        "result": result
                    }
            
            # å¤„ç†æ–‡æœ¬æ£€ç´¢ç»“æœ
            for result in text_results:
                chunk_id = result.get("chunk_id")
                if chunk_id:
                    if chunk_id in chunk_scores:
                        chunk_scores[chunk_id]["text_score"] = 1.0
                    else:
                        chunk_scores[chunk_id] = {
                            "vector_score": 0.0,
                            "text_score": 1.0,
                            "result": result
                        }
            
            # è®¡ç®—æ··åˆåˆ†æ•°å¹¶æ’åº
            hybrid_results = []
            for chunk_id, scores in chunk_scores.items():
                hybrid_score = (
                    scores["vector_score"] * vector_weight +
                    scores["text_score"] * text_weight
                )
                
                result = scores["result"].copy()
                result["hybrid_score"] = hybrid_score
                result["vector_score"] = scores["vector_score"]
                result["text_score"] = scores["text_score"]
                
                hybrid_results.append(result)
            
            # æŒ‰æ··åˆåˆ†æ•°æ’åº
            hybrid_results.sort(key=lambda x: x["hybrid_score"], reverse=True)
            
            # è¿”å›å‰Nä¸ªç»“æœ
            final_results = hybrid_results[:limit]
            
            logger.info(f"æ··åˆæ£€ç´¢å®Œæˆ: æŸ¥è¯¢='{query}' æ‰¾åˆ°{len(final_results)}ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"æ··åˆæ£€ç´¢å¤±è´¥: {query} - {e}")
            raise create_service_exception(
                ErrorCode.SEARCH_FAILED,
                f"æ··åˆæ£€ç´¢å¤±è´¥: {str(e)}"
            )
    
    async def generate_answer(
        self,
        query: str,
        search_results: List[Dict[str, Any]],
        max_context_length: int = 4000
    ) -> Dict[str, Any]:
        """åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”"""
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context_texts = []
            sources = []
            total_length = 0
            
            for result in search_results:
                text = result.get("text", "")
                if text and total_length + len(text) <= max_context_length:
                    context_texts.append(text)
                    sources.append({
                        "chunk_id": result.get("chunk_id"),
                        "file_id": result.get("file_id"),
                        "filename": result.get("file_metadata", {}).get("filename"),
                        "score": result.get("score", 0.0)
                    })
                    total_length += len(text)
                
                if total_length >= max_context_length:
                    break
            
            if not context_texts:
                return {
                    "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    "sources": [],
                    "context_used": ""
                }
            
            context = "\n\n".join(context_texts)
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·ç¡®ä¿ç­”æ¡ˆå‡†ç¡®ã€å®Œæ•´ï¼Œå¹¶åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æä¾›ä¸€ä¸ªè¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼š"""
            
            # è°ƒç”¨LLMç”Ÿæˆå›ç­”
            answer = await self._call_llm(prompt)
            
            result = {
                "answer": answer,
                "sources": sources,
                "context_used": context,
                "query": query,
                "generated_at": datetime.now().isoformat()
            }
            
            logger.info(f"ç­”æ¡ˆç”Ÿæˆå®Œæˆ: æŸ¥è¯¢='{query}' ä½¿ç”¨äº†{len(sources)}ä¸ªæ¥æº")
            return result
            
        except Exception as e:
            logger.error(f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {query} - {e}")
            raise create_service_exception(
                ErrorCode.LLM_CONNECTION_ERROR,
                f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}"
            )
    
    async def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLMç”Ÿæˆå›ç­”"""
        try:
            import httpx
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{settings.LLM_API_BASE}/chat/completions",
                    json={
                        "model": settings.LLM_MODEL_NAME,
                        "messages": [
                            {"role": "user", "content": prompt}
                        ],
                        "max_tokens": 1000,
                        "temperature": 0.7
                    },
                    headers={
                        "Authorization": f"Bearer {settings.LLM_API_KEY}",
                        "Content-Type": "application/json"
                    },
                    timeout=60
                )
                response.raise_for_status()
                
                data = response.json()
                answer = data["choices"][0]["message"]["content"]
                return answer.strip()
                
        except Exception as e:
            logger.error(f"è°ƒç”¨LLMå¤±è´¥: {e}")
            raise
    
    async def search_and_answer(
        self,
        query: str,
        search_type: str = "hybrid",
        limit: int = 10,
        score_threshold: float = 0.5,
        file_ids: Optional[List[str]] = None,
        generate_answer: bool = True
    ) -> Dict[str, Any]:
        """æ£€ç´¢å¹¶ç”Ÿæˆå›ç­”"""
        await self._get_services()
        
        try:
            # æ ¹æ®æ£€ç´¢ç±»å‹æ‰§è¡Œæœç´¢
            if search_type == "vector":
                search_results = await self.vector_search(
                    query=query,
                    limit=limit,
                    score_threshold=score_threshold,
                    file_ids=file_ids
                )
            elif search_type == "text":
                search_results = await self.text_search(
                    query=query,
                    limit=limit,
                    file_ids=file_ids
                )
            elif search_type == "hybrid":
                search_results = await self.hybrid_search(
                    query=query,
                    limit=limit,
                    score_threshold=score_threshold,
                    file_ids=file_ids
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢ç±»å‹: {search_type}")
            
            result = {
                "query": query,
                "search_type": search_type,
                "search_results": search_results,
                "search_count": len(search_results),
                "searched_at": datetime.now().isoformat()
            }
            
            # ç”Ÿæˆå›ç­”
            if generate_answer and search_results:
                answer_result = await self.generate_answer(query, search_results)
                result["answer"] = answer_result["answer"]
                result["sources"] = answer_result["sources"]
                result["context_used"] = answer_result["context_used"]
            elif generate_answer:
                result["answer"] = "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
                result["sources"] = []
                result["context_used"] = ""
            
            # ä¿å­˜æœç´¢å†å²åˆ°ç¼“å­˜
            await self._save_search_history(query, result)
            
            return result
            
        except Exception as e:
            logger.error(f"æ£€ç´¢å’Œå›ç­”å¤±è´¥: {query} - {e}")
            raise create_service_exception(
                ErrorCode.SEARCH_FAILED,
                f"æ£€ç´¢å’Œå›ç­”å¤±è´¥: {str(e)}"
            )
    
    async def _save_search_history(self, query: str, result: Dict[str, Any]):
        """ä¿å­˜æœç´¢å†å²"""
        try:
            # ç®€åŒ–ç»“æœç”¨äºå­˜å‚¨
            simplified_result = {
                "query": query,
                "search_type": result.get("search_type"),
                "search_count": result.get("search_count"),
                "searched_at": result.get("searched_at"),
                "has_answer": "answer" in result
            }
            
            # ç”Ÿæˆæœç´¢å†å²key
            search_key = f"search_history:{datetime.now().strftime('%Y%m%d')}"
            
            # æ·»åŠ åˆ°æœç´¢å†å²åˆ—è¡¨
            await self.cache_service.rpush(search_key, simplified_result)
            
            # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆ7å¤©ï¼‰
            await self.cache_service.expire(search_key, 7 * 24 * 3600)
            
        except Exception as e:
            logger.warning(f"ä¿å­˜æœç´¢å†å²å¤±è´¥: {e}")
    
    async def get_search_suggestions(self, query: str, limit: int = 5) -> List[str]:
        """è·å–æœç´¢å»ºè®®"""
        await self._get_services()
        
        try:
            # ç®€å•çš„æœç´¢å»ºè®®å®ç°ï¼ŒåŸºäºå·²æœ‰çš„æ–‡æœ¬å†…å®¹
            # å¯ä»¥åç»­ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•ä¼˜åŒ–
            
            # ä»å‘é‡æ•°æ®åº“ä¸­è·å–ç›¸å…³æ–‡æœ¬ç‰‡æ®µ
            search_results = await self.text_search(
                query=query,
                limit=20
            )
            
            suggestions = set()
            query_words = set(query.lower().split())
            
            for result in search_results:
                text = result.get("text", "")
                words = text.lower().split()
                
                # æå–åŒ…å«æŸ¥è¯¢è¯çš„çŸ­è¯­
                for i, word in enumerate(words):
                    if any(query_word in word for query_word in query_words):
                        # æå–å‰åå‡ ä¸ªè¯ä½œä¸ºå»ºè®®
                        start = max(0, i - 2)
                        end = min(len(words), i + 3)
                        phrase = " ".join(words[start:end])
                        if len(phrase) > len(query) and len(phrase) < 50:
                            suggestions.add(phrase)
                        
                        if len(suggestions) >= limit * 2:
                            break
                
                if len(suggestions) >= limit * 2:
                    break
            
            # è¿”å›æœ€ç›¸å…³çš„å»ºè®®
            suggestion_list = list(suggestions)[:limit]
            
            logger.debug(f"ç”Ÿæˆæœç´¢å»ºè®®: æŸ¥è¯¢='{query}' å»ºè®®æ•°={len(suggestion_list)}")
            return suggestion_list
            
        except Exception as e:
            logger.error(f"è·å–æœç´¢å»ºè®®å¤±è´¥: {query} - {e}")
            return []
    
    async def get_search_statistics(self) -> Dict[str, Any]:
        """è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        await self._get_services()
        
        try:
            # è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡
            collection_info = await self.vector_service.get_collection_info(
                settings.QDRANT_COLLECTION_NAME
            )
            
            # è·å–ä»Šæ—¥æœç´¢å†å²
            today_key = f"search_history:{datetime.now().strftime('%Y%m%d')}"
            today_searches = await self.cache_service.llen(today_key)
            
            # è·å–æ–‡ä»¶ç»Ÿè®¡
            file_keys = []
            cursor = 0
            while True:
                cursor, new_keys = await self.cache_service.redis.scan(
                    cursor, match="file:*", count=100
                )
                file_keys.extend(new_keys)
                if cursor == 0:
                    break
            
            statistics = {
                "vector_database": {
                    "total_points": collection_info.get("points_count", 0) if collection_info else 0,
                    "collection_status": collection_info.get("status") if collection_info else "unknown"
                },
                "search_activity": {
                    "today_searches": today_searches,
                },
                "document_library": {
                    "total_files": len(file_keys),
                },
                "updated_at": datetime.now().isoformat()
            }
            
            return statistics
            
        except Exception as e:
            logger.error(f"è·å–æœç´¢ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                "error": str(e),
                "updated_at": datetime.now().isoformat()
            }
    
    async def search(
        self,
        query: str,
        search_type: str = "hybrid",
        limit: int = 10,
        score_threshold: float = 0.5,  # ğŸ”§ é™ä½é»˜è®¤é˜ˆå€¼
        file_ids: Optional[List[str]] = None,
        collection_name: Optional[str] = None,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """é€šç”¨æœç´¢æ–¹æ³• - æ ¹æ®search_typeè°ƒç”¨ä¸åŒçš„æœç´¢ç­–ç•¥"""
        if search_type == "vector" or search_type == "semantic":
            return await self.vector_search(
                query=query,
                limit=limit,
                score_threshold=score_threshold,
                file_ids=file_ids,
                collection_name=collection_name
            )
        elif search_type == "text":
            return await self.text_search(
                query=query,
                limit=limit,
                file_ids=file_ids,
                collection_name=collection_name
            )
        elif search_type == "hybrid":
            vector_weight = kwargs.get("vector_weight", 0.7)
            semantic_weight = kwargs.get("semantic_weight", 0.3)
            return await self.hybrid_search(
                query=query,
                limit=limit,
                vector_weight=vector_weight,
                text_weight=semantic_weight,  # è½¬æ¢å‚æ•°å
                score_threshold=score_threshold,
                file_ids=file_ids,
                collection_name=collection_name
            )
        else:
            raise create_service_exception(
                ErrorCode.INVALID_REQUEST,
                f"ä¸æ”¯æŒçš„æœç´¢ç±»å‹: {search_type}"
            )
    
    async def semantic_search(
        self,
        query: str,
        limit: int = 10,
        score_threshold: float = 0.5,  # ğŸ”§ é™ä½é»˜è®¤é˜ˆå€¼
        file_ids: Optional[List[str]] = None,
        collection_name: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æ£€ç´¢ - ä½¿ç”¨å‘é‡æœç´¢"""
        return await self.vector_search(
            query=query,
            limit=limit,
            score_threshold=score_threshold,
            file_ids=file_ids,
            collection_name=collection_name
        )
    
    async def search_in_knowledge_base(
        self,
        kb_id: str,
        collection_name: str,
        query: str,
        top_k: int = 10,
        score_threshold: float = 0.5,  # ğŸ”§ é™ä½é»˜è®¤é˜ˆå€¼
        return_images: bool = True,
        return_metadata: bool = True,
        file_types: Optional[List[str]] = None,
        date_range: Optional[Dict[str, str]] = None
    ) -> List[Dict[str, Any]]:
        """
        åœ¨çŸ¥è¯†åº“ä¸­è¿›è¡Œä¸“ç”¨æ£€ç´¢
        
        æ”¯æŒè¿”å›æ–‡æœ¬å’Œå›¾ç‰‡ï¼Œé’ˆå¯¹çŸ¥è¯†åº“ä¼˜åŒ–çš„æ£€ç´¢ç®—æ³•
        """
        await self._get_services()
        
        try:
            # è·å–çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨ï¼ˆç”¨äºè¿‡æ»¤ï¼‰
            from app.services.knowledge_base_service import get_knowledge_base_service
            kb_service = await get_knowledge_base_service()
            kb_file_ids = await kb_service.get_knowledge_base_files(kb_id)
            
            if not kb_file_ids:
                logger.warning(f"çŸ¥è¯†åº“ {kb_id} ä¸­æ²¡æœ‰æ–‡ä»¶")
                return []
            
            # åº”ç”¨æ–‡ä»¶ç±»å‹è¿‡æ»¤
            filtered_file_ids = kb_file_ids
            if file_types:
                filtered_file_ids = []
                for file_id in kb_file_ids:
                    file_metadata = await self.cache_service.get_file_metadata(file_id)
                    if file_metadata:
                        file_ext = file_metadata.get("filename", "").lower().split(".")[-1]
                        if f".{file_ext}" in file_types:
                            filtered_file_ids.append(file_id)
            
            # åº”ç”¨æ—¥æœŸèŒƒå›´è¿‡æ»¤
            if date_range and ("start_date" in date_range or "end_date" in date_range):
                date_filtered_ids = []
                for file_id in filtered_file_ids:
                    file_metadata = await self.cache_service.get_file_metadata(file_id)
                    if file_metadata:
                        upload_date = file_metadata.get("upload_date")
                        if upload_date:
                            # ç®€å•çš„æ—¥æœŸæ¯”è¾ƒï¼ˆè¿™é‡Œå¯ä»¥ä¼˜åŒ–ï¼‰
                            if date_range.get("start_date"):
                                if upload_date >= date_range["start_date"]:
                                    date_filtered_ids.append(file_id)
                            elif date_range.get("end_date"):
                                if upload_date <= date_range["end_date"]:
                                    date_filtered_ids.append(file_id)
                            else:
                                date_filtered_ids.append(file_id)
                filtered_file_ids = date_filtered_ids
            
            # æ‰§è¡Œå‘é‡æ£€ç´¢
            search_results = await self.vector_search(
                query=query,
                limit=top_k,
                score_threshold=score_threshold,
                file_ids=filtered_file_ids,
                collection_name=collection_name
            )
            
            # å¢å¼ºç»“æœ - æ·»åŠ å›¾ç‰‡å’Œå…ƒæ•°æ®
            enhanced_results = []
            for result in search_results:
                enhanced_result = {
                    "score": result.get("score", 0.0),
                    "text": result.get("text", ""),
                    "chunk_id": result.get("chunk_id"),
                    "file_id": result.get("file_id"),
                    "chunk_index": result.get("chunk_index", 0),
                    "block_type": result.get("block_type", "text"),
                    "source_file": result.get("source_file")
                }
                
                # æ·»åŠ æ–‡ä»¶å…ƒæ•°æ®
                if return_metadata and result.get("file_metadata"):
                    enhanced_result["file_metadata"] = result["file_metadata"]
                
                # æŸ¥æ‰¾å’Œæ·»åŠ ç›¸å…³å›¾ç‰‡
                if return_images:
                    images = await self._get_related_images(
                        result.get("file_id"),
                        result.get("chunk_index", 0)
                    )
                    enhanced_result["images"] = images
                
                enhanced_results.append(enhanced_result)
            
            logger.info(f"çŸ¥è¯†åº“ {kb_id} æ£€ç´¢å®Œæˆ: æŸ¥è¯¢='{query}' æ‰¾åˆ°{len(enhanced_results)}ä¸ªç»“æœ")
            return enhanced_results
            
        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {kb_id} - {query} - {e}")
            raise create_service_exception(
                ErrorCode.SEARCH_FAILED,
                f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {str(e)}"
            )
    
    async def _get_related_images(
        self, 
        file_id: str, 
        chunk_index: int = 0
    ) -> List[Dict[str, Any]]:
        """
        è·å–ä¸æ–‡æœ¬å—ç›¸å…³çš„å›¾ç‰‡
        
        ä»MinIOä¸­æŸ¥æ‰¾è§£æç»“æœä¸­çš„å›¾ç‰‡
        """
        try:
            from app.services.storage_service import get_minio_service
            minio_service = await get_minio_service()
            
            # è·å–æ–‡ä»¶çš„è§£æç»“æœè·¯å¾„
            file_metadata = await self.cache_service.get_file_metadata(file_id)
            if not file_metadata:
                return []
            
            # æ„å»ºå›¾ç‰‡è·¯å¾„å‰ç¼€
            image_prefix = f"parsed/{file_id}/"
            
            # åˆ—å‡ºæ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
            image_files = []
            try:
                objects = minio_service.client.list_objects(
                    settings.MINIO_BUCKET_NAME,
                    prefix=image_prefix,
                    recursive=True
                )
                
                for obj in objects:
                    if obj.object_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp')):
                        # ç”Ÿæˆé¢„ç­¾åURLç”¨äºå›¾ç‰‡è®¿é—®
                        image_url = minio_service.client.presigned_get_object(
                            settings.MINIO_BUCKET_NAME,
                            obj.object_name,
                            expires=timedelta(hours=24)  # 24å°æ—¶æœ‰æ•ˆæœŸ
                        )
                        
                        image_info = {
                            "filename": obj.object_name.split("/")[-1],
                            "path": obj.object_name,
                            "url": image_url,
                            "size": obj.size,
                            "last_modified": obj.last_modified.isoformat() if obj.last_modified else None
                        }
                        image_files.append(image_info)
                        
            except Exception as e:
                logger.warning(f"è·å–å›¾ç‰‡åˆ—è¡¨å¤±è´¥: {file_id} - {e}")
            
            # æ ¹æ®chunk_indexè¿›è¡Œç®€å•çš„å›¾ç‰‡å…³è”ï¼ˆå¯ä»¥ä¼˜åŒ–ç®—æ³•ï¼‰
            # è¿™é‡Œè¿”å›å‰3å¼ å›¾ç‰‡ä½œä¸ºç›¸å…³å›¾ç‰‡
            related_images = image_files[:3] if len(image_files) > 3 else image_files
            
            logger.info(f"ä¸ºæ–‡ä»¶ {file_id} æ‰¾åˆ° {len(related_images)} å¼ ç›¸å…³å›¾ç‰‡")
            return related_images
            
        except Exception as e:
            logger.error(f"è·å–ç›¸å…³å›¾ç‰‡å¤±è´¥: {file_id} - {e}")
            return []
    
    async def get_knowledge_base_image_gallery(
        self,
        kb_id: str,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """
        è·å–çŸ¥è¯†åº“çš„å›¾ç‰‡ç”»å»Š
        
        è¿”å›çŸ¥è¯†åº“ä¸­æ‰€æœ‰æ–‡ä»¶çš„å›¾ç‰‡é›†åˆ
        """
        try:
            await self._get_services()
            
            # è·å–çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨
            from app.services.knowledge_base_service import get_knowledge_base_service
            kb_service = await get_knowledge_base_service()
            file_ids = await kb_service.get_knowledge_base_files(kb_id)
            
            all_images = []
            for file_id in file_ids:
                images = await self._get_related_images(file_id)
                for image in images:
                    image["file_id"] = file_id
                    all_images.append(image)
            
            # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„å›¾ç‰‡
            all_images.sort(key=lambda x: x.get("last_modified", ""), reverse=True)
            
            return all_images[:limit]
            
        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†åº“å›¾ç‰‡ç”»å»Šå¤±è´¥: {kb_id} - {e}")
            return []

    async def search_tender_documents(
        self,
        query: str,
        file_ids: Optional[List[str]] = None,
        analysis_type: str = "general",
        limit: int = 20,
        score_threshold: float = 0.4,  # é™ä½é˜ˆå€¼ï¼Œæé«˜å¬å›
        collection_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """ğŸ¯ æ‹›æ ‡ä¹¦ä¸“ç”¨æœç´¢åˆ†æ
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            file_ids: æ–‡ä»¶IDåˆ—è¡¨
            analysis_type: åˆ†æç±»å‹ (general/project_info/technical_specs/commercial_terms/risks)
            limit: ç»“æœæ•°é‡é™åˆ¶
            score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            collection_name: å‘é‡é›†åˆåç§°
        
        Returns:
            ç»“æ„åŒ–çš„æ‹›æ ‡ä¹¦åˆ†æç»“æœ
        """
        try:
            await self._get_services()
            
            # 1ï¸âƒ£ æŸ¥è¯¢é¢„å¤„ç†å’Œæ‰©å±•
            enhanced_queries = self._expand_tender_query(query, analysis_type)
            
            # 2ï¸âƒ£ å¤šå±‚æ¬¡æ£€ç´¢ç­–ç•¥
            all_results = []
            for enhanced_query in enhanced_queries:
                results = await self.vector_search(
                    query=enhanced_query["query"],
                    file_ids=file_ids,
                    limit=limit * 2,  # å¢å¤§æœç´¢èŒƒå›´
                    score_threshold=score_threshold,
                    collection_name=collection_name
                )
                
                # ä¸ºç»“æœæ·»åŠ æŸ¥è¯¢ç±»å‹æ ‡è®°
                for result in results:
                    result["query_type"] = enhanced_query["type"]
                    result["query_importance"] = enhanced_query["importance"]
                
                all_results.extend(results)
            
            # 3ï¸âƒ£ ç»“æœå»é‡å’Œé‡æ–°æ’åº
            deduped_results = self._deduplicate_tender_results(all_results)
            reranked_results = self._rerank_tender_results(deduped_results, query, analysis_type)
            
            # 4ï¸âƒ£ ç»“æ„åŒ–åˆ†æ
            structured_analysis = await self._analyze_tender_results(
                reranked_results[:limit], 
                query, 
                analysis_type
            )
            
            # 5ï¸âƒ£ ç”Ÿæˆä¸“ä¸šæŠ¥å‘Š
            tender_report = self._generate_tender_report(
                structured_analysis, 
                query, 
                analysis_type
            )
            
            logger.info(f"ğŸ¯ æ‹›æ ‡ä¹¦ä¸“ç”¨æœç´¢å®Œæˆ: æŸ¥è¯¢='{query}' ç±»å‹={analysis_type} æ‰¾åˆ°{len(reranked_results)}ä¸ªç»“æœ")
            
            return {
                "query": query,
                "analysis_type": analysis_type,
                "search_results": reranked_results[:limit],
                "structured_analysis": structured_analysis,
                "tender_report": tender_report,
                "total_results": len(reranked_results),
                "search_strategy": {
                    "enhanced_queries": len(enhanced_queries),
                    "score_threshold": score_threshold,
                    "deduplication": len(all_results) - len(deduped_results)
                }
            }
            
        except Exception as e:
            logger.error(f"æ‹›æ ‡ä¹¦ä¸“ç”¨æœç´¢å¤±è´¥: {query} - {e}")
            raise create_service_exception(
                ErrorCode.SEARCH_FAILED,
                f"æ‹›æ ‡ä¹¦æœç´¢å¤±è´¥: {str(e)}"
            )
    
    def _expand_tender_query(self, query: str, analysis_type: str) -> List[Dict[str, Any]]:
        """ğŸ” æ‹›æ ‡ä¹¦æŸ¥è¯¢æ‰©å±•å’Œä¼˜åŒ–"""
        
        # åŸºç¡€æŸ¥è¯¢
        enhanced_queries = [{
            "query": query,
            "type": "original",
            "importance": 1.0
        }]
        
        # æ ¹æ®åˆ†æç±»å‹æ‰©å±•æŸ¥è¯¢
        if analysis_type == "project_info":
            # é¡¹ç›®ä¿¡æ¯ç›¸å…³æ‰©å±•
            project_expansions = [
                f"{query} é¡¹ç›®æ¦‚å†µ",
                f"{query} å·¥ç¨‹æ¦‚å†µ", 
                f"{query} å»ºè®¾è§„æ¨¡",
                f"é¡¹ç›®æ€§è´¨ {query}",
                f"å»ºè®¾åœ°ç‚¹ {query}"
            ]
            for exp in project_expansions:
                enhanced_queries.append({
                    "query": exp,
                    "type": "project_context",
                    "importance": 0.8
                })
        
        elif analysis_type == "technical_specs":
            # æŠ€æœ¯è§„èŒƒæ‰©å±•
            tech_expansions = [
                f"{query} æŠ€æœ¯è¦æ±‚",
                f"{query} è´¨é‡æ ‡å‡†",
                f"{query} æ–½å·¥å·¥è‰º",
                f"æŠ€æœ¯è§„èŒƒ {query}",
                f"å·¥ç¨‹æ ‡å‡† {query}"
            ]
            for exp in tech_expansions:
                enhanced_queries.append({
                    "query": exp,
                    "type": "technical_context",
                    "importance": 0.9
                })
        
        elif analysis_type == "commercial_terms":
            # å•†åŠ¡æ¡æ¬¾æ‰©å±•
            commercial_expansions = [
                f"{query} æŠ¥ä»·è¦æ±‚",
                f"{query} ä»˜æ¬¾æ¡ä»¶",
                f"{query} åˆåŒæ¡æ¬¾",
                f"å•†åŠ¡è¦æ±‚ {query}",
                f"ä»·æ ¼ {query}"
            ]
            for exp in commercial_expansions:
                enhanced_queries.append({
                    "query": exp,
                    "type": "commercial_context", 
                    "importance": 0.85
                })
        
        elif analysis_type == "risks":
            # é£é™©åˆ†ææ‰©å±•
            risk_expansions = [
                f"{query} é£é™©",
                f"{query} éš¾ç‚¹",
                f"{query} æ³¨æ„äº‹é¡¹",
                f"é£é™©ç‚¹ {query}",
                f"æ½œåœ¨é—®é¢˜ {query}"
            ]
            for exp in risk_expansions:
                enhanced_queries.append({
                    "query": exp,
                    "type": "risk_context",
                    "importance": 0.7
                })
        
        # æ·»åŠ åŒä¹‰è¯æ‰©å±•
        synonym_map = {
            "æ‹›æ ‡äºº": ["å‘åŒ…æ–¹", "å»ºè®¾å•ä½", "ä¸šä¸»æ–¹"],
            "æŠ•æ ‡äºº": ["æ‰¿åŒ…æ–¹", "æ–½å·¥å•ä½", "æŠ•æ ‡æ–¹"],
            "å·¥æœŸ": ["æ–½å·¥å‘¨æœŸ", "å»ºè®¾å‘¨æœŸ", "å®Œå·¥æ—¶é—´"],
            "è´¨é‡": ["å“è´¨", "æ ‡å‡†", "ç­‰çº§"],
            "ææ–™": ["ç‰©æ–™", "å»ºæ", "åŸææ–™"],
            "è®¾å¤‡": ["æœºæ¢°", "å™¨æ¢°", "è£…å¤‡"]
        }
        
        for term, synonyms in synonym_map.items():
            if term in query:
                for synonym in synonyms:
                    syn_query = query.replace(term, synonym)
                    enhanced_queries.append({
                        "query": syn_query,
                        "type": "synonym",
                        "importance": 0.6
                    })
        
        return enhanced_queries
    
    def _deduplicate_tender_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡æ‹›æ ‡ä¹¦æœç´¢ç»“æœ"""
        seen_chunks = set()
        deduped_results = []
        
        for result in results:
            chunk_id = result.get("chunk_id")
            if chunk_id and chunk_id not in seen_chunks:
                seen_chunks.add(chunk_id)
                deduped_results.append(result)
        
        return deduped_results
    
    def _rerank_tender_results(self, results: List[Dict[str, Any]], query: str, analysis_type: str) -> List[Dict[str, Any]]:
        """ğŸ¯ æ‹›æ ‡ä¹¦ç»“æœé‡æ–°æ’åº"""
        
        for result in results:
            original_score = result.get("score", 0.0)
            
            # 1ï¸âƒ£ åŸºäºå—ç±»å‹çš„æƒé‡è°ƒæ•´
            block_type = result.get("block_type", "text")
            type_boost = {
                "key_info_date_info": 0.2,
                "key_info_amount_info": 0.25,
                "key_info_tech_requirement": 0.2,
                "key_info_qualification": 0.15,
                "table": 0.15,
                "section_aligned": 0.1,
                "text": 0.0
            }.get(block_type, 0.0)
            
            # 2ï¸âƒ£ åŸºäºæŸ¥è¯¢ç±»å‹çš„æƒé‡è°ƒæ•´
            query_type = result.get("query_type", "original")
            query_boost = {
                "original": 0.0,
                "project_context": 0.1 if analysis_type == "project_info" else 0.05,
                "technical_context": 0.15 if analysis_type == "technical_specs" else 0.05,
                "commercial_context": 0.1 if analysis_type == "commercial_terms" else 0.05,
                "risk_context": 0.1 if analysis_type == "risks" else 0.05,
                "synonym": -0.05  # åŒä¹‰è¯æŸ¥è¯¢ç•¥å¾®é™æƒ
            }.get(query_type, 0.0)
            
            # 3ï¸âƒ£ åŸºäºé‡è¦æ€§åˆ†æ•°çš„è°ƒæ•´
            importance_score = result.get("tender_info", {}).get("importance_score", 0.5)
            importance_boost = (importance_score - 0.5) * 0.1  # é‡è¦æ€§åˆ†æ•°è½¬æ¢ä¸ºboost
            
            # 4ï¸âƒ£ åŸºäºç»“æ„åŒ–æ•°æ®çš„åŠ æƒ
            has_structured_data = bool(result.get("tender_info", {}).get("structured_data"))
            structured_boost = 0.05 if has_structured_data else 0.0
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            final_score = original_score + type_boost + query_boost + importance_boost + structured_boost
            result["final_score"] = min(1.0, final_score)
            result["score_details"] = {
                "original": original_score,
                "type_boost": type_boost,
                "query_boost": query_boost,
                "importance_boost": importance_boost,
                "structured_boost": structured_boost
            }
        
                # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        results.sort(key=lambda x: x.get("final_score", 0.0), reverse=True)
        return results
    
    async def _analyze_tender_results(self, results: List[Dict[str, Any]], query: str, analysis_type: str) -> Dict[str, Any]:
        """ğŸ”¬ ç»“æ„åŒ–åˆ†ææ‹›æ ‡ä¹¦æœç´¢ç»“æœ"""
        
        analysis = {
            "key_information": self._extract_key_information(results),
            "dates_timeline": self._extract_dates_timeline(results),
            "financial_info": self._extract_financial_info(results),
            "technical_requirements": self._extract_technical_requirements(results),
            "qualification_requirements": self._extract_qualification_requirements(results),
            "risks_and_issues": self._identify_risks_and_issues(results),
            "contradictions": self._detect_contradictions(results),
            "completeness_analysis": self._analyze_completeness(results, analysis_type)
        }
        
        return analysis
    
    def _extract_key_information(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æå–å…³é”®ä¿¡æ¯"""
        key_info = {
            "project_name": [],
            "project_location": [],
            "project_scale": [],
            "construction_period": [],
            "budget": []
        }
        
        for result in results:
            text = result.get("text", "")
            structured_data = result.get("tender_info", {}).get("structured_data", {})
            
            # é¡¹ç›®åç§°
            if any(keyword in text for keyword in ["é¡¹ç›®åç§°", "å·¥ç¨‹åç§°", "å·¥ç¨‹é¡¹ç›®"]):
                key_info["project_name"].append({
                    "text": text[:200],
                    "score": result.get("final_score", 0),
                    "source": result.get("chunk_id")
                })
            
            # å»ºè®¾åœ°ç‚¹
            if any(keyword in text for keyword in ["å»ºè®¾åœ°ç‚¹", "æ–½å·¥åœ°ç‚¹", "å·¥ç¨‹åœ°å€"]):
                key_info["project_location"].append({
                    "text": text[:200],
                    "score": result.get("final_score", 0),
                    "source": result.get("chunk_id")
                })
            
            # å»ºè®¾è§„æ¨¡
            if any(keyword in text for keyword in ["å»ºè®¾è§„æ¨¡", "å·¥ç¨‹è§„æ¨¡", "é¡¹ç›®è§„æ¨¡"]):
                key_info["project_scale"].append({
                    "text": text[:200],
                    "score": result.get("final_score", 0),
                    "source": result.get("chunk_id")
                })
            
            # å·¥æœŸä¿¡æ¯
            if structured_data.get("dates") or any(keyword in text for keyword in ["å·¥æœŸ", "æ–½å·¥å‘¨æœŸ", "å»ºè®¾å‘¨æœŸ"]):
                key_info["construction_period"].append({
                    "text": text[:200],
                    "dates": structured_data.get("dates", []),
                    "score": result.get("final_score", 0),
                    "source": result.get("chunk_id")
                })
            
            # é¢„ç®—ä¿¡æ¯
            if structured_data.get("amounts") or any(keyword in text for keyword in ["é¢„ç®—", "æŠ•èµ„", "é™ä»·"]):
                key_info["budget"].append({
                    "text": text[:200],
                    "amounts": structured_data.get("amounts", []),
                    "score": result.get("final_score", 0),
                    "source": result.get("chunk_id")
                })
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶å»é‡
        for category in key_info:
            key_info[category] = sorted(key_info[category], key=lambda x: x["score"], reverse=True)[:3]
        
        return key_info
    
    def _extract_dates_timeline(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æå–æ—¶é—´çº¿ä¿¡æ¯"""
        timeline = {
            "bidding_deadline": [],
            "opening_time": [],
            "construction_start": [],
            "construction_end": [],
            "milestones": []
        }
        
        for result in results:
            text = result.get("text", "")
            structured_data = result.get("tender_info", {}).get("structured_data", {})
            
            # æˆªæ ‡æ—¶é—´
            if any(keyword in text for keyword in ["æˆªæ ‡æ—¶é—´", "æŠ•æ ‡æˆªæ­¢", "é€’äº¤æˆªæ­¢"]):
                timeline["bidding_deadline"].append({
                    "text": text[:200],
                    "dates": structured_data.get("deadlines", []),
                    "score": result.get("final_score", 0)
                })
            
            # å¼€æ ‡æ—¶é—´  
            if any(keyword in text for keyword in ["å¼€æ ‡æ—¶é—´", "å¼€æ ‡æ—¥æœŸ"]):
                timeline["opening_time"].append({
                    "text": text[:200],
                    "dates": structured_data.get("dates", []),
                    "score": result.get("final_score", 0)
                })
            
            # é‡Œç¨‹ç¢‘èŠ‚ç‚¹
            if any(keyword in text for keyword in ["é‡Œç¨‹ç¢‘", "èŠ‚ç‚¹", "å…³é”®èŠ‚ç‚¹", "é‡è¦èŠ‚ç‚¹"]):
                timeline["milestones"].append({
                    "text": text[:200],
                    "dates": structured_data.get("dates", []),
                    "score": result.get("final_score", 0)
                })
        
        return timeline
    
    def _extract_financial_info(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æå–è´¢åŠ¡ä¿¡æ¯"""
        financial = {
            "budget_limit": [],
            "bid_bond": [],
            "performance_bond": [],
            "payment_terms": []
        }
        
        for result in results:
            text = result.get("text", "")
            structured_data = result.get("tender_info", {}).get("structured_data", {})
            
            # é¢„ç®—é™ä»·
            if any(keyword in text for keyword in ["æŠ•æ ‡é™ä»·", "é¢„ç®—é‡‘é¢", "æ§åˆ¶ä»·"]):
                financial["budget_limit"].append({
                    "text": text[:200],
                    "amounts": structured_data.get("amounts", []),
                    "score": result.get("final_score", 0)
                })
            
            # æŠ•æ ‡ä¿è¯é‡‘
            if any(keyword in text for keyword in ["æŠ•æ ‡ä¿è¯é‡‘", "ä¿è¯é‡‘"]):
                financial["bid_bond"].append({
                    "text": text[:200], 
                    "amounts": structured_data.get("amounts", []),
                    "score": result.get("final_score", 0)
                })
            
            # å±¥çº¦ä¿è¯é‡‘
            if any(keyword in text for keyword in ["å±¥çº¦ä¿è¯é‡‘", "å±¥çº¦ä¿è¯"]):
                financial["performance_bond"].append({
                    "text": text[:200],
                    "amounts": structured_data.get("amounts", []),
                    "score": result.get("final_score", 0)
                })
            
            # ä»˜æ¬¾æ¡ä»¶
            if any(keyword in text for keyword in ["ä»˜æ¬¾æ¡ä»¶", "ä»˜æ¬¾æ–¹å¼", "ç»“ç®—æ–¹å¼"]):
                financial["payment_terms"].append({
                    "text": text[:200],
                    "score": result.get("final_score", 0)
                })
        
        return financial
    
    def _extract_technical_requirements(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æå–æŠ€æœ¯è¦æ±‚"""
        technical = {
            "quality_standards": [],
            "technical_specs": [],
            "materials": [],
            "equipment": [],
            "construction_methods": []
        }
        
        for result in results:
            text = result.get("text", "")
            block_type = result.get("block_type", "")
            
            # æŠ€æœ¯è¦æ±‚ç›¸å…³å†…å®¹
            if block_type == "key_info_tech_requirement" or any(keyword in text for keyword in ["è´¨é‡æ ‡å‡†", "è´¨é‡ç­‰çº§"]):
                technical["quality_standards"].append({
                    "text": text[:300],
                    "score": result.get("final_score", 0)
                })
            
            if any(keyword in text for keyword in ["æŠ€æœ¯è§„èŒƒ", "æŠ€æœ¯æ ‡å‡†", "æŠ€æœ¯è¦æ±‚"]):
                technical["technical_specs"].append({
                    "text": text[:300],
                    "score": result.get("final_score", 0)
                })
            
            if any(keyword in text for keyword in ["ææ–™è¦æ±‚", "ææ–™æ ‡å‡†", "ææ–™è§„æ ¼"]):
                technical["materials"].append({
                    "text": text[:300],
                    "score": result.get("final_score", 0)
                })
            
            if any(keyword in text for keyword in ["è®¾å¤‡è¦æ±‚", "è®¾å¤‡è§„æ ¼", "æœºæ¢°è®¾å¤‡"]):
                technical["equipment"].append({
                    "text": text[:300],
                    "score": result.get("final_score", 0)
                })
            
            if any(keyword in text for keyword in ["æ–½å·¥æ–¹æ³•", "æ–½å·¥å·¥è‰º", "æ–½å·¥æŠ€æœ¯"]):
                technical["construction_methods"].append({
                    "text": text[:300],
                    "score": result.get("final_score", 0)
                })
        
        return technical
    
    def _extract_qualification_requirements(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æå–èµ„æ ¼è¦æ±‚"""
        qualification = {
            "company_qualifications": [],
            "personnel_requirements": [],
            "experience_requirements": [],
            "financial_requirements": []
        }
        
        for result in results:
            text = result.get("text", "")
            block_type = result.get("block_type", "")
            
            if block_type == "key_info_qualification" or any(keyword in text for keyword in ["èµ„è´¨è¦æ±‚", "ä¼ä¸šèµ„è´¨"]):
                qualification["company_qualifications"].append({
                    "text": text[:300],
                    "score": result.get("final_score", 0)
                })
            
            if any(keyword in text for keyword in ["äººå‘˜è¦æ±‚", "é¡¹ç›®ç»ç†", "æŠ€æœ¯è´Ÿè´£äºº"]):
                qualification["personnel_requirements"].append({
                    "text": text[:300],
                    "score": result.get("final_score", 0)
                })
            
            if any(keyword in text for keyword in ["ä¸šç»©è¦æ±‚", "ç±»ä¼¼å·¥ç¨‹", "æ–½å·¥ç»éªŒ"]):
                qualification["experience_requirements"].append({
                    "text": text[:300],
                    "score": result.get("final_score", 0)
                })
            
            if any(keyword in text for keyword in ["æ³¨å†Œèµ„é‡‘", "è´¢åŠ¡çŠ¶å†µ", "èµ„äº§çŠ¶å†µ"]):
                qualification["financial_requirements"].append({
                    "text": text[:300],
                    "score": result.get("final_score", 0)
                })
        
        return qualification
    
    def _identify_risks_and_issues(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯†åˆ«é£é™©å’Œé—®é¢˜ç‚¹"""
        risks = []
        
        risk_keywords = [
            "é£é™©", "éš¾ç‚¹", "æ³¨æ„", "ç‰¹æ®Šè¦æ±‚", "é™åˆ¶", "ç¦æ­¢",
            "ä¸¥ç¦", "å¿…é¡»", "ä¸å¾—", "åº”å½“", "è¿çº¦", "ç½šæ¬¾", "æ‰£åˆ†"
        ]
        
        for result in results:
            text = result.get("text", "")
            risk_score = 0
            
            for keyword in risk_keywords:
                if keyword in text:
                    risk_score += 1
            
            if risk_score > 0:
                risks.append({
                    "text": text[:400],
                    "risk_score": risk_score,
                    "final_score": result.get("final_score", 0),
                    "risk_keywords": [kw for kw in risk_keywords if kw in text]
                })
        
        # æŒ‰é£é™©åˆ†æ•°æ’åº
        risks.sort(key=lambda x: (x["risk_score"], x["final_score"]), reverse=True)
        return risks[:10]  # è¿”å›å‰10ä¸ªé£é™©ç‚¹
    
    def _detect_contradictions(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ£€æµ‹çŸ›ç›¾å’Œä¸ä¸€è‡´"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„çŸ›ç›¾æ£€æµ‹ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥æ›´å¤æ‚
        contradictions = []
        
        # æ£€æŸ¥æ—¥æœŸçŸ›ç›¾
        all_dates = []
        for result in results:
            structured_data = result.get("tender_info", {}).get("structured_data", {})
            dates = structured_data.get("dates", [])
            for date in dates:
                all_dates.append({
                    "date": date,
                    "source": result.get("chunk_id"),
                    "text": result.get("text", "")[:100]
                })
        
        # æ£€æŸ¥é‡‘é¢çŸ›ç›¾
        all_amounts = []
        for result in results:
            structured_data = result.get("tender_info", {}).get("structured_data", {})
            amounts = structured_data.get("amounts", [])
            for amount in amounts:
                all_amounts.append({
                    "amount": amount,
                    "source": result.get("chunk_id"),
                    "text": result.get("text", "")[:100]
                })
        
        # æ£€æµ‹é€»è¾‘å¯ä»¥åœ¨è¿™é‡Œæ‰©å±•
        if len(set([d["date"] for d in all_dates])) != len(all_dates):
            contradictions.append({
                "type": "date_inconsistency",
                "description": "å‘ç°é‡å¤æˆ–ç›¸äº’çŸ›ç›¾çš„æ—¥æœŸä¿¡æ¯",
                "details": all_dates
            })
        
        return contradictions
    
    def _analyze_completeness(self, results: List[Dict[str, Any]], analysis_type: str) -> Dict[str, Any]:
        """åˆ†æä¿¡æ¯å®Œæ•´æ€§"""
        
        # æ ¹æ®åˆ†æç±»å‹å®šä¹‰å¿…éœ€ä¿¡æ¯
        required_info = {
            "project_info": ["é¡¹ç›®åç§°", "å»ºè®¾åœ°ç‚¹", "å»ºè®¾è§„æ¨¡", "é¡¹ç›®æ€§è´¨"],
            "technical_specs": ["æŠ€æœ¯æ ‡å‡†", "è´¨é‡è¦æ±‚", "ææ–™è§„æ ¼", "æ–½å·¥æ–¹æ³•"],
            "commercial_terms": ["é¢„ç®—é™ä»·", "ä»˜æ¬¾æ¡ä»¶", "ä¿è¯é‡‘", "åˆåŒæ¡æ¬¾"],
            "risks": ["é£é™©æç¤º", "æ³¨æ„äº‹é¡¹", "ç‰¹æ®Šè¦æ±‚", "è¿çº¦æ¡æ¬¾"]
        }
        
        found_info = set()
        missing_info = []
        
        # æ£€æŸ¥å·²æ‰¾åˆ°çš„ä¿¡æ¯
        for result in results:
            text = result.get("text", "")
            for category, keywords in required_info.items():
                if analysis_type == "general" or analysis_type == category:
                    for keyword in keywords:
                        if keyword in text:
                            found_info.add(keyword)
        
        # ç¡®å®šç¼ºå¤±ä¿¡æ¯
        for category, keywords in required_info.items():
            if analysis_type == "general" or analysis_type == category:
                for keyword in keywords:
                    if keyword not in found_info:
                        missing_info.append(keyword)
        
        total_required = sum(len(keywords) for category, keywords in required_info.items() 
                            if analysis_type == "general" or analysis_type == category)
        
        completeness_score = len(found_info) / total_required if total_required > 0 else 1.0
        
        return {
            "completeness_score": completeness_score,
            "found_information": list(found_info),
            "missing_information": missing_info,
            "coverage_analysis": f"è¦†ç›–äº† {len(found_info)}/{total_required} é¡¹å¿…éœ€ä¿¡æ¯"
        }
    
    def _generate_tender_report(self, analysis: Dict[str, Any], query: str, analysis_type: str) -> Dict[str, Any]:
        """ğŸ¯ ç”Ÿæˆæ‹›æ ‡ä¹¦ä¸“ä¸šåˆ†ææŠ¥å‘Š"""
        
        report = {
            "executive_summary": self._generate_executive_summary(analysis, query),
            "detailed_findings": self._generate_detailed_findings(analysis, analysis_type),
            "recommendations": self._generate_recommendations(analysis, analysis_type),
            "risk_assessment": self._generate_risk_assessment(analysis),
            "action_items": self._generate_action_items(analysis, analysis_type),
            "confidence_metrics": self._calculate_confidence_metrics(analysis)
        }
        
        return report
    
    def _generate_executive_summary(self, analysis: Dict[str, Any], query: str) -> str:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        key_info = analysis.get("key_information", {})
        completeness = analysis.get("completeness_analysis", {})
        risks = analysis.get("risks_and_issues", [])
        
        summary_parts = [
            f"é’ˆå¯¹æŸ¥è¯¢ã€Œ{query}ã€çš„æ‹›æ ‡ä¹¦åˆ†æç»“æœå¦‚ä¸‹ï¼š",
            f"ä¿¡æ¯å®Œæ•´æ€§è¯„åˆ†ï¼š{completeness.get('completeness_score', 0):.1%}",
            f"è¯†åˆ«é£é™©ç‚¹ï¼š{len(risks)}ä¸ª",
            f"å…³é”®ä¿¡æ¯è¦†ç›–ï¼š{completeness.get('coverage_analysis', 'æœªçŸ¥')}"
        ]
        
        # æ·»åŠ å…³é”®å‘ç°
        if key_info.get("project_name"):
            summary_parts.append(f"é¡¹ç›®ä¿¡æ¯ï¼šå·²è¯†åˆ«é¡¹ç›®åç§°ç­‰åŸºæœ¬ä¿¡æ¯")
        
        if analysis.get("financial_info", {}).get("budget_limit"):
            summary_parts.append(f"è´¢åŠ¡ä¿¡æ¯ï¼šå·²è¯†åˆ«é¢„ç®—é™ä»·ç­‰è´¢åŠ¡æ¡æ¬¾")
        
        return "\n".join(summary_parts)
    
    def _generate_detailed_findings(self, analysis: Dict[str, Any], analysis_type: str) -> Dict[str, List[str]]:
        """ç”Ÿæˆè¯¦ç»†å‘ç°"""
        findings = {
            "positive_findings": [],
            "concerns": [],
            "missing_information": []
        }
        
        # æ­£é¢å‘ç°
        if analysis.get("key_information", {}).get("project_name"):
            findings["positive_findings"].append("âœ… é¡¹ç›®åŸºæœ¬ä¿¡æ¯å®Œæ•´")
        
        if analysis.get("dates_timeline", {}).get("bidding_deadline"):
            findings["positive_findings"].append("âœ… å…³é”®æ—¶é—´èŠ‚ç‚¹æ˜ç¡®")
        
        if analysis.get("financial_info", {}).get("budget_limit"):
            findings["positive_findings"].append("âœ… è´¢åŠ¡æ¡æ¬¾æ¸…æ™°")
        
        # å…³æ³¨ç‚¹
        risks = analysis.get("risks_and_issues", [])
        if len(risks) > 5:
            findings["concerns"].append(f"âš ï¸ è¯†åˆ«åˆ°{len(risks)}ä¸ªæ½œåœ¨é£é™©ç‚¹ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨")
        
        contradictions = analysis.get("contradictions", [])
        if contradictions:
            findings["concerns"].append(f"âš ï¸ å‘ç°{len(contradictions)}å¤„ä¿¡æ¯ä¸ä¸€è‡´ï¼Œéœ€è¦æ¾„æ¸…")
        
        # ç¼ºå¤±ä¿¡æ¯
        missing = analysis.get("completeness_analysis", {}).get("missing_information", [])
        for item in missing[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            findings["missing_information"].append(f"âŒ ç¼ºå°‘{item}ç›¸å…³ä¿¡æ¯")
        
        return findings
    
    def _generate_recommendations(self, analysis: Dict[str, Any], analysis_type: str) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        completeness_score = analysis.get("completeness_analysis", {}).get("completeness_score", 0)
        
        if completeness_score < 0.7:
            recommendations.append("ğŸ” å»ºè®®è¿›ä¸€æ­¥æ”¶é›†ç¼ºå¤±çš„å…³é”®ä¿¡æ¯")
        
        risks = analysis.get("risks_and_issues", [])
        if risks:
            recommendations.append("âš ï¸ å»ºè®®é’ˆå¯¹è¯†åˆ«çš„é£é™©ç‚¹åˆ¶å®šåº”å¯¹æªæ–½")
        
        contradictions = analysis.get("contradictions", [])
        if contradictions:
            recommendations.append("ğŸ“ å»ºè®®è”ç³»æ‹›æ ‡æ–¹æ¾„æ¸…çŸ›ç›¾ä¿¡æ¯")
        
        if analysis_type == "technical_specs":
            tech_req = analysis.get("technical_requirements", {})
            if not tech_req.get("quality_standards"):
                recommendations.append("ğŸ—ï¸ å»ºè®®æ˜ç¡®è´¨é‡æ ‡å‡†è¦æ±‚")
        
        return recommendations
    
    def _generate_risk_assessment(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆé£é™©è¯„ä¼°"""
        risks = analysis.get("risks_and_issues", [])
        
        risk_levels = {"é«˜": 0, "ä¸­": 0, "ä½": 0}
        
        for risk in risks:
            risk_score = risk.get("risk_score", 0)
            if risk_score >= 3:
                risk_levels["é«˜"] += 1
            elif risk_score >= 2:
                risk_levels["ä¸­"] += 1
            else:
                risk_levels["ä½"] += 1
        
        total_risks = sum(risk_levels.values())
        overall_risk = "ä½"
        if risk_levels["é«˜"] > 0:
            overall_risk = "é«˜"
        elif risk_levels["ä¸­"] > 2:
            overall_risk = "ä¸­"
        
        return {
            "overall_risk_level": overall_risk,
            "risk_distribution": risk_levels,
            "total_risks": total_risks,
            "top_risks": risks[:3] if risks else []
        }
    
    def _generate_action_items(self, analysis: Dict[str, Any], analysis_type: str) -> List[Dict[str, Any]]:
        """ç”Ÿæˆè¡ŒåŠ¨é¡¹"""
        actions = []
        
        # åŸºäºç¼ºå¤±ä¿¡æ¯ç”Ÿæˆè¡ŒåŠ¨é¡¹
        missing = analysis.get("completeness_analysis", {}).get("missing_information", [])
        for item in missing[:3]:
            actions.append({
                "action": f"æ”¶é›†{item}ç›¸å…³ä¿¡æ¯",
                "priority": "é«˜",
                "category": "ä¿¡æ¯æ”¶é›†"
            })
        
        # åŸºäºé£é™©ç”Ÿæˆè¡ŒåŠ¨é¡¹
        risks = analysis.get("risks_and_issues", [])
        for risk in risks[:2]:
            actions.append({
                "action": f"åˆ†æé£é™©ï¼š{risk.get('text', '')[:50]}...",
                "priority": "ä¸­",
                "category": "é£é™©åˆ†æ"
            })
        
        return actions
    
    def _calculate_confidence_metrics(self, analysis: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—ç½®ä¿¡åº¦æŒ‡æ ‡"""
        
        completeness_score = analysis.get("completeness_analysis", {}).get("completeness_score", 0)
        risks_count = len(analysis.get("risks_and_issues", []))
        contradictions_count = len(analysis.get("contradictions", []))
        
        # ä¿¡æ¯å®Œæ•´æ€§ç½®ä¿¡åº¦
        info_confidence = completeness_score
        
        # é£é™©è¯†åˆ«ç½®ä¿¡åº¦ï¼ˆé£é™©ç‚¹è¶Šå¤šï¼Œç½®ä¿¡åº¦è¶Šé«˜ï¼‰
        risk_confidence = min(1.0, risks_count / 10)
        
        # ä¸€è‡´æ€§ç½®ä¿¡åº¦ï¼ˆçŸ›ç›¾è¶Šå°‘ï¼Œç½®ä¿¡åº¦è¶Šé«˜ï¼‰
        consistency_confidence = max(0.0, 1.0 - contradictions_count * 0.2)
        
        # æ•´ä½“ç½®ä¿¡åº¦
        overall_confidence = (info_confidence + risk_confidence + consistency_confidence) / 3
        
        return {
            "overall_confidence": overall_confidence,
            "information_completeness": info_confidence,
            "risk_identification": risk_confidence,
            "consistency_check": consistency_confidence
        }


# å…¨å±€æœç´¢æœåŠ¡å®ä¾‹
search_service = SearchService()


async def get_search_service() -> SearchService:
    """è·å–æœç´¢æœåŠ¡å®ä¾‹"""
    return search_service 