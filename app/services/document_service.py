"""
æ–‡æ¡£å¤„ç†æœåŠ¡
è´Ÿè´£æ–‡æ¡£çš„ä¸Šä¼ ã€è§£æã€å‘é‡åŒ–å’Œç®¡ç†ï¼Œä½¿ç”¨åˆ†å¸ƒå¼å­˜å‚¨æ¶æ„
"""

import os
import uuid
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import asyncio
from pathlib import Path

from app.core.config import settings
from app.models.responses import ErrorCode
from app.core.exceptions import create_service_exception
from app.services.storage_service import get_minio_service
from app.services.cache_service import get_cache_service
from app.services.vector_service import get_vector_service

# å…ˆå®šä¹‰logger
logger = logging.getLogger("rag-anything")

# RAGAnything ç›¸å…³å¯¼å…¥
try:
    from raganything import RAGAnything
    from raganything.modalprocessors import (
        ImageModalProcessor, 
        TableModalProcessor, 
        EquationModalProcessor, 
        GenericModalProcessor
    )
except ImportError:
    logger.warning("RAGAnything æœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
    RAGAnything = None
    ImageModalProcessor = None
    TableModalProcessor = None
    EquationModalProcessor = None
    GenericModalProcessor = None


class DocumentService:
    """æ–‡æ¡£å¤„ç†æœåŠ¡"""
    
    def __init__(self):
        self.minio_service = None
        self.cache_service = None
        self.vector_service = None
        self.rag_processor = None
    
    async def _get_services(self):
        """è·å–ä¾èµ–æœåŠ¡"""
        if self.minio_service is None:
            self.minio_service = await get_minio_service()
        if self.cache_service is None:
            self.cache_service = await get_cache_service()
        if self.vector_service is None:
            self.vector_service = await get_vector_service()
        
        # åˆå§‹åŒ–RAGå¤„ç†å™¨
        if self.rag_processor is None and RAGAnything is not None:
            try:
                # RAGAnythingéœ€è¦æ¨¡å‹å‡½æ•°ï¼Œä¸æ˜¯APIå‚æ•°
                # æš‚æ—¶ä½¿ç”¨Noneï¼Œç­‰åç»­éœ€è¦æ—¶å†é…ç½®å…·ä½“çš„æ¨¡å‹å‡½æ•°
                self.rag_processor = RAGAnything(
                    llm_model_func=None,  # éœ€è¦æ—¶å†é…ç½®å…·ä½“çš„LLMå‡½æ•°
                    embedding_func=None,  # éœ€è¦æ—¶å†é…ç½®å…·ä½“çš„åµŒå…¥å‡½æ•°  
                    vision_model_func=None,  # éœ€è¦æ—¶å†é…ç½®å…·ä½“çš„è§†è§‰å‡½æ•°
                    config=None  # ä½¿ç”¨é»˜è®¤é…ç½®
                )
                logger.info("RAGAnything å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"RAGAnything å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                # å¦‚æœåˆå§‹åŒ–å¤±è´¥ï¼Œè®¾ç½®ä¸ºNoneï¼Œä¸å½±å“åŸºæœ¬çš„æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
                self.rag_processor = None
    
    async def upload_file(
        self, 
        file_content: bytes = None,
        filename: str = None, 
        content_type: str = None,
        metadata: Optional[Dict[str, Any]] = None,
        original_name: Optional[str] = None,
        description: Optional[str] = None,
        **kwargs  # æ¥æ”¶é¢å¤–å‚æ•°ï¼Œæé«˜å…¼å®¹æ€§
    ) -> str:
        """ä¸Šä¼ æ–‡ä»¶åˆ°MinIO"""
        # ğŸ’¡ æ·»åŠ æ–¹æ³•å…¥å£æ—¥å¿— - ç«‹å³è®°å½•æ‰€æœ‰ä¼ å…¥å‚æ•°
        logger.info(f"ğŸ” upload_file æ–¹æ³•è¢«è°ƒç”¨ï¼")
        logger.info(f"ğŸ“‹ ä¼ å…¥å‚æ•°è¯¦æƒ…:")
        logger.info(f"  - file_content: {'å­˜åœ¨' if file_content else 'ç¼ºå¤±'} (é•¿åº¦: {len(file_content) if file_content else 0})")
        logger.info(f"  - filename: '{filename}' (ç±»å‹: {type(filename)})")
        logger.info(f"  - content_type: '{content_type}'")
        logger.info(f"  - original_name: '{original_name}'")
        logger.info(f"  - description: '{description}'")
        logger.info(f"  - metadata: {metadata}")
        logger.info(f"  - kwargs: {kwargs}")
        
        # å‚æ•°éªŒè¯å’Œå…¼å®¹æ€§å¤„ç† - å‚è€ƒmineru-webçš„å®ç°
        if not file_content:  # æ£€æŸ¥Noneæˆ–ç©ºå†…å®¹
            # å°è¯•ä»kwargsè·å–
            file_content = kwargs.get('data', kwargs.get('file_data'))
            if not file_content:  # æ£€æŸ¥Noneæˆ–ç©ºå†…å®¹
                raise create_service_exception(
                    ErrorCode.INVALID_REQUEST,
                    "ç¼ºå°‘æ–‡ä»¶å†…å®¹å‚æ•° (file_content)"
                )
        
        if not filename:  # æ£€æŸ¥Noneæˆ–ç©ºå­—ç¬¦ä¸²
            # å°è¯•ä»kwargsè·å–
            filename = kwargs.get('name', kwargs.get('file_name'))
            if not filename:  # æ£€æŸ¥Noneæˆ–ç©ºå­—ç¬¦ä¸²
                raise create_service_exception(
                    ErrorCode.INVALID_REQUEST,
                    "ç¼ºå°‘æ–‡ä»¶åå‚æ•° (filename)"
                )
        
        # å¦‚æœè¿˜æ²¡æœ‰content_typeï¼Œå°è¯•ä»kwargsè·å–
        if content_type is None:
            content_type = kwargs.get('content_type', kwargs.get('mime_type'))
        
        # è°ƒè¯•æ—¥å¿— - è®°å½•å‚æ•°ä¿¡æ¯
        logger.info(f"upload_file è°ƒç”¨å‚æ•°: filename='{filename}', content_type='{content_type}', "
                    f"file_size={len(file_content) if file_content else 0}, "
                    f"original_name='{original_name}', description='{description}', "
                    f"kwargs={list(kwargs.keys()) if kwargs else []}")
        
        await self._get_services()
        
        # ç”Ÿæˆæ–‡ä»¶ID
        file_id = str(uuid.uuid4())
        file_extension = Path(filename).suffix.lower()
        
        # ç”Ÿæˆå¯¹è±¡åï¼ˆåŒ…å«è·¯å¾„ç»“æ„ï¼‰
        upload_date = datetime.now().strftime("%Y/%m/%d")
        object_name = f"documents/{upload_date}/{file_id}{file_extension}"
        
        try:
            # ä¸Šä¼ åˆ°MinIO
            file_url = await self.minio_service.upload_file(
                object_name=object_name,
                file_data=file_content,
                content_type=content_type
            )
            
            # å‡†å¤‡æ–‡ä»¶å…ƒæ•°æ®
            display_name = original_name or filename
            
            # å¤„ç†descriptionå‚æ•°
            final_metadata = metadata or {}
            if description:
                final_metadata = {**final_metadata, "description": description}
            
            file_metadata = {
                "file_id": file_id,
                "filename": filename,
                "original_name": display_name,
                "object_name": object_name,
                "file_url": file_url,
                "file_size": len(file_content),
                "content_type": content_type,
                "file_extension": file_extension,
                "upload_date": datetime.now().isoformat(),
                "status": "uploaded",
                "parse_status": "pending",
                "custom_metadata": final_metadata
            }
            
            # ä¿å­˜å…ƒæ•°æ®åˆ°Redis
            await self.cache_service.save_file_metadata(file_id, file_metadata)
            
            logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename} -> {file_id}")
            return file_id
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {filename} - {e}")
            raise create_service_exception(
                ErrorCode.FILE_UPLOAD_FAILED,
                f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
            )
    
    async def get_file_info(self, file_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        await self._get_services()
        
        # ä»Redisè·å–æ–‡ä»¶å…ƒæ•°æ®
        metadata = await self.cache_service.get_file_metadata(file_id)
        if not metadata:
            return None
        
        # è¡¥å……MinIOä¸­çš„å®æ—¶ä¿¡æ¯
        try:
            object_name = metadata.get("object_name")
            if object_name:
                minio_info = await self.minio_service.get_file_info(object_name)
                if minio_info:
                    metadata.update({
                        "minio_etag": minio_info.get("etag"),
                        "minio_last_modified": minio_info.get("last_modified"),
                        "actual_size": minio_info.get("size")
                    })
        except Exception as e:
            logger.warning(f"è·å–MinIOæ–‡ä»¶ä¿¡æ¯å¤±è´¥: {file_id} - {e}")
        
        return metadata
    
    async def download_file(self, file_id: str) -> Tuple[bytes, Dict[str, Any]]:
        """ä¸‹è½½æ–‡ä»¶"""
        await self._get_services()
        
        # è·å–æ–‡ä»¶å…ƒæ•°æ®
        metadata = await self.get_file_info(file_id)
        if not metadata:
            raise create_service_exception(
                ErrorCode.FILE_NOT_FOUND,
                f"æ–‡ä»¶ä¸å­˜åœ¨: {file_id}"
            )
        
        object_name = metadata.get("object_name")
        if not object_name:
            raise create_service_exception(
                ErrorCode.FILE_NOT_FOUND,
                f"æ–‡ä»¶å¯¹è±¡åä¸å­˜åœ¨: {file_id}"
            )
        
        try:
            # ä»MinIOä¸‹è½½æ–‡ä»¶
            file_data = await self.minio_service.download_file(object_name)
            return file_data, metadata
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {file_id} - {e}")
            raise create_service_exception(
                ErrorCode.FILE_DOWNLOAD_FAILED,
                f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}"
            )
    
    async def delete_file(
        self, 
        file_id: str, 
        delete_parsed_data: bool = True, 
        delete_vector_data: bool = True
    ) -> bool:
        """åˆ é™¤æ–‡ä»¶"""
        await self._get_services()
        
        try:
            # è·å–æ–‡ä»¶å…ƒæ•°æ®
            metadata = await self.get_file_info(file_id)
            if not metadata:
                logger.warning(f"æ–‡ä»¶å…ƒæ•°æ®ä¸å­˜åœ¨: {file_id}")
                return False
            
            success = True
            
            # åˆ é™¤å‘é‡æ•°æ®åº“ä¸­çš„æ•°æ®
            if delete_vector_data:
                try:
                    await self.vector_service.delete_document(file_id)
                except Exception as e:
                    logger.error(f"åˆ é™¤å‘é‡æ•°æ®å¤±è´¥: {file_id} - {e}")
                    success = False
            
            # åˆ é™¤è§£ææ•°æ®
            if delete_parsed_data:
                try:
                    # åˆ é™¤è§£æç»“æœç¼“å­˜
                    await self.cache_service.delete(f"parse_result:{file_id}")
                    await self.cache_service.delete(f"text_chunks:{file_id}")
                except Exception as e:
                    logger.error(f"åˆ é™¤è§£ææ•°æ®å¤±è´¥: {file_id} - {e}")
                    success = False
            
            # åˆ é™¤MinIOä¸­çš„æ–‡ä»¶
            object_name = metadata.get("object_name")
            if object_name:
                try:
                    await self.minio_service.delete_file(object_name)
                except Exception as e:
                    logger.error(f"åˆ é™¤MinIOæ–‡ä»¶å¤±è´¥: {file_id} - {e}")
                    success = False
            
            # åˆ é™¤Redisä¸­çš„å…ƒæ•°æ®
            try:
                await self.cache_service.delete(f"file:{file_id}")
            except Exception as e:
                logger.error(f"åˆ é™¤Rediså…ƒæ•°æ®å¤±è´¥: {file_id} - {e}")
                success = False
            
            if success:
                logger.info(f"æ–‡ä»¶åˆ é™¤æˆåŠŸ: {file_id}")
            else:
                logger.warning(f"æ–‡ä»¶åˆ é™¤éƒ¨åˆ†å¤±è´¥: {file_id}")
            
            return success
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶åˆ é™¤å¤±è´¥: {file_id} - {e}")
            return False
    
    async def start_parse_task(self, file_id: str, priority: int = 0) -> str:
        """å¯åŠ¨æ–‡æ¡£è§£æä»»åŠ¡ - æ”¯æŒä¼˜å…ˆçº§è®¾ç½®"""
        await self._get_services()
        
        try:
            # è·å–ä»»åŠ¡æœåŠ¡
            from app.services.task_service import get_task_service
            task_service = await get_task_service()
            
            # ğŸ”§ ä¿®å¤ï¼šå®šä¹‰è§£æä»»åŠ¡å‡½æ•°ï¼Œç„¶åä¼ é€’ç»™create_task
            async def parse_task_func():
                """å®é™…çš„è§£æä»»åŠ¡å‡½æ•°"""
                try:
                    logger.info(f"å¼€å§‹è§£ææ–‡æ¡£: {file_id}")
                    result = await self.parse_document(file_id)
                    logger.info(f"æ–‡æ¡£è§£æå®Œæˆ: {file_id}")
                    return result
                except Exception as e:
                    logger.error(f"æ–‡æ¡£è§£æä»»åŠ¡å¤±è´¥: {file_id} - {e}")
                    raise
            
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è°ƒç”¨create_taskï¼Œä¼ é€’å‡½æ•°ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°
            task_id = await task_service.create_task(
                task_func=parse_task_func,  # âœ… ä¼ é€’å‡½æ•°ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°
                task_name=f"è§£ææ–‡æ¡£ {file_id}",
                created_by="document_service"
            )
            
            logger.info(f"æ–‡æ¡£è§£æä»»åŠ¡å·²å¯åŠ¨: {task_id} - {file_id} - ä¼˜å…ˆçº§: {priority}")
            return task_id
            
        except Exception as e:
            logger.error(f"å¯åŠ¨æ–‡æ¡£è§£æä»»åŠ¡å¤±è´¥: {file_id} - {e}")
            raise create_service_exception(
                ErrorCode.TASK_CREATION_FAILED,
                f"å¯åŠ¨è§£æä»»åŠ¡å¤±è´¥: {str(e)}"
            )
    
    async def _run_mineru_with_sglang(self, input_file: str, file_id: str, original_filename: str = None) -> Dict[str, Any]:
        """ä½¿ç”¨MinerUå’ŒSGLangæœåŠ¡è§£ææ–‡æ¡£ - ğŸ”§ ä¼˜åŒ–ï¼šè§£æç»“æœç›´æ¥å­˜å‚¨åˆ°MinIO"""
        try:
            import subprocess
            import tempfile
            import shutil
            
            # åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                # ğŸ”§ ä¿®å¤ï¼šä¿æŒåŸå§‹æ–‡ä»¶åè€Œä¸æ˜¯ä½¿ç”¨"input"
                if original_filename:
                    # ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼Œä½†æ¸…ç†ç‰¹æ®Šå­—ç¬¦ä»¥é¿å…è·¯å¾„é—®é¢˜
                    safe_filename = "".join(c for c in original_filename if c.isalnum() or c in '.-_')
                    temp_input = os.path.join(temp_dir, safe_filename)
                else:
                    # å…œåº•æ–¹æ¡ˆï¼šä½¿ç”¨file_idä½œä¸ºæ–‡ä»¶å
                    temp_input = os.path.join(temp_dir, f"{file_id}" + Path(input_file).suffix)
                
                shutil.copy2(input_file, temp_input)
                logger.info(f"ğŸ“„ ä½¿ç”¨æ–‡ä»¶åè¿›è¡Œè§£æ: {os.path.basename(temp_input)}")
                
                # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
                temp_output = os.path.join(temp_dir, "output")
                os.makedirs(temp_output, exist_ok=True)
                
                # æ„å»ºMinerUå‘½ä»¤
                cmd = [
                    "mineru",
                    "-p", temp_input,
                    "-o", temp_output,
                    "-b", "vlm-sglang-client",
                    "-u", settings.SGLANG_API_BASE
                ]
                
                logger.info(f"æ‰§è¡ŒMinerUå‘½ä»¤: {' '.join(cmd)}")
                
                # ğŸ”§ ä¿®å¤ï¼šå¢åŠ è¶…æ—¶æ—¶é—´å¹¶æ·»åŠ æ›´è¯¦ç»†çš„æ—¥å¿—
                logger.info(f"â±ï¸  å¼€å§‹æ‰§è¡ŒMinerUè§£æï¼Œé¢„è®¡éœ€è¦10-15åˆ†é’Ÿ...")
                
                # æ‰§è¡Œå‘½ä»¤ - å¢åŠ è¶…æ—¶åˆ°20åˆ†é’Ÿï¼Œé€‚åº”å¤§æ–‡ä»¶å¤„ç†
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=1200  # ğŸ”§ 20åˆ†é’Ÿè¶…æ—¶ï¼Œé€‚åº”å¤§æ–‡ä»¶
                )
                
                logger.info(f"âœ… MinerUå‘½ä»¤æ‰§è¡Œå®Œæˆï¼Œè¿”å›ç : {result.returncode}")
                
                if result.returncode != 0:
                    logger.error(f"MinerUæ‰§è¡Œå¤±è´¥: {result.stderr}")
                    raise Exception(f"MinerUæ‰§è¡Œå¤±è´¥: {result.stderr}")
                
                # ğŸ”§ ä¼˜åŒ–ï¼šå°†è§£æç»“æœä¸Šä¼ åˆ°MinIOè€Œä¸æ˜¯ä¿å­˜åˆ°æœ¬åœ°
                minio_files = []
                content_blocks = []
                
                logger.info(f"ğŸ“¤ å¼€å§‹å°†è§£æç»“æœä¸Šä¼ åˆ°MinIO...")
                
                if os.path.exists(temp_output):
                    # ç»Ÿè®¡è¦ä¸Šä¼ çš„æ–‡ä»¶
                    total_files = sum([len(files) for _, _, files in os.walk(temp_output)])
                    logger.info(f"ğŸ“Š å‘ç°{total_files}ä¸ªè§£æç»“æœæ–‡ä»¶ï¼Œå¼€å§‹ä¸Šä¼ ...")
                    
                    uploaded_count = 0
                    # æ‰«æä¸´æ—¶è¾“å‡ºç›®å½•å¹¶ä¸Šä¼ æ‰€æœ‰æ–‡ä»¶åˆ°MinIO
                    for root, dirs, files in os.walk(temp_output):
                        for file in files:
                            local_file_path = os.path.join(root, file)
                            
                            # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
                            rel_path = os.path.relpath(local_file_path, temp_output)
                            
                            # MinIOä¸­çš„è·¯å¾„ï¼šparsed/{file_id}/{ç›¸å¯¹è·¯å¾„}
                            minio_path = f"parsed/{file_id}/{rel_path}"
                            
                            try:
                                # è¯»å–æ–‡ä»¶å†…å®¹
                                with open(local_file_path, 'rb') as f:
                                    file_content = f.read()
                                
                                # ä¸Šä¼ åˆ°MinIO
                                content_type = "text/markdown" if file.endswith('.md') else \
                                             "application/json" if file.endswith('.json') else \
                                             "application/octet-stream"
                                
                                await self.minio_service.upload_file(
                                    object_name=minio_path,
                                    file_data=file_content,
                                    content_type=content_type
                                )
                                
                                minio_files.append(minio_path)
                                uploaded_count += 1
                                
                                # æ·»åŠ åˆ°å†…å®¹å—åˆ—è¡¨
                                if file.endswith('.md'):
                                    content_blocks.append({
                                        "type": "markdown",
                                        "minio_path": minio_path,
                                        "local_filename": file,
                                        "size": len(file_content)
                                    })
                                elif file.endswith('.json'):
                                    content_blocks.append({
                                        "type": "json", 
                                        "minio_path": minio_path,
                                        "local_filename": file,
                                        "size": len(file_content)
                                    })
                                
                                logger.info(f"âœ… [{uploaded_count}/{total_files}] å·²ä¸Šä¼ : {minio_path} ({len(file_content)} å­—èŠ‚)")
                                
                            except Exception as e:
                                logger.error(f"âŒ ä¸Šä¼ è§£æç»“æœå¤±è´¥: {local_file_path} -> {minio_path} - {e}")
                                continue
                    
                    if uploaded_count > 0:
                        logger.info(f"ğŸ‰ MinIOä¸Šä¼ å®Œæˆ: {uploaded_count}/{total_files} ä¸ªæ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
                    else:
                        logger.warning(f"âš ï¸  æ²¡æœ‰æ–‡ä»¶è¢«ä¸Šä¼ åˆ°MinIO")
                else:
                    logger.warning(f"âš ï¸  MinerUè¾“å‡ºç›®å½•ä¸å­˜åœ¨: {temp_output}")
                
                # è§£æç»“æœ
                parse_result = {
                    "status": "success",
                    "minio_base_path": f"parsed/{file_id}",
                    "minio_files": minio_files,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "content_blocks": content_blocks,
                    "uploaded_files_count": len(minio_files),
                    "processing_time": "æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—è·å–è¯¦ç»†æ—¶é—´"
                }
                
                logger.info(f"ğŸ‰ MinerUè§£æå®Œæˆ: æ‰¾åˆ°{len(content_blocks)}ä¸ªå†…å®¹å—ï¼Œå·²ä¸Šä¼ {len(minio_files)}ä¸ªæ–‡ä»¶åˆ°MinIO")
                return parse_result
                
        except subprocess.TimeoutExpired as e:
            logger.error(f"â° MinerUè§£æè¶…æ—¶ï¼ˆ20åˆ†é’Ÿï¼‰: {e}")
            return {
                "status": "failed",
                "error": f"è§£æè¶…æ—¶ï¼Œå¤§æ–‡ä»¶å¤„ç†éœ€è¦æ›´é•¿æ—¶é—´: {str(e)}",
                "content_blocks": [],
                "timeout": True
            }
        except Exception as e:
            logger.error(f"âŒ MinerUè§£æå¤±è´¥: {e}")
            import traceback
            logger.error(f"ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return {
                "status": "failed",
                "error": str(e),
                "content_blocks": [],
                "detailed_error": traceback.format_exc()
            }
    
    async def parse_document(self, file_id: str) -> Dict[str, Any]:
        """è§£ææ–‡æ¡£å†…å®¹"""
        await self._get_services()
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        metadata = await self.get_file_info(file_id)
        if not metadata:
            raise create_service_exception(
                ErrorCode.FILE_NOT_FOUND,
                f"æ–‡ä»¶ä¸å­˜åœ¨: {file_id}"
            )
        
        try:
            # ğŸ”§ ä¿®å¤æ–‡ä»¶å¤§å°è®¡ç®—é”™è¯¯
            filename = metadata.get("filename", "æœªçŸ¥æ–‡ä»¶")
            file_size = metadata.get("file_size", 0)
            
            # ç¡®ä¿file_sizeæ˜¯æ•°å­—ç±»å‹
            try:
                file_size_num = int(file_size) if isinstance(file_size, str) else file_size
                file_size_mb = round(file_size_num / (1024*1024), 2) if file_size_num else 0
                estimated_time = file_size_mb * 0.5
            except (ValueError, TypeError):
                file_size_mb = 0
                estimated_time = 5  # é»˜è®¤ä¼°è®¡5åˆ†é’Ÿ
            
            logger.info(f"ğŸš€ å¼€å§‹è§£ææ–‡æ¡£: {filename}")
            logger.info(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {file_size_mb} MB")
            logger.info(f"   ğŸ†” æ–‡ä»¶ID: {file_id}")
            logger.info(f"   â±ï¸  é¢„è®¡å¤„ç†æ—¶é—´: {estimated_time:.1f} åˆ†é’Ÿï¼ˆå¤§æ–‡ä»¶éœ€è¦æ›´é•¿æ—¶é—´ï¼‰")
            
            # æ›´æ–°è§£æçŠ¶æ€
            await self.cache_service.save_file_metadata(
                file_id, 
                {**metadata, "parse_status": "parsing", "parse_started_at": datetime.now().isoformat()}
            )
            
            # ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            logger.info(f"ğŸ“¥ ä»MinIOä¸‹è½½æ–‡ä»¶è¿›è¡Œè§£æ...")
            file_data, _ = await self.download_file(file_id)
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            import tempfile
            with tempfile.NamedTemporaryFile(
                suffix=metadata.get("file_extension", ".pdf"), 
                delete=False
            ) as temp_file:
                temp_file.write(file_data)
                temp_input_path = temp_file.name
            
            try:
                # ğŸ”§ ä¼˜åŒ–ï¼šä¸å†éœ€è¦æœ¬åœ°è¾“å‡ºç›®å½•ï¼Œç›´æ¥ä½¿ç”¨MinIOå­˜å‚¨
                
                # ä½¿ç”¨MinerUè§£æ - ğŸ”§ ä¼ é€’åŸå§‹æ–‡ä»¶å
                original_filename = metadata.get("filename", metadata.get("original_filename"))
                parse_result = await self._run_mineru_with_sglang(temp_input_path, file_id, original_filename)
                
                # æ›´æ–°å…ƒæ•°æ® - ğŸ”§ ä¼˜åŒ–ï¼šä¿å­˜MinIOè·¯å¾„ä¿¡æ¯è€Œä¸æ˜¯æœ¬åœ°è·¯å¾„
                # ğŸ”§ ä¿®å¤ï¼šåŒæ—¶æ›´æ–°statuså’Œparse_statuså­—æ®µä»¥ä¿æŒAPIå…¼å®¹æ€§
                is_success = parse_result["status"] == "success"
                updated_metadata = {
                    **metadata,
                    "status": "parsed" if is_success else "parse_failed",  # APIå±‚æ£€æŸ¥çš„å­—æ®µ
                    "parse_status": "completed" if is_success else "failed",  # Serviceå±‚ä½¿ç”¨çš„å­—æ®µ
                    "parse_result": parse_result,
                    "parsed_at": datetime.now().isoformat(),
                    "minio_base_path": parse_result.get("minio_base_path"),  # MinIOä¸­çš„åŸºç¡€è·¯å¾„
                    "parsed_files_count": len(parse_result.get("content_blocks", [])),
                    "storage_location": "minio"  # æ ‡è®°å­˜å‚¨ä½ç½®
                }
                
                await self.cache_service.save_file_metadata(file_id, updated_metadata)
                
                # ğŸ”§ æ·»åŠ è¯¦ç»†çš„å®Œæˆæ—¥å¿—
                if parse_result.get("status") == "success":
                    uploaded_files = parse_result.get("uploaded_files_count", 0)
                    content_blocks = len(parse_result.get("content_blocks", []))
                    minio_path = parse_result.get("minio_base_path")
                    
                    logger.info(f"ğŸ‰ æ–‡æ¡£è§£æå®Œå…¨æˆåŠŸ: {file_id}")
                    logger.info(f"   ğŸ“ MinIOå­˜å‚¨è·¯å¾„: {minio_path}")
                    logger.info(f"   ğŸ“„ è§£æå‡ºå†…å®¹å—: {content_blocks}ä¸ª")
                    logger.info(f"   ğŸ’¾ ä¸Šä¼ æ–‡ä»¶æ•°é‡: {uploaded_files}ä¸ª")
                    logger.info(f"   âœ… æ‰€æœ‰æ•°æ®å·²å®‰å…¨å­˜å‚¨åˆ°MinIO")
                else:
                    error_msg = parse_result.get("error", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"âŒ æ–‡æ¡£è§£æå¤±è´¥: {file_id}")
                    logger.error(f"   ğŸ” é”™è¯¯ä¿¡æ¯: {error_msg}")
                    if parse_result.get("timeout"):
                        logger.warning(f"   â° å»ºè®®ï¼šå¤§æ–‡ä»¶å¯èƒ½éœ€è¦æ›´é•¿å¤„ç†æ—¶é—´")
                
                return parse_result
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(temp_input_path)
                except:
                    pass
                    
        except Exception as e:
            # æ›´æ–°è§£æçŠ¶æ€ä¸ºå¤±è´¥ - ğŸ”§ ä¿®å¤ï¼šåŒæ—¶æ›´æ–°statuså’Œparse_status
            await self.cache_service.save_file_metadata(
                file_id,
                {
                    **metadata, 
                    "status": "parse_failed",  # APIå±‚æ£€æŸ¥çš„å­—æ®µ
                    "parse_status": "failed",  # Serviceå±‚ä½¿ç”¨çš„å­—æ®µ
                    "parse_error": str(e)
                }
            )
            
            logger.error(f"æ–‡æ¡£è§£æå¤±è´¥: {file_id} - {e}")
            raise create_service_exception(
                ErrorCode.FILE_PARSE_FAILED,
                f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}"
            )
    
    async def extract_text_chunks(self, file_id: str) -> List[Dict[str, Any]]:
        """ä»è§£æç»“æœä¸­æå–æ–‡æœ¬å—"""
        await self._get_services()
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        metadata = await self.get_file_info(file_id)
        if not metadata:
            raise create_service_exception(
                ErrorCode.FILE_NOT_FOUND,
                f"æ–‡ä»¶ä¸å­˜åœ¨: {file_id}"
            )
        
        parse_result = metadata.get("parse_result")
        if not parse_result or parse_result.get("status") != "success":
            raise create_service_exception(
                ErrorCode.FILE_PARSE_FAILED,
                f"æ–‡æ¡£æœªè§£ææˆ–è§£æå¤±è´¥: {file_id}"
            )
        
        chunks = []
        content_blocks = parse_result.get("content_blocks", [])
        
        for i, block in enumerate(content_blocks):
            if block.get("type") == "markdown":
                try:
                    # ğŸ”§ ä¼˜åŒ–ï¼šä»MinIOè¯»å–æ–‡ä»¶è€Œä¸æ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
                    minio_path = block.get("minio_path")
                    if minio_path:
                        # ä»MinIOä¸‹è½½æ–‡ä»¶å†…å®¹
                        file_content = await self.minio_service.download_file(minio_path)
                        content = file_content.decode('utf-8')
                        
                        # ç®€å•çš„æ–‡æœ¬åˆ†å—ï¼ˆå¯ä»¥åç»­ä¼˜åŒ–ï¼‰
                        chunk_size = 1000
                        overlap = 200
                        
                        for j in range(0, len(content), chunk_size - overlap):
                            chunk_text = content[j:j + chunk_size]
                            if len(chunk_text.strip()) > 50:  # å¿½ç•¥å¤ªçŸ­çš„å—
                                chunks.append({
                                    "chunk_id": f"{file_id}_{i}_{j}",
                                    "text": chunk_text.strip(),
                                    "chunk_index": len(chunks),
                                    "source_minio_path": minio_path,  # ğŸ”§ æ”¹ä¸ºMinIOè·¯å¾„
                                    "block_type": "markdown",
                                    "start_pos": j,
                                    "end_pos": j + len(chunk_text)
                                })
                                
                except Exception as e:
                    logger.warning(f"ä»MinIOè¯»å–è§£ææ–‡ä»¶å¤±è´¥: {minio_path} - {e}")
        
        logger.info(f"æå–æ–‡æœ¬å—å®Œæˆ: {file_id} - {len(chunks)}ä¸ªå—")
        return chunks
    
    async def vectorize_document(self, file_id: str) -> Dict[str, Any]:
        """å°†æ–‡æ¡£å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“"""
        await self._get_services()
        
        if not self.rag_processor:
            raise create_service_exception(
                ErrorCode.INTERNAL_SERVER_ERROR,
                "RAGAnything å¤„ç†å™¨æœªåˆå§‹åŒ–"
            )
        
        try:
            # è·å–æ–‡ä»¶å…ƒæ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦å±äºçŸ¥è¯†åº“
            file_metadata = await self.get_file_info(file_id)
            if not file_metadata:
                raise create_service_exception(
                    ErrorCode.FILE_NOT_FOUND,
                    f"æ–‡ä»¶ä¸å­˜åœ¨: {file_id}"
                )
            
            # ç¡®å®šå‘é‡é›†åˆåç§°
            collection_name = None
            kb_id = file_metadata.get("kb_id")
            
            if kb_id:
                # æ–‡ä»¶å±äºçŸ¥è¯†åº“ï¼Œè·å–çŸ¥è¯†åº“çš„é›†åˆåç§°
                from app.services.knowledge_base_service import get_knowledge_base_service
                kb_service = await get_knowledge_base_service()
                knowledge_base = await kb_service.get_knowledge_base(kb_id)
                if knowledge_base:
                    collection_name = knowledge_base.qdrant_config.collection_name
                    logger.info(f"æ–‡ä»¶ {file_id} å±äºçŸ¥è¯†åº“ {kb_id}ï¼Œä½¿ç”¨é›†åˆ: {collection_name}")
                else:
                    logger.warning(f"æ–‡ä»¶ {file_id} å…³è”çš„çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é›†åˆ")
            
            # æå–æ–‡æœ¬å—
            chunks = await self.extract_text_chunks(file_id)
            
            if not chunks:
                raise create_service_exception(
                    ErrorCode.FILE_PARSE_FAILED,
                    f"æ–‡æ¡£æ²¡æœ‰å¯æå–çš„å†…å®¹: {file_id}"
                )
            
            # ç”Ÿæˆå‘é‡
            texts = [chunk["text"] for chunk in chunks]
            
            # ä½¿ç”¨RAGAnythingç”Ÿæˆembeddings
            embeddings = []
            for text in texts:
                # _get_embeddingæ–¹æ³•å†…éƒ¨å·²ç»åŒ…å«äº†fallbacké€»è¾‘
                embedding = await self._get_embedding(text)
                embeddings.append(embedding)
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨çŸ¥è¯†åº“çš„é›†åˆæˆ–é»˜è®¤é›†åˆï¼‰
            point_ids = await self.vector_service.add_document_chunks(
                file_id=file_id,
                chunks=chunks,
                vectors=embeddings,
                collection_name=collection_name  # ğŸ”§ ä½¿ç”¨çŸ¥è¯†åº“çš„é›†åˆåç§°
            )
            
            # æ›´æ–°æ–‡ä»¶å…ƒæ•°æ®
            if file_metadata:
                updated_metadata = {
                    **file_metadata,
                    "vector_status": "completed",
                    "vectorized_at": datetime.now().isoformat(),
                    "chunk_count": len(chunks),
                    "vector_point_ids": point_ids,
                    "vector_collection": collection_name or "rag_documents"  # è®°å½•å‘é‡é›†åˆåç§°
                }
                await self.cache_service.save_file_metadata(file_id, updated_metadata)
            
            result = {
                "file_id": file_id,
                "chunk_count": len(chunks),
                "vector_count": len(embeddings),
                "point_ids": point_ids
            }
            
            logger.info(f"æ–‡æ¡£å‘é‡åŒ–å®Œæˆ: {file_id} - {len(chunks)}ä¸ªå—")
            return result
            
        except Exception as e:
            # æ›´æ–°å‘é‡åŒ–çŠ¶æ€ä¸ºå¤±è´¥
            try:
                current_metadata = await self.get_file_info(file_id)
                if current_metadata:
                    updated_metadata = {
                        **current_metadata,
                        "vector_status": "failed",
                        "vector_error": str(e)
                    }
                    await self.cache_service.save_file_metadata(file_id, updated_metadata)
            except Exception as meta_error:
                logger.error(f"æ›´æ–°å¤±è´¥çŠ¶æ€å…ƒæ•°æ®å¤±è´¥: {file_id} - {meta_error}")
            
            logger.error(f"æ–‡æ¡£å‘é‡åŒ–å¤±è´¥: {file_id} - {e}")
            raise create_service_exception(
                ErrorCode.INTERNAL_SERVER_ERROR,
                f"æ–‡æ¡£å‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    async def _get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„embeddingå‘é‡"""
        try:
            # æ£€æŸ¥embedding APIé…ç½®
            if not settings.EMBEDDING_API_BASE or not settings.EMBEDDING_API_KEY:
                logger.warning("Embedding APIé…ç½®ä¸å®Œæ•´ï¼Œä½¿ç”¨æœ¬åœ°fallbackæ–¹æ¡ˆ")
                raise ValueError("Embedding APIé…ç½®ä¸å®Œæ•´")
            
            # ä½¿ç”¨å¤–éƒ¨embedding API
            import httpx
            
            logger.info(f"DocumentServiceä½¿ç”¨embedding API: {settings.EMBEDDING_API_BASE}")
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{settings.EMBEDDING_API_BASE}/embeddings",
                    json={
                        "model": settings.EMBEDDING_MODEL_NAME,
                        "input": text
                    },
                    headers={
                        "Authorization": f"Bearer {settings.EMBEDDING_API_KEY}",
                        "Content-Type": "application/json"
                    },
                    timeout=30
                )
                response.raise_for_status()
                
                data = response.json()
                embedding = data["data"][0]["embedding"]
                logger.info(f"DocumentServiceè·å–embeddingæˆåŠŸ: {len(embedding)}ç»´")
                return embedding
                
        except Exception as e:
            logger.warning(f"DocumentService Embedding APIå¤±è´¥: {e}")
            # ä½¿ç”¨æœ¬åœ°embeddingä½œä¸ºfallback
            from app.services.search_service import SearchService
            search_service = SearchService()
            fallback_embedding = await search_service._get_local_embedding(text)
            logger.info(f"DocumentServiceä½¿ç”¨æœ¬åœ°embeddingç”Ÿæˆå‘é‡: {len(fallback_embedding)}ç»´")
            return fallback_embedding
    
    async def list_files(
        self,
        limit: int = 20,
        offset: int = 0,
        status_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ–‡ä»¶"""
        await self._get_services()
        
        try:
            # è·å–æ‰€æœ‰æ–‡ä»¶keys
            keys = []
            cursor = 0
            while True:
                cursor, new_keys = await self.cache_service.redis.scan(
                    cursor, match="file:*", count=100
                )
                keys.extend(new_keys)
                if cursor == 0:
                    break
            
            files = []
            for key in keys:
                file_data = await self.cache_service.hgetall(key)
                if file_data:
                    # åº”ç”¨çŠ¶æ€è¿‡æ»¤
                    if status_filter and file_data.get("status") != status_filter:
                        continue
                    files.append(file_data)
            
            # æŒ‰ä¸Šä¼ æ—¶é—´æ’åº
            files.sort(key=lambda x: x.get("upload_date", ""), reverse=True)
            
            # åº”ç”¨åˆ†é¡µ
            start = offset
            end = offset + limit
            return files[start:end]
            
        except Exception as e:
            logger.error(f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    async def start_vectorize_task(self, file_id: str, priority: int = 0) -> str:
        """å¯åŠ¨æ–‡æ¡£å‘é‡åŒ–ä»»åŠ¡ - å‚è€ƒmineru-webçš„åå°å¤„ç†æœºåˆ¶"""
        try:
            await self._get_services()
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            file_info = await self.get_file_info(file_id)
            if not file_info:
                raise create_service_exception(
                    ErrorCode.FILE_NOT_FOUND,
                    f"æ–‡ä»¶ä¸å­˜åœ¨: {file_id}"
                )
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²è§£æ - ğŸ”§ ä¿®å¤ï¼šå…¼å®¹ä¸¤ç§çŠ¶æ€å­—æ®µ
            status = file_info.get("status")
            parse_status = file_info.get("parse_status")
            
            # æ£€æŸ¥APIå±‚çŠ¶æ€å­—æ®µæˆ–Serviceå±‚çŠ¶æ€å­—æ®µ
            is_parsed = (status == "parsed") or (parse_status == "completed")
            
            if not is_parsed:
                current_status = status or parse_status or "unknown"
                raise create_service_exception(
                    ErrorCode.INVALID_REQUEST,
                    f"æ–‡ä»¶å°šæœªè§£æå®Œæˆï¼Œå½“å‰çŠ¶æ€: {current_status}"
                )
            
            # ç”Ÿæˆä»»åŠ¡ID
            task_id = f"vectorize_{file_id}_{uuid.uuid4().hex[:8]}"
            current_time = datetime.utcnow().isoformat()
            
            # å‡†å¤‡ä»»åŠ¡æ•°æ®
            task_data = {
                "task_id": task_id,
                "task_type": "vectorize",
                "file_id": file_id,
                "filename": file_info.get("filename"),
                "status": "pending",
                "created_at": current_time,
                "priority": priority,
                "metadata": {
                    "file_size": file_info.get("file_size"),
                    "content_type": file_info.get("content_type"),
                    "parse_result": file_info.get("parse_result")
                }
            }
            
            # ä¿å­˜ä»»åŠ¡ä¿¡æ¯
            await self.cache_service.set_task_info(task_id, task_data)
            
            # æ ¹æ®ä¼˜å…ˆçº§æ·»åŠ åˆ°ä¸åŒé˜Ÿåˆ—
            if priority > 0:
                await self.cache_service.add_priority_task("document_vectorize", task_data, priority)
            else:
                await self.cache_service.add_to_queue("document_vectorize", task_data)
            
            # æ›´æ–°æ–‡ä»¶çŠ¶æ€
            await self.cache_service.hset_field(f"file:{file_id}", "vectorize_status", "pending")
            await self.cache_service.hset_field(f"file:{file_id}", "vectorize_task_id", task_id)
            
            logger.info(f"å‘é‡åŒ–ä»»åŠ¡å·²åˆ›å»º: {task_id} - æ–‡ä»¶: {file_id}")
            return task_id
            
        except Exception as e:
            logger.error(f"åˆ›å»ºå‘é‡åŒ–ä»»åŠ¡å¤±è´¥: {file_id} - {e}")
            if hasattr(e, 'code'):
                raise e
            else:
                raise create_service_exception(
                    ErrorCode.TASK_CREATION_FAILED,
                    f"åˆ›å»ºå‘é‡åŒ–ä»»åŠ¡å¤±è´¥: {str(e)}"
                )
    
    async def batch_process_files(self, file_ids: List[str], operations: List[str], priority: int = 0) -> Dict[str, List[str]]:
        """æ‰¹é‡å¤„ç†æ–‡ä»¶ - ç±»ä¼¼mineru-webçš„æ‰¹é‡æ“ä½œ"""
        try:
            results = {
                "parse_tasks": [],
                "vectorize_tasks": [],
                "failed_operations": []
            }
            
            for file_id in file_ids:
                try:
                    if "parse" in operations:
                        task_id = await self.start_parse_task(file_id, priority)
                        results["parse_tasks"].append(task_id)
                    
                    if "vectorize" in operations:
                        # å¦‚æœåŒæ—¶æœ‰è§£æä»»åŠ¡ï¼Œç­‰è§£æå®Œæˆåå†è¿›è¡Œå‘é‡åŒ–
                        # è¿™é‡Œå¯ä»¥è®¾ç½®ä¾èµ–å…³ç³»æˆ–å»¶è¿Ÿå¤„ç†
                        task_id = await self.start_vectorize_task(file_id, priority)
                        results["vectorize_tasks"].append(task_id)
                        
                except Exception as e:
                    logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {file_id} - {e}")
                    results["failed_operations"].append(f"{file_id}: {str(e)}")
            
            logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ - è§£æä»»åŠ¡: {len(results['parse_tasks'])}, å‘é‡åŒ–ä»»åŠ¡: {len(results['vectorize_tasks'])}")
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            raise create_service_exception(
                ErrorCode.INTERNAL_SERVER_ERROR,
                f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}"
            )
    
    async def get_file_processing_status(self, file_id: str) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶å®Œæ•´å¤„ç†çŠ¶æ€ - ç±»ä¼¼mineru-webçš„çŠ¶æ€ç›‘æ§"""
        try:
            await self._get_services()
            
            # è·å–åŸºæœ¬æ–‡ä»¶ä¿¡æ¯
            file_info = await self.get_file_info(file_id)
            if not file_info:
                return {"exists": False}
            
            # è·å–è§£æçŠ¶æ€
            parse_status = file_info.get("parse_status", "pending")
            parse_task_id = file_info.get("parse_task_id")
            
            # è·å–å‘é‡åŒ–çŠ¶æ€
            vectorize_status = file_info.get("vectorize_status", "pending")
            vectorize_task_id = file_info.get("vectorize_task_id")
            
            # è·å–ä»»åŠ¡è¯¦ç»†ä¿¡æ¯
            parse_task_info = None
            vectorize_task_info = None
            
            if parse_task_id:
                parse_task_info = await self.cache_service.get_task_info(parse_task_id)
            
            if vectorize_task_id:
                vectorize_task_info = await self.cache_service.get_task_info(vectorize_task_id)
            
            # è®¡ç®—æ€»ä½“è¿›åº¦
            progress = 0
            if parse_status == "completed":
                progress += 50
            elif parse_status == "running":
                progress += 25
            
            if vectorize_status == "completed":
                progress += 50
            elif vectorize_status == "running":
                progress += 25
            
            status_data = {
                "exists": True,
                "file_id": file_id,
                "filename": file_info.get("filename"),
                "upload_date": file_info.get("upload_date"),
                "file_size": file_info.get("file_size"),
                "total_progress": progress,
                "parse": {
                    "status": parse_status,
                    "task_id": parse_task_id,
                    "task_info": parse_task_info,
                    "result": file_info.get("parse_result")
                },
                "vectorize": {
                    "status": vectorize_status,
                    "task_id": vectorize_task_id,
                    "task_info": vectorize_task_info,
                    "vector_count": file_info.get("vector_count", 0)
                },
                "last_updated": file_info.get("updated_at", file_info.get("upload_date"))
            }
            
            return status_data
            
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶å¤„ç†çŠ¶æ€å¤±è´¥: {file_id} - {e}")
            raise create_service_exception(
                ErrorCode.INTERNAL_SERVER_ERROR,
                f"è·å–å¤„ç†çŠ¶æ€å¤±è´¥: {str(e)}"
            )
    
    async def get_processing_statistics(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯ - å‚è€ƒmineru-webçš„ç›‘æ§é¢æ¿"""
        try:
            await self._get_services()
            
            # è·å–é˜Ÿåˆ—ç»Ÿè®¡
            parse_stats = await self.cache_service.get_queue_stats("document_parse")
            vectorize_stats = await self.cache_service.get_queue_stats("document_vectorize")
            
            # è·å–æ–‡ä»¶ç»Ÿè®¡
            files = await self.list_files(limit=1000)
            
            # ç»Ÿè®¡å„ç§çŠ¶æ€çš„æ–‡ä»¶
            status_counts = {
                "total_files": len(files),
                "parsed_files": 0,
                "vectorized_files": 0,
                "processing_files": 0,
                "failed_files": 0
            }
            
            file_sizes = []
            processing_times = []
            
            for file in files:
                parse_status = file.get("parse_status", "pending")
                vectorize_status = file.get("vectorize_status", "pending")
                
                if parse_status == "completed":
                    status_counts["parsed_files"] += 1
                
                if vectorize_status == "completed":
                    status_counts["vectorized_files"] += 1
                
                if parse_status == "running" or vectorize_status == "running":
                    status_counts["processing_files"] += 1
                
                if parse_status == "failed" or vectorize_status == "failed":
                    status_counts["failed_files"] += 1
                
                # æ”¶é›†æ–‡ä»¶å¤§å°ç”¨äºç»Ÿè®¡
                if file.get("file_size"):
                    file_sizes.append(file["file_size"])
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            avg_file_size = sum(file_sizes) / len(file_sizes) if file_sizes else 0
            success_rate = (status_counts["parsed_files"] / status_counts["total_files"] * 100) if status_counts["total_files"] > 0 else 0
            
            statistics = {
                "file_statistics": status_counts,
                "queue_statistics": {
                    "parse_queue": parse_stats,
                    "vectorize_queue": vectorize_stats
                },
                "performance_metrics": {
                    "average_file_size_mb": round(avg_file_size / (1024 * 1024), 2),
                    "success_rate_percent": round(success_rate, 1),
                    "total_storage_mb": round(sum(file_sizes) / (1024 * 1024), 2)
                },
                "system_health": {
                    "active_tasks": parse_stats["running_tasks"] + vectorize_stats["running_tasks"],
                    "pending_tasks": parse_stats["pending_tasks"] + vectorize_stats["pending_tasks"],
                    "failed_tasks": parse_stats["failed_tasks"] + vectorize_stats["failed_tasks"]
                },
                "updated_at": datetime.utcnow().isoformat()
            }
            
            return statistics
            
        except Exception as e:
            logger.error(f"è·å–å¤„ç†ç»Ÿè®¡å¤±è´¥: {e}")
            raise create_service_exception(
                ErrorCode.INTERNAL_SERVER_ERROR,
                f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"
            )


# å…¨å±€æ–‡æ¡£æœåŠ¡å®ä¾‹
document_service = DocumentService()


async def get_document_service() -> DocumentService:
    """è·å–æ–‡æ¡£æœåŠ¡å®ä¾‹"""
    return document_service 