"""
æ–‡æ¡£å¤„ç†æœåŠ¡
è´Ÿè´£æ–‡æ¡£çš„ä¸Šä¼ ã€è§£æã€å‘é‡åŒ–å’Œç®¡ç†ï¼Œä½¿ç”¨åˆ†å¸ƒå¼å­˜å‚¨æ¶æ„
"""

import os
import uuid
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import asyncio
from pathlib import Path

from app.core.config import settings
from app.models.responses import ErrorCode
from app.core.exceptions import create_service_exception
from app.services.storage_service import get_minio_service
from app.services.cache_service import get_cache_service
from app.services.vector_service import get_vector_service

# å…ˆå®šä¹‰logger
logger = logging.getLogger("rag-anything")

# RAGAnything ç›¸å…³å¯¼å…¥
try:
    from raganything import RAGAnything
    from raganything.modalprocessors import (
        ImageModalProcessor, 
        TableModalProcessor, 
        EquationModalProcessor, 
        GenericModalProcessor
    )
except ImportError:
    logger.warning("RAGAnything æœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
    RAGAnything = None
    ImageModalProcessor = None
    TableModalProcessor = None
    EquationModalProcessor = None
    GenericModalProcessor = None


class DocumentService:
    """æ–‡æ¡£å¤„ç†æœåŠ¡"""
    
    def __init__(self):
        self.minio_service = None
        self.cache_service = None
        self.vector_service = None
        self.rag_processor = None
    
    async def _get_services(self):
        """è·å–ä¾èµ–æœåŠ¡"""
        if self.minio_service is None:
            self.minio_service = await get_minio_service()
        if self.cache_service is None:
            self.cache_service = await get_cache_service()
        if self.vector_service is None:
            self.vector_service = await get_vector_service()
        
        # åˆå§‹åŒ–RAGå¤„ç†å™¨
        if self.rag_processor is None and RAGAnything is not None:
            try:
                # RAGAnythingéœ€è¦æ¨¡å‹å‡½æ•°ï¼Œä¸æ˜¯APIå‚æ•°
                # æš‚æ—¶ä½¿ç”¨Noneï¼Œç­‰åç»­éœ€è¦æ—¶å†é…ç½®å…·ä½“çš„æ¨¡å‹å‡½æ•°
                self.rag_processor = RAGAnything(
                    llm_model_func=None,  # éœ€è¦æ—¶å†é…ç½®å…·ä½“çš„LLMå‡½æ•°
                    embedding_func=None,  # éœ€è¦æ—¶å†é…ç½®å…·ä½“çš„åµŒå…¥å‡½æ•°  
                    vision_model_func=None,  # éœ€è¦æ—¶å†é…ç½®å…·ä½“çš„è§†è§‰å‡½æ•°
                    config=None  # ä½¿ç”¨é»˜è®¤é…ç½®
                )
                logger.info("RAGAnything å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"RAGAnything å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                # å¦‚æœåˆå§‹åŒ–å¤±è´¥ï¼Œè®¾ç½®ä¸ºNoneï¼Œä¸å½±å“åŸºæœ¬çš„æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
                self.rag_processor = None
    
    async def upload_file(
        self, 
        file_content: bytes = None,
        filename: str = None, 
        content_type: str = None,
        metadata: Optional[Dict[str, Any]] = None,
        original_name: Optional[str] = None,
        description: Optional[str] = None,
        **kwargs  # æ¥æ”¶é¢å¤–å‚æ•°ï¼Œæé«˜å…¼å®¹æ€§
    ) -> str:
        """ä¸Šä¼ æ–‡ä»¶åˆ°MinIO"""
        # ğŸ’¡ æ·»åŠ æ–¹æ³•å…¥å£æ—¥å¿— - ç«‹å³è®°å½•æ‰€æœ‰ä¼ å…¥å‚æ•°
        logger.info(f"ğŸ” upload_file æ–¹æ³•è¢«è°ƒç”¨ï¼")
        logger.info(f"ğŸ“‹ ä¼ å…¥å‚æ•°è¯¦æƒ…:")
        logger.info(f"  - file_content: {'å­˜åœ¨' if file_content else 'ç¼ºå¤±'} (é•¿åº¦: {len(file_content) if file_content else 0})")
        logger.info(f"  - filename: '{filename}' (ç±»å‹: {type(filename)})")
        logger.info(f"  - content_type: '{content_type}'")
        logger.info(f"  - original_name: '{original_name}'")
        logger.info(f"  - description: '{description}'")
        logger.info(f"  - metadata: {metadata}")
        logger.info(f"  - kwargs: {kwargs}")
        
        # å‚æ•°éªŒè¯å’Œå…¼å®¹æ€§å¤„ç† - å‚è€ƒmineru-webçš„å®ç°
        if not file_content:  # æ£€æŸ¥Noneæˆ–ç©ºå†…å®¹
            # å°è¯•ä»kwargsè·å–
            file_content = kwargs.get('data', kwargs.get('file_data'))
            if not file_content:  # æ£€æŸ¥Noneæˆ–ç©ºå†…å®¹
                raise create_service_exception(
                    ErrorCode.INVALID_REQUEST,
                    "ç¼ºå°‘æ–‡ä»¶å†…å®¹å‚æ•° (file_content)"
                )
        
        if not filename:  # æ£€æŸ¥Noneæˆ–ç©ºå­—ç¬¦ä¸²
            # å°è¯•ä»kwargsè·å–
            filename = kwargs.get('name', kwargs.get('file_name'))
            if not filename:  # æ£€æŸ¥Noneæˆ–ç©ºå­—ç¬¦ä¸²
                raise create_service_exception(
                    ErrorCode.INVALID_REQUEST,
                    "ç¼ºå°‘æ–‡ä»¶åå‚æ•° (filename)"
                )
        
        # å¦‚æœè¿˜æ²¡æœ‰content_typeï¼Œå°è¯•ä»kwargsè·å–
        if content_type is None:
            content_type = kwargs.get('content_type', kwargs.get('mime_type'))
        
        # è°ƒè¯•æ—¥å¿— - è®°å½•å‚æ•°ä¿¡æ¯
        logger.info(f"upload_file è°ƒç”¨å‚æ•°: filename='{filename}', content_type='{content_type}', "
                    f"file_size={len(file_content) if file_content else 0}, "
                    f"original_name='{original_name}', description='{description}', "
                    f"kwargs={list(kwargs.keys()) if kwargs else []}")
        
        await self._get_services()
        
        # ç”Ÿæˆæ–‡ä»¶ID
        file_id = str(uuid.uuid4())
        file_extension = Path(filename).suffix.lower()
        
        # ç”Ÿæˆå¯¹è±¡åï¼ˆåŒ…å«è·¯å¾„ç»“æ„ï¼‰
        upload_date = datetime.now().strftime("%Y/%m/%d")
        object_name = f"documents/{upload_date}/{file_id}{file_extension}"
        
        try:
            # ä¸Šä¼ åˆ°MinIO
            file_url = await self.minio_service.upload_file(
                object_name=object_name,
                file_data=file_content,
                content_type=content_type
            )
            
            # å‡†å¤‡æ–‡ä»¶å…ƒæ•°æ®
            display_name = original_name or filename
            
            # å¤„ç†descriptionå‚æ•°
            final_metadata = metadata or {}
            if description:
                final_metadata = {**final_metadata, "description": description}
            
            file_metadata = {
                "file_id": file_id,
                "filename": filename,
                "original_name": display_name,
                "object_name": object_name,
                "file_url": file_url,
                "file_size": len(file_content),
                "content_type": content_type,
                "file_extension": file_extension,
                "upload_date": datetime.now().isoformat(),
                "status": "uploaded",
                "parse_status": "pending",
                "custom_metadata": final_metadata
            }
            
            # ä¿å­˜å…ƒæ•°æ®åˆ°Redis
            await self.cache_service.save_file_metadata(file_id, file_metadata)
            
            logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename} -> {file_id}")
            return file_id
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {filename} - {e}")
            raise create_service_exception(
                ErrorCode.FILE_UPLOAD_FAILED,
                f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
            )
    
    async def get_file_info(self, file_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        await self._get_services()
        
        # ä»Redisè·å–æ–‡ä»¶å…ƒæ•°æ®
        metadata = await self.cache_service.get_file_metadata(file_id)
        if not metadata:
            return None
        
        # è¡¥å……MinIOä¸­çš„å®æ—¶ä¿¡æ¯
        try:
            object_name = metadata.get("object_name")
            if object_name:
                minio_info = await self.minio_service.get_file_info(object_name)
                if minio_info:
                    metadata.update({
                        "minio_etag": minio_info.get("etag"),
                        "minio_last_modified": minio_info.get("last_modified"),
                        "actual_size": minio_info.get("size")
                    })
        except Exception as e:
            logger.warning(f"è·å–MinIOæ–‡ä»¶ä¿¡æ¯å¤±è´¥: {file_id} - {e}")
        
        return metadata
    
    async def download_file(self, file_id: str) -> Tuple[bytes, Dict[str, Any]]:
        """ä¸‹è½½æ–‡ä»¶"""
        await self._get_services()
        
        # è·å–æ–‡ä»¶å…ƒæ•°æ®
        metadata = await self.get_file_info(file_id)
        if not metadata:
            raise create_service_exception(
                ErrorCode.FILE_NOT_FOUND,
                f"æ–‡ä»¶ä¸å­˜åœ¨: {file_id}"
            )
        
        object_name = metadata.get("object_name")
        if not object_name:
            raise create_service_exception(
                ErrorCode.FILE_NOT_FOUND,
                f"æ–‡ä»¶å¯¹è±¡åä¸å­˜åœ¨: {file_id}"
            )
        
        try:
            # ä»MinIOä¸‹è½½æ–‡ä»¶
            file_data = await self.minio_service.download_file(object_name)
            return file_data, metadata
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {file_id} - {e}")
            raise create_service_exception(
                ErrorCode.FILE_DOWNLOAD_FAILED,
                f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}"
            )
    
    async def delete_file(
        self, 
        file_id: str, 
        delete_parsed_data: bool = True, 
        delete_vector_data: bool = True
    ) -> bool:
        """åˆ é™¤æ–‡ä»¶"""
        await self._get_services()
        
        try:
            # è·å–æ–‡ä»¶å…ƒæ•°æ®
            metadata = await self.get_file_info(file_id)
            if not metadata:
                logger.warning(f"æ–‡ä»¶å…ƒæ•°æ®ä¸å­˜åœ¨: {file_id}")
                return False
            
            success = True
            
            # åˆ é™¤å‘é‡æ•°æ®åº“ä¸­çš„æ•°æ®
            if delete_vector_data:
                try:
                    await self.vector_service.delete_document(file_id)
                except Exception as e:
                    logger.error(f"åˆ é™¤å‘é‡æ•°æ®å¤±è´¥: {file_id} - {e}")
                    success = False
            
            # åˆ é™¤è§£ææ•°æ®
            if delete_parsed_data:
                try:
                    # åˆ é™¤è§£æç»“æœç¼“å­˜
                    await self.cache_service.delete(f"parse_result:{file_id}")
                    await self.cache_service.delete(f"text_chunks:{file_id}")
                except Exception as e:
                    logger.error(f"åˆ é™¤è§£ææ•°æ®å¤±è´¥: {file_id} - {e}")
                    success = False
            
            # åˆ é™¤MinIOä¸­çš„æ–‡ä»¶
            object_name = metadata.get("object_name")
            if object_name:
                try:
                    await self.minio_service.delete_file(object_name)
                except Exception as e:
                    logger.error(f"åˆ é™¤MinIOæ–‡ä»¶å¤±è´¥: {file_id} - {e}")
                    success = False
            
            # åˆ é™¤Redisä¸­çš„å…ƒæ•°æ®
            try:
                await self.cache_service.delete(f"file:{file_id}")
            except Exception as e:
                logger.error(f"åˆ é™¤Rediså…ƒæ•°æ®å¤±è´¥: {file_id} - {e}")
                success = False
            
            if success:
                logger.info(f"æ–‡ä»¶åˆ é™¤æˆåŠŸ: {file_id}")
            else:
                logger.warning(f"æ–‡ä»¶åˆ é™¤éƒ¨åˆ†å¤±è´¥: {file_id}")
            
            return success
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶åˆ é™¤å¤±è´¥: {file_id} - {e}")
            return False
    
    async def start_parse_task(self, file_id: str, priority: int = 0) -> str:
        """å¯åŠ¨æ–‡æ¡£è§£æä»»åŠ¡ - æ”¯æŒä¼˜å…ˆçº§è®¾ç½®"""
        await self._get_services()
        
        try:
            # è·å–ä»»åŠ¡æœåŠ¡
            from app.services.task_service import get_task_service
            task_service = await get_task_service()
            
            # ğŸ”§ ä¿®å¤ï¼šå®šä¹‰è§£æä»»åŠ¡å‡½æ•°ï¼Œç„¶åä¼ é€’ç»™create_task
            async def parse_task_func():
                """å®é™…çš„è§£æä»»åŠ¡å‡½æ•°"""
                try:
                    logger.info(f"å¼€å§‹è§£ææ–‡æ¡£: {file_id}")
                    result = await self.parse_document(file_id)
                    logger.info(f"æ–‡æ¡£è§£æå®Œæˆ: {file_id}")
                    return result
                except Exception as e:
                    logger.error(f"æ–‡æ¡£è§£æä»»åŠ¡å¤±è´¥: {file_id} - {e}")
                    raise
            
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è°ƒç”¨create_taskï¼Œä¼ é€’å‡½æ•°ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°
            task_id = await task_service.create_task(
                task_func=parse_task_func,  # âœ… ä¼ é€’å‡½æ•°ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°
                task_name=f"è§£ææ–‡æ¡£ {file_id}",
                created_by="document_service"
            )
            
            logger.info(f"æ–‡æ¡£è§£æä»»åŠ¡å·²å¯åŠ¨: {task_id} - {file_id} - ä¼˜å…ˆçº§: {priority}")
            return task_id
            
        except Exception as e:
            logger.error(f"å¯åŠ¨æ–‡æ¡£è§£æä»»åŠ¡å¤±è´¥: {file_id} - {e}")
            raise create_service_exception(
                ErrorCode.TASK_CREATION_FAILED,
                f"å¯åŠ¨è§£æä»»åŠ¡å¤±è´¥: {str(e)}"
            )
    
    async def _run_mineru_with_sglang(self, input_file: str, file_id: str, original_filename: str = None) -> Dict[str, Any]:
        """ä½¿ç”¨MinerUå’ŒSGLangæœåŠ¡è§£ææ–‡æ¡£ - ğŸ”§ ä¼˜åŒ–ï¼šè§£æç»“æœç›´æ¥å­˜å‚¨åˆ°MinIO"""
        try:
            import subprocess
            import tempfile
            import shutil
            
            # åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                # ğŸ”§ ä¿®å¤ï¼šä¿æŒåŸå§‹æ–‡ä»¶åè€Œä¸æ˜¯ä½¿ç”¨"input"
                if original_filename:
                    # ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼Œä½†æ¸…ç†ç‰¹æ®Šå­—ç¬¦ä»¥é¿å…è·¯å¾„é—®é¢˜
                    safe_filename = "".join(c for c in original_filename if c.isalnum() or c in '.-_')
                    temp_input = os.path.join(temp_dir, safe_filename)
                else:
                    # å…œåº•æ–¹æ¡ˆï¼šä½¿ç”¨file_idä½œä¸ºæ–‡ä»¶å
                    temp_input = os.path.join(temp_dir, f"{file_id}" + Path(input_file).suffix)
                
                shutil.copy2(input_file, temp_input)
                logger.info(f"ğŸ“„ ä½¿ç”¨æ–‡ä»¶åè¿›è¡Œè§£æ: {os.path.basename(temp_input)}")
                
                # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
                temp_output = os.path.join(temp_dir, "output")
                os.makedirs(temp_output, exist_ok=True)
                
                # æ„å»ºMinerUå‘½ä»¤
                cmd = [
                    "mineru",
                    "-p", temp_input,
                    "-o", temp_output,
                    "-b", "vlm-sglang-client",
                    "-u", settings.SGLANG_API_BASE
                ]
                
                logger.info(f"æ‰§è¡ŒMinerUå‘½ä»¤: {' '.join(cmd)}")
                
                # ğŸ”§ ä¿®å¤ï¼šå¢åŠ è¶…æ—¶æ—¶é—´å¹¶æ·»åŠ æ›´è¯¦ç»†çš„æ—¥å¿—
                logger.info(f"â±ï¸  å¼€å§‹æ‰§è¡ŒMinerUè§£æï¼Œé¢„è®¡éœ€è¦10-15åˆ†é’Ÿ...")
                
                # æ‰§è¡Œå‘½ä»¤ - å¢åŠ è¶…æ—¶åˆ°20åˆ†é’Ÿï¼Œé€‚åº”å¤§æ–‡ä»¶å¤„ç†
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=1200  # ğŸ”§ 20åˆ†é’Ÿè¶…æ—¶ï¼Œé€‚åº”å¤§æ–‡ä»¶
                )
                
                logger.info(f"âœ… MinerUå‘½ä»¤æ‰§è¡Œå®Œæˆï¼Œè¿”å›ç : {result.returncode}")
                
                if result.returncode != 0:
                    logger.error(f"MinerUæ‰§è¡Œå¤±è´¥: {result.stderr}")
                    raise Exception(f"MinerUæ‰§è¡Œå¤±è´¥: {result.stderr}")
                
                # ğŸ”§ ä¼˜åŒ–ï¼šå°†è§£æç»“æœä¸Šä¼ åˆ°MinIOè€Œä¸æ˜¯ä¿å­˜åˆ°æœ¬åœ°
                minio_files = []
                content_blocks = []
                
                logger.info(f"ğŸ“¤ å¼€å§‹å°†è§£æç»“æœä¸Šä¼ åˆ°MinIO...")
                
                if os.path.exists(temp_output):
                    # ç»Ÿè®¡è¦ä¸Šä¼ çš„æ–‡ä»¶
                    total_files = sum([len(files) for _, _, files in os.walk(temp_output)])
                    logger.info(f"ğŸ“Š å‘ç°{total_files}ä¸ªè§£æç»“æœæ–‡ä»¶ï¼Œå¼€å§‹ä¸Šä¼ ...")
                    
                    uploaded_count = 0
                    # æ‰«æä¸´æ—¶è¾“å‡ºç›®å½•å¹¶ä¸Šä¼ æ‰€æœ‰æ–‡ä»¶åˆ°MinIO
                    for root, dirs, files in os.walk(temp_output):
                        for file in files:
                            local_file_path = os.path.join(root, file)
                            
                            # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
                            rel_path = os.path.relpath(local_file_path, temp_output)
                            
                            # MinIOä¸­çš„è·¯å¾„ï¼šparsed/{file_id}/{ç›¸å¯¹è·¯å¾„}
                            minio_path = f"parsed/{file_id}/{rel_path}"
                            
                            try:
                                # è¯»å–æ–‡ä»¶å†…å®¹
                                with open(local_file_path, 'rb') as f:
                                    file_content = f.read()
                                
                                # ä¸Šä¼ åˆ°MinIO
                                content_type = "text/markdown" if file.endswith('.md') else \
                                             "application/json" if file.endswith('.json') else \
                                             "application/octet-stream"
                                
                                await self.minio_service.upload_file(
                                    object_name=minio_path,
                                    file_data=file_content,
                                    content_type=content_type
                                )
                                
                                minio_files.append(minio_path)
                                uploaded_count += 1
                                
                                # æ·»åŠ åˆ°å†…å®¹å—åˆ—è¡¨
                                if file.endswith('.md'):
                                    content_blocks.append({
                                        "type": "markdown",
                                        "minio_path": minio_path,
                                        "local_filename": file,
                                        "size": len(file_content)
                                    })
                                elif file.endswith('.json'):
                                    content_blocks.append({
                                        "type": "json", 
                                        "minio_path": minio_path,
                                        "local_filename": file,
                                        "size": len(file_content)
                                    })
                                
                                logger.info(f"âœ… [{uploaded_count}/{total_files}] å·²ä¸Šä¼ : {minio_path} ({len(file_content)} å­—èŠ‚)")
                                
                            except Exception as e:
                                logger.error(f"âŒ ä¸Šä¼ è§£æç»“æœå¤±è´¥: {local_file_path} -> {minio_path} - {e}")
                                continue
                    
                    if uploaded_count > 0:
                        logger.info(f"ğŸ‰ MinIOä¸Šä¼ å®Œæˆ: {uploaded_count}/{total_files} ä¸ªæ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
                    else:
                        logger.warning(f"âš ï¸  æ²¡æœ‰æ–‡ä»¶è¢«ä¸Šä¼ åˆ°MinIO")
                else:
                    logger.warning(f"âš ï¸  MinerUè¾“å‡ºç›®å½•ä¸å­˜åœ¨: {temp_output}")
                
                # è§£æç»“æœ
                parse_result = {
                    "status": "success",
                    "minio_base_path": f"parsed/{file_id}",
                    "minio_files": minio_files,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "content_blocks": content_blocks,
                    "uploaded_files_count": len(minio_files),
                    "processing_time": "æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—è·å–è¯¦ç»†æ—¶é—´"
                }
                
                logger.info(f"ğŸ‰ MinerUè§£æå®Œæˆ: æ‰¾åˆ°{len(content_blocks)}ä¸ªå†…å®¹å—ï¼Œå·²ä¸Šä¼ {len(minio_files)}ä¸ªæ–‡ä»¶åˆ°MinIO")
                return parse_result
                
        except subprocess.TimeoutExpired as e:
            logger.error(f"â° MinerUè§£æè¶…æ—¶ï¼ˆ20åˆ†é’Ÿï¼‰: {e}")
            return {
                "status": "failed",
                "error": f"è§£æè¶…æ—¶ï¼Œå¤§æ–‡ä»¶å¤„ç†éœ€è¦æ›´é•¿æ—¶é—´: {str(e)}",
                "content_blocks": [],
                "timeout": True
            }
        except Exception as e:
            logger.error(f"âŒ MinerUè§£æå¤±è´¥: {e}")
            import traceback
            logger.error(f"ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return {
                "status": "failed",
                "error": str(e),
                "content_blocks": [],
                "detailed_error": traceback.format_exc()
            }
    
    async def parse_document(self, file_id: str) -> Dict[str, Any]:
        """è§£ææ–‡æ¡£å†…å®¹"""
        await self._get_services()
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        metadata = await self.get_file_info(file_id)
        if not metadata:
            raise create_service_exception(
                ErrorCode.FILE_NOT_FOUND,
                f"æ–‡ä»¶ä¸å­˜åœ¨: {file_id}"
            )
        
        try:
            # ğŸ”§ ä¿®å¤æ–‡ä»¶å¤§å°è®¡ç®—é”™è¯¯
            filename = metadata.get("filename", "æœªçŸ¥æ–‡ä»¶")
            file_size = metadata.get("file_size", 0)
            
            # ç¡®ä¿file_sizeæ˜¯æ•°å­—ç±»å‹
            try:
                file_size_num = int(file_size) if isinstance(file_size, str) else file_size
                file_size_mb = round(file_size_num / (1024*1024), 2) if file_size_num else 0
                estimated_time = file_size_mb * 0.5
            except (ValueError, TypeError):
                file_size_mb = 0
                estimated_time = 5  # é»˜è®¤ä¼°è®¡5åˆ†é’Ÿ
            
            logger.info(f"ğŸš€ å¼€å§‹è§£ææ–‡æ¡£: {filename}")
            logger.info(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {file_size_mb} MB")
            logger.info(f"   ğŸ†” æ–‡ä»¶ID: {file_id}")
            logger.info(f"   â±ï¸  é¢„è®¡å¤„ç†æ—¶é—´: {estimated_time:.1f} åˆ†é’Ÿï¼ˆå¤§æ–‡ä»¶éœ€è¦æ›´é•¿æ—¶é—´ï¼‰")
            
            # æ›´æ–°è§£æçŠ¶æ€
            await self.cache_service.save_file_metadata(
                file_id, 
                {**metadata, "parse_status": "parsing", "parse_started_at": datetime.now().isoformat()}
            )
            
            # ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            logger.info(f"ğŸ“¥ ä»MinIOä¸‹è½½æ–‡ä»¶è¿›è¡Œè§£æ...")
            file_data, _ = await self.download_file(file_id)
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            import tempfile
            with tempfile.NamedTemporaryFile(
                suffix=metadata.get("file_extension", ".pdf"), 
                delete=False
            ) as temp_file:
                temp_file.write(file_data)
                temp_input_path = temp_file.name
            
            try:
                # ğŸ”§ ä¼˜åŒ–ï¼šä¸å†éœ€è¦æœ¬åœ°è¾“å‡ºç›®å½•ï¼Œç›´æ¥ä½¿ç”¨MinIOå­˜å‚¨
                
                # ä½¿ç”¨MinerUè§£æ - ğŸ”§ ä¼ é€’åŸå§‹æ–‡ä»¶å
                original_filename = metadata.get("filename", metadata.get("original_filename"))
                parse_result = await self._run_mineru_with_sglang(temp_input_path, file_id, original_filename)
                
                # æ›´æ–°å…ƒæ•°æ® - ğŸ”§ ä¼˜åŒ–ï¼šä¿å­˜MinIOè·¯å¾„ä¿¡æ¯è€Œä¸æ˜¯æœ¬åœ°è·¯å¾„
                # ğŸ”§ ä¿®å¤ï¼šåŒæ—¶æ›´æ–°statuså’Œparse_statuså­—æ®µä»¥ä¿æŒAPIå…¼å®¹æ€§
                is_success = parse_result["status"] == "success"
                updated_metadata = {
                    **metadata,
                    "status": "parsed" if is_success else "parse_failed",  # APIå±‚æ£€æŸ¥çš„å­—æ®µ
                    "parse_status": "completed" if is_success else "failed",  # Serviceå±‚ä½¿ç”¨çš„å­—æ®µ
                    "parse_result": parse_result,
                    "parsed_at": datetime.now().isoformat(),
                    "minio_base_path": parse_result.get("minio_base_path"),  # MinIOä¸­çš„åŸºç¡€è·¯å¾„
                    "parsed_files_count": len(parse_result.get("content_blocks", [])),
                    "storage_location": "minio"  # æ ‡è®°å­˜å‚¨ä½ç½®
                }
                
                await self.cache_service.save_file_metadata(file_id, updated_metadata)
                
                # ğŸ”§ æ·»åŠ è¯¦ç»†çš„å®Œæˆæ—¥å¿—
                if parse_result.get("status") == "success":
                    uploaded_files = parse_result.get("uploaded_files_count", 0)
                    content_blocks = len(parse_result.get("content_blocks", []))
                    minio_path = parse_result.get("minio_base_path")
                    
                    logger.info(f"ğŸ‰ æ–‡æ¡£è§£æå®Œå…¨æˆåŠŸ: {file_id}")
                    logger.info(f"   ğŸ“ MinIOå­˜å‚¨è·¯å¾„: {minio_path}")
                    logger.info(f"   ğŸ“„ è§£æå‡ºå†…å®¹å—: {content_blocks}ä¸ª")
                    logger.info(f"   ğŸ’¾ ä¸Šä¼ æ–‡ä»¶æ•°é‡: {uploaded_files}ä¸ª")
                    logger.info(f"   âœ… æ‰€æœ‰æ•°æ®å·²å®‰å…¨å­˜å‚¨åˆ°MinIO")
                else:
                    error_msg = parse_result.get("error", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"âŒ æ–‡æ¡£è§£æå¤±è´¥: {file_id}")
                    logger.error(f"   ğŸ” é”™è¯¯ä¿¡æ¯: {error_msg}")
                    if parse_result.get("timeout"):
                        logger.warning(f"   â° å»ºè®®ï¼šå¤§æ–‡ä»¶å¯èƒ½éœ€è¦æ›´é•¿å¤„ç†æ—¶é—´")
                
                return parse_result
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(temp_input_path)
                except:
                    pass
                    
        except Exception as e:
            # æ›´æ–°è§£æçŠ¶æ€ä¸ºå¤±è´¥ - ğŸ”§ ä¿®å¤ï¼šåŒæ—¶æ›´æ–°statuså’Œparse_status
            await self.cache_service.save_file_metadata(
                file_id,
                {
                    **metadata, 
                    "status": "parse_failed",  # APIå±‚æ£€æŸ¥çš„å­—æ®µ
                    "parse_status": "failed",  # Serviceå±‚ä½¿ç”¨çš„å­—æ®µ
                    "parse_error": str(e)
                }
            )
            
            logger.error(f"æ–‡æ¡£è§£æå¤±è´¥: {file_id} - {e}")
            raise create_service_exception(
                ErrorCode.FILE_PARSE_FAILED,
                f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}"
            )
    
    async def extract_text_chunks(self, file_id: str) -> List[Dict[str, Any]]:
        """ä»è§£æç»“æœä¸­æå–æ–‡æœ¬å— - ğŸ”§ å‡çº§ï¼šæ™ºèƒ½è¡¨æ ¼æ„ŸçŸ¥åˆ†å—"""
        await self._get_services()
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        metadata = await self.get_file_info(file_id)
        if not metadata:
            raise create_service_exception(
                ErrorCode.FILE_NOT_FOUND,
                f"æ–‡ä»¶ä¸å­˜åœ¨: {file_id}"
            )
        
        parse_result = metadata.get("parse_result")
        if not parse_result or parse_result.get("status") != "success":
            raise create_service_exception(
                ErrorCode.FILE_PARSE_FAILED,
                f"æ–‡æ¡£æœªè§£ææˆ–è§£æå¤±è´¥: {file_id}"
            )
        
        chunks = []
        content_blocks = parse_result.get("content_blocks", [])
        
        for i, block in enumerate(content_blocks):
            if block.get("type") == "markdown":
                try:
                    # ğŸ”§ ä¼˜åŒ–ï¼šä»MinIOè¯»å–æ–‡ä»¶è€Œä¸æ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
                    minio_path = block.get("minio_path")
                    if minio_path:
                        # ä»MinIOä¸‹è½½æ–‡ä»¶å†…å®¹
                        file_content = await self.minio_service.download_file(minio_path)
                        content = file_content.decode('utf-8')
                        
                        # ğŸš€ æ–°å¢ï¼šæ™ºèƒ½è¡¨æ ¼æ„ŸçŸ¥åˆ†å—ç®—æ³•
                        smart_chunks = self._smart_chunk_content(content, file_id, i, minio_path)
                        chunks.extend(smart_chunks)
                                
                except Exception as e:
                    logger.warning(f"ä»MinIOè¯»å–è§£ææ–‡ä»¶å¤±è´¥: {minio_path} - {e}")
        
        logger.info(f"æ™ºèƒ½åˆ†å—å®Œæˆ: {file_id} - {len(chunks)}ä¸ªè¯­ä¹‰å—")
        return chunks
    
    def _smart_chunk_content(self, content: str, file_id: str, block_index: int, minio_path: str) -> List[Dict[str, Any]]:
        """ğŸš€ æ™ºèƒ½è¡¨æ ¼æ„ŸçŸ¥åˆ†å—ç®—æ³• - ğŸ¯ æ‹›æ ‡ä¹¦ä¸“ç”¨ä¼˜åŒ–"""
        import re
        
        chunks = []
        current_pos = 0
        
        # ğŸ¯ æ‹›æ ‡ä¹¦ä¸“ç”¨é…ç½®
        max_chunk_size = 2000  # å¢å¤§å—å°ºå¯¸ï¼Œé€‚åº”æ‹›æ ‡ä¹¦å¤æ‚å†…å®¹
        min_chunk_size = 150   # æé«˜æœ€å°å—å°ºå¯¸ï¼Œç¡®ä¿ä¿¡æ¯å®Œæ•´
        overlap_size = 200     # å¢å¤§é‡å ï¼Œç¡®ä¿å…³é”®ä¿¡æ¯ä¸ä¸¢å¤±
        
        # 1ï¸âƒ£ æ‹›æ ‡ä¹¦å…³é”®ç»“æ„è¯†åˆ«
        tender_sections = self._identify_tender_sections(content)
        
        # 2ï¸âƒ£ è¡¨æ ¼è¾¹ç•Œæ£€æµ‹ï¼ˆç»§æ‰¿åŸæœ‰é€»è¾‘ï¼‰
        table_start_pattern = r'<table[^>]*>'
        table_end_pattern = r'</table>'
        
        table_ranges = []
        for match in re.finditer(table_start_pattern, content, re.IGNORECASE):
            start_pos = match.start()
            remaining_content = content[start_pos:]
            end_match = re.search(table_end_pattern, remaining_content, re.IGNORECASE)
            
            if end_match:
                end_pos = start_pos + end_match.end()
                table_ranges.append((start_pos, end_pos))
        
        # 3ï¸âƒ£ å…³é”®ä¿¡æ¯åŒºåŸŸæ£€æµ‹
        key_info_ranges = self._detect_key_info_ranges(content)
        
        logger.info(f"ğŸ—ï¸ æ‹›æ ‡ä¹¦ç»“æ„åˆ†æ: {len(tender_sections)}ä¸ªç« èŠ‚, {len(table_ranges)}ä¸ªè¡¨æ ¼, {len(key_info_ranges)}ä¸ªå…³é”®ä¿¡æ¯åŒºåŸŸ")
        
        # 4ï¸âƒ£ æ™ºèƒ½åˆ†å—ç­–ç•¥ï¼ˆæ‹›æ ‡ä¹¦ä¼˜åŒ–ç‰ˆï¼‰
        while current_pos < len(content):
            chunk_start = current_pos
            chunk_end = min(chunk_start + max_chunk_size, len(content))
            
            # 5ï¸âƒ£ ä¼˜å…ˆä¿æŠ¤å…³é”®ä¿¡æ¯å®Œæ•´æ€§
            protected_chunk = None
            chunk_type = "text"
            
            # æ£€æŸ¥æ˜¯å¦ä¸å…³é”®ä¿¡æ¯åŒºåŸŸé‡å 
            for info_range in key_info_ranges:
                info_start, info_end, info_type = info_range
                if (chunk_start < info_end and chunk_end > info_start):
                    # æ‰©å±•åˆ°åŒ…å«å®Œæ•´å…³é”®ä¿¡æ¯
                    if info_end - chunk_start <= max_chunk_size * 1.5:
                        chunk_start = max(0, info_start - 50)  # åŒ…å«å‰æ–‡ä¸Šä¸‹æ–‡
                        chunk_end = min(len(content), info_end + 50)  # åŒ…å«åæ–‡ä¸Šä¸‹æ–‡
                        protected_chunk = info_range
                        chunk_type = f"key_info_{info_type}"
                        logger.info(f"ğŸ”‘ ä¿æŠ¤å…³é”®ä¿¡æ¯åŒºåŸŸ: {info_type} ({info_start}-{info_end})")
                        break
            
            # 6ï¸âƒ£ è¡¨æ ¼å®Œæ•´æ€§ä¿æŠ¤ï¼ˆç»§æ‰¿åŸæœ‰é€»è¾‘ï¼‰
            if not protected_chunk:
                for table_start, table_end in table_ranges:
                    if (chunk_start < table_end and chunk_end > table_start and 
                        not (chunk_start <= table_start and chunk_end >= table_end)):
                        
                        if table_start >= chunk_start and table_start < chunk_end:
                            if table_end - chunk_start <= max_chunk_size * 2:
                                chunk_end = table_end
                                protected_chunk = (table_start, table_end)
                                chunk_type = "table"
                                logger.info(f"ğŸ“Š æ‰©å±•å—ä»¥åŒ…å«å®Œæ•´è¡¨æ ¼: {table_start}-{table_end}")
                            else:
                                chunk_end = table_start
                            break
                        
                        elif table_end > chunk_start and table_end <= chunk_end:
                            chunk_start = table_start
                            chunk_end = table_end
                            protected_chunk = (table_start, table_end)
                            chunk_type = "table"
                            logger.info(f"ğŸ“Š è°ƒæ•´å—ä»¥å¯¹é½è¡¨æ ¼è¾¹ç•Œ: {table_start}-{table_end}")
                            break
            
            # 7ï¸âƒ£ ç« èŠ‚è¾¹ç•Œä¼˜åŒ–
            if not protected_chunk:
                section_boundary = self._find_section_boundary(content, chunk_end, tender_sections)
                if section_boundary:
                    chunk_end = section_boundary
                    chunk_type = "section_aligned"
            
            # 8ï¸âƒ£ è¯­ä¹‰è¾¹ç•Œä¼˜åŒ–ï¼ˆæ‹›æ ‡ä¹¦ä¸“ç”¨ï¼‰
            if not protected_chunk and chunk_end < len(content):
                boundary_chars = [
                    '\n\n', 'ã€‚\n', 'ï¼š\n', ';\n',  # æ®µè½è¾¹ç•Œ
                    'ï¼‰\n', ')\n', 'ã€\n',          # åˆ—è¡¨é¡¹è¾¹ç•Œ
                    'è¦æ±‚ï¼š', 'è§„å®šï¼š', 'è¯´æ˜ï¼š',    # æ‹›æ ‡ä¹¦å¸¸è§åˆ†éš”è¯
                    'ã€‚', 'ï¼', 'ï¼Ÿ'               # å¥å­è¾¹ç•Œ
                ]
                best_boundary = chunk_end
                
                search_end = min(chunk_end + 300, len(content))  # æ‰©å¤§æœç´¢èŒƒå›´
                for boundary in boundary_chars:
                    pos = content.find(boundary, chunk_end, search_end)
                    if pos != -1:
                        best_boundary = pos + len(boundary)
                        break
                
                chunk_end = best_boundary
            
            # 9ï¸âƒ£ åˆ›å»ºå¢å¼ºå‹å—
            chunk_text = content[chunk_start:chunk_end].strip()
            
            if len(chunk_text) >= min_chunk_size:
                # ğŸ¯ æ‹›æ ‡ä¹¦ä¸“ç”¨å†…å®¹å¢å¼º
                enhanced_text = self._enhance_tender_chunk(chunk_text, chunk_type, protected_chunk)
                
                # ğŸ” æå–ç»“æ„åŒ–ä¿¡æ¯
                structured_info = self._extract_structured_info(chunk_text)
                
                chunk_data = {
                    "chunk_id": f"{file_id}_{block_index}_{len(chunks)}",
                    "text": enhanced_text,
                    "chunk_index": len(chunks),
                    "source_minio_path": minio_path,
                    "block_type": chunk_type,
                    "start_pos": chunk_start,
                    "end_pos": chunk_end,
                    "size": len(chunk_text),
                    "tender_info": {
                        "section_type": self._identify_section_type(chunk_text),
                        "has_dates": bool(structured_info.get("dates")),
                        "has_amounts": bool(structured_info.get("amounts")),
                        "has_requirements": bool(structured_info.get("requirements")),
                        "structured_data": structured_info,
                        "importance_score": self._calculate_importance_score(chunk_text, chunk_type)
                    },
                    "protection_info": {
                        "is_protected": protected_chunk is not None,
                        "protection_type": chunk_type,
                        "protected_range": protected_chunk
                    }
                }
                
                chunks.append(chunk_data)
                logger.debug(f"âœ… åˆ›å»ºæ‹›æ ‡ä¹¦{chunk_type}å—: {chunk_start}-{chunk_end} ({len(chunk_text)}å­—ç¬¦)")
            
            # ğŸ”„ ä½ç½®æ›´æ–°ç­–ç•¥
            if protected_chunk or chunk_type.startswith("key_info"):
                # å…³é”®ä¿¡æ¯å—åä¸é‡å 
                current_pos = chunk_end
            else:
                # æ™®é€šå—ä½¿ç”¨é‡å ç­–ç•¥
                current_pos = max(chunk_end - overlap_size, chunk_start + 1)
        
        # ğŸ“Š ç”Ÿæˆåˆ†å—è´¨é‡æŠ¥å‘Š
        self._generate_chunking_report(chunks, file_id)
        
        return chunks
    
    def _extract_table_content(self, html_content: str) -> str:
        """ä»HTMLè¡¨æ ¼ä¸­æå–çº¯æ–‡æœ¬å†…å®¹ï¼Œä¿æŒç»“æ„æ€§"""
        import re
        
        try:
            # ç§»é™¤HTMLæ ‡ç­¾ï¼Œä½†ä¿æŒè¡¨æ ¼ç»“æ„
            content = html_content
            
            # è¡¨æ ¼è¡Œåˆ†éš”
            content = re.sub(r'</tr[^>]*>', '\n', content)
            content = re.sub(r'<tr[^>]*>', '', content)
            
            # è¡¨æ ¼å•å…ƒæ ¼åˆ†éš”  
            content = re.sub(r'</td[^>]*>', ' | ', content)
            content = re.sub(r'<td[^>]*>', '', content)
            
            # ç§»é™¤å…¶ä»–HTMLæ ‡ç­¾
            content = re.sub(r'<[^>]+>', '', content)
            
            # æ¸…ç†ç©ºç™½
            content = re.sub(r'\n\s*\n', '\n', content)
            content = re.sub(r'\s+', ' ', content)
            
            # ç§»é™¤ç©ºè¡Œå’Œå¤šä½™çš„åˆ†éš”ç¬¦
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            filtered_lines = []
            
            for line in lines:
                # ç§»é™¤åªåŒ…å«åˆ†éš”ç¬¦çš„è¡Œ
                if line.replace('|', '').replace(' ', ''):
                    # æ¸…ç†è¿ç»­çš„åˆ†éš”ç¬¦
                    line = re.sub(r'\s*\|\s*\|\s*', ' | ', line)
                    line = re.sub(r'^\s*\|\s*', '', line)
                    line = re.sub(r'\s*\|\s*$', '', line)
                    filtered_lines.append(line)
            
            result = '\n'.join(filtered_lines)
            return result.strip()
            
        except Exception as e:
            logger.warning(f"è¡¨æ ¼å†…å®¹æå–å¤±è´¥: {e}")
            # å…œåº•ï¼šç®€å•ç§»é™¤HTMLæ ‡ç­¾
            return re.sub(r'<[^>]+>', ' ', html_content).strip()
    
    def _identify_tender_sections(self, content: str) -> List[Dict[str, Any]]:
        """ğŸ—ï¸ è¯†åˆ«æ‹›æ ‡ä¹¦æ ‡å‡†ç« èŠ‚ç»“æ„"""
        import re
        
        # æ‹›æ ‡ä¹¦å¸¸è§ç« èŠ‚æ¨¡å¼
        section_patterns = [
            # ä¸€çº§æ ‡é¢˜æ¨¡å¼
            (r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*ç« [^ï¼›ã€‚]*', 'chapter'),
            (r'ç¬¬[0-9]+\s*ç« [^ï¼›ã€‚]*', 'chapter'),
            (r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€[^ï¼›ã€‚]*', 'major_section'),
            (r'[0-9]+ã€[^ï¼›ã€‚]*', 'major_section'),
            
            # å…³é”®ç« èŠ‚è¯†åˆ«
            (r'(æŠ•æ ‡é¡»çŸ¥|æ‹›æ ‡å…¬å‘Š|æŠ€æœ¯è§„èŒƒ|å•†åŠ¡æ¡æ¬¾|åˆåŒæ¡æ¬¾|è¯„æ ‡åŠæ³•|å·¥ç¨‹é‡æ¸…å•)', 'key_chapter'),
            (r'(é¡¹ç›®æ¦‚å†µ|å·¥ç¨‹æ¦‚å†µ|å»ºè®¾è§„æ¨¡|æŠ•æ ‡äººèµ„æ ¼|æŠ•æ ‡æ–‡ä»¶)', 'important_section'),
            (r'(æŠ€æœ¯è¦æ±‚|è´¨é‡æ ‡å‡†|æ–½å·¥æ–¹æ¡ˆ|ææ–™è®¾å¤‡|å·¥æœŸè¦æ±‚)', 'technical_section'),
            (r'(æŠ¥ä»·è¦æ±‚|ä»˜æ¬¾æ¡ä»¶|ä¿è¯é‡‘|å±¥çº¦ä¿è¯|è¿çº¦è´£ä»»)', 'commercial_section'),
            
            # å­ç« èŠ‚æ¨¡å¼
            (r'\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\)[^ï¼›ã€‚]*', 'subsection'),
            (r'\([0-9]+\)[^ï¼›ã€‚]*', 'subsection'),
            (r'[0-9]+\.[0-9]+[^ï¼›ã€‚]*', 'subsection'),
        ]
        
        sections = []
        for pattern, section_type in section_patterns:
            for match in re.finditer(pattern, content, re.MULTILINE):
                sections.append({
                    'start': match.start(),
                    'end': match.end(),
                    'title': match.group().strip(),
                    'type': section_type,
                    'level': self._get_section_level(section_type)
                })
        
        # æŒ‰ä½ç½®æ’åº
        sections.sort(key=lambda x: x['start'])
        logger.info(f"ğŸ“‹ è¯†åˆ«æ‹›æ ‡ä¹¦ç« èŠ‚: {len(sections)}ä¸ª")
        return sections
    
    def _detect_key_info_ranges(self, content: str) -> List[tuple]:
        """ğŸ” æ£€æµ‹å…³é”®ä¿¡æ¯åŒºåŸŸï¼ˆæ—¥æœŸã€é‡‘é¢ã€å·¥æœŸç­‰ï¼‰"""
        import re
        from datetime import datetime
        
        key_ranges = []
        
        # 1ï¸âƒ£ æ—¥æœŸä¿¡æ¯æ£€æµ‹
        date_patterns = [
            r'[0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥',
            r'[0-9]{4}-[0-9]{1,2}-[0-9]{1,2}',
            r'[0-9]{4}\.[0-9]{1,2}\.[0-9]{1,2}',
            r'æˆªæ ‡æ—¶é—´[ï¼š:][^ï¼›ã€‚\n]*',
            r'å¼€æ ‡æ—¶é—´[ï¼š:][^ï¼›ã€‚\n]*',
            r'å·¥æœŸ[ï¼š:]?[^ï¼›ã€‚\n]*å¤©',
            r'äº¤å·¥æ—¥æœŸ[ï¼š:][^ï¼›ã€‚\n]*',
            r'ç«£å·¥æ—¥æœŸ[ï¼š:][^ï¼›ã€‚\n]*'
        ]
        
        for pattern in date_patterns:
            for match in re.finditer(pattern, content):
                start = max(0, match.start() - 30)
                end = min(len(content), match.end() + 30)
                key_ranges.append((start, end, 'date_info'))
        
        # 2ï¸âƒ£ é‡‘é¢ä¿¡æ¯æ£€æµ‹
        amount_patterns = [
            r'[0-9,]+\.?[0-9]*\s*ä¸‡å…ƒ',
            r'[0-9,]+\.?[0-9]*\s*å…ƒ',
            r'é¢„ç®—[ï¼š:]?[^ï¼›ã€‚\n]*å…ƒ',
            r'æŠ•æ ‡é™ä»·[ï¼š:]?[^ï¼›ã€‚\n]*',
            r'ä¿è¯é‡‘[ï¼š:]?[^ï¼›ã€‚\n]*å…ƒ',
            r'äººæ°‘å¸[^ï¼›ã€‚\n]*å…ƒ'
        ]
        
        for pattern in amount_patterns:
            for match in re.finditer(pattern, content):
                start = max(0, match.start() - 50)
                end = min(len(content), match.end() + 50)
                key_ranges.append((start, end, 'amount_info'))
        
        # 3ï¸âƒ£ æŠ€æœ¯è¦æ±‚ä¿¡æ¯
        tech_patterns = [
            r'æŠ€æœ¯æ ‡å‡†[ï¼š:]?[^ï¼›ã€‚\n]{20,}',
            r'è´¨é‡ç­‰çº§[ï¼š:]?[^ï¼›ã€‚\n]*',
            r'æ–½å·¥å·¥è‰º[ï¼š:]?[^ï¼›ã€‚\n]{20,}',
            r'ææ–™è¦æ±‚[ï¼š:]?[^ï¼›ã€‚\n]{20,}',
            r'è®¾å¤‡è§„æ ¼[ï¼š:]?[^ï¼›ã€‚\n]{20,}'
        ]
        
        for pattern in tech_patterns:
            for match in re.finditer(pattern, content):
                start = max(0, match.start() - 30)
                end = min(len(content), match.end() + 100)
                key_ranges.append((start, end, 'tech_requirement'))
        
        # 4ï¸âƒ£ èµ„æ ¼è¦æ±‚ä¿¡æ¯
        qualification_patterns = [
            r'èµ„è´¨è¦æ±‚[ï¼š:]?[^ï¼›ã€‚\n]{20,}',
            r'ä¸šç»©è¦æ±‚[ï¼š:]?[^ï¼›ã€‚\n]{20,}',
            r'äººå‘˜è¦æ±‚[ï¼š:]?[^ï¼›ã€‚\n]{20,}',
            r'æ³¨å†Œèµ„é‡‘[ï¼š:]?[^ï¼›ã€‚\n]*'
        ]
        
        for pattern in qualification_patterns:
            for match in re.finditer(pattern, content):
                start = max(0, match.start() - 30)
                end = min(len(content), match.end() + 100)
                key_ranges.append((start, end, 'qualification'))
        
        # å»é‡å’Œåˆå¹¶é‡å åŒºåŸŸ
        key_ranges = self._merge_overlapping_ranges(key_ranges)
        logger.info(f"ğŸ”‘ æ£€æµ‹å…³é”®ä¿¡æ¯åŒºåŸŸ: {len(key_ranges)}ä¸ª")
        return key_ranges
    
    def _find_section_boundary(self, content: str, position: int, sections: List[Dict]) -> int:
        """ğŸ¯ æŸ¥æ‰¾æœ€ä½³ç« èŠ‚è¾¹ç•Œ"""
        # æŸ¥æ‰¾positionåæœ€è¿‘çš„ç« èŠ‚å¼€å§‹ä½ç½®
        for section in sections:
            if section['start'] > position and section['start'] - position < 500:
                return section['start']
        return None
    
    def _enhance_tender_chunk(self, text: str, chunk_type: str, protected_info: Any) -> str:
        """ğŸ¯ æ‹›æ ‡ä¹¦å†…å®¹å¢å¼ºå¤„ç†"""
        enhanced_text = text
        
        # 1ï¸âƒ£ è¡¨æ ¼å†…å®¹å¢å¼º
        if chunk_type == "table":
            clean_table = self._extract_table_content(text)
            if clean_table:
                enhanced_text = f"ğŸ“Š è¡¨æ ¼å†…å®¹:\n{clean_table}\n\nåŸå§‹æ ¼å¼:\n{text}"
        
        # 2ï¸âƒ£ å…³é”®ä¿¡æ¯å¢å¼º
        elif chunk_type.startswith("key_info"):
            info_type = chunk_type.replace("key_info_", "")
            enhanced_text = f"ğŸ”‘ å…³é”®ä¿¡æ¯ç±»å‹: {info_type}\n\n{text}"
            
            # æ·»åŠ ç»“æ„åŒ–æ ‡æ³¨
            if info_type == "date_info":
                dates = self._extract_dates(text)
                if dates:
                    enhanced_text += f"\n\nğŸ“… æå–çš„æ—¥æœŸä¿¡æ¯: {', '.join(dates)}"
            
            elif info_type == "amount_info":
                amounts = self._extract_amounts(text)
                if amounts:
                    enhanced_text += f"\n\nğŸ’° æå–çš„é‡‘é¢ä¿¡æ¯: {', '.join(amounts)}"
        
        # 3ï¸âƒ£ ç« èŠ‚ç±»å‹å¢å¼º
        section_type = self._identify_section_type(text)
        if section_type != "unknown":
            enhanced_text = f"ğŸ“‹ ç« èŠ‚ç±»å‹: {section_type}\n\n{enhanced_text}"
        
        return enhanced_text
    
    def _extract_structured_info(self, text: str) -> Dict[str, Any]:
        """ğŸ” æå–ç»“æ„åŒ–ä¿¡æ¯"""
        info = {
            "dates": self._extract_dates(text),
            "amounts": self._extract_amounts(text),
            "requirements": self._extract_requirements(text),
            "deadlines": self._extract_deadlines(text),
            "specifications": self._extract_specifications(text)
        }
        return {k: v for k, v in info.items() if v}  # åªè¿”å›éç©ºé¡¹
    
    def _identify_section_type(self, text: str) -> str:
        """ğŸ“‹ è¯†åˆ«ç« èŠ‚ç±»å‹"""
        import re
        
        section_keywords = {
            "project_overview": ["é¡¹ç›®æ¦‚å†µ", "å·¥ç¨‹æ¦‚å†µ", "å»ºè®¾è§„æ¨¡", "é¡¹ç›®æ€§è´¨"],
            "bidding_notice": ["æŠ•æ ‡é¡»çŸ¥", "æŠ•æ ‡è¯´æ˜", "æŠ•æ ‡äººé¡»çŸ¥"],
            "technical_specs": ["æŠ€æœ¯è§„èŒƒ", "æŠ€æœ¯è¦æ±‚", "æ–½å·¥æ ‡å‡†", "è´¨é‡æ ‡å‡†"],
            "commercial_terms": ["å•†åŠ¡æ¡æ¬¾", "æŠ¥ä»·è¦æ±‚", "ä»˜æ¬¾æ¡ä»¶", "åˆåŒæ¡æ¬¾"],
            "qualification": ["èµ„æ ¼è¦æ±‚", "æŠ•æ ‡äººèµ„æ ¼", "èµ„è´¨è¦æ±‚", "ä¸šç»©è¦æ±‚"],
            "schedule": ["å·¥æœŸè¦æ±‚", "è¿›åº¦å®‰æ’", "é‡Œç¨‹ç¢‘", "èŠ‚ç‚¹è®¡åˆ’"],
            "materials": ["ææ–™è¦æ±‚", "è®¾å¤‡è§„æ ¼", "ææ–™æ ‡å‡†"],
            "evaluation": ["è¯„æ ‡åŠæ³•", "è¯„æ ‡æ ‡å‡†", "è¯„æ ‡ç¨‹åº"],
            "contract": ["åˆåŒæ¡æ¬¾", "å±¥çº¦è¦æ±‚", "è¿çº¦è´£ä»»"]
        }
        
        text_lower = text.lower()
        for section_type, keywords in section_keywords.items():
            if any(keyword in text for keyword in keywords):
                return section_type
        
        return "unknown"
    
    def _calculate_importance_score(self, text: str, chunk_type: str) -> float:
        """ğŸ“Š è®¡ç®—å†…å®¹é‡è¦æ€§åˆ†æ•°ï¼ˆ0-1ï¼‰"""
        score = 0.5  # åŸºç¡€åˆ†æ•°
        
        # 1ï¸âƒ£ å—ç±»å‹æƒé‡
        type_weights = {
            "key_info_date_info": 0.9,
            "key_info_amount_info": 0.95,
            "key_info_tech_requirement": 0.85,
            "key_info_qualification": 0.8,
            "table": 0.75,
            "section_aligned": 0.6,
            "text": 0.5
        }
        score = type_weights.get(chunk_type, 0.5)
        
        # 2ï¸âƒ£ å…³é”®è¯æƒé‡è°ƒæ•´
        high_importance_keywords = [
            "æˆªæ ‡æ—¶é—´", "å¼€æ ‡æ—¶é—´", "æŠ•æ ‡æˆªæ­¢", "å·¥æœŸ", "é¢„ç®—", "æŠ•æ ‡é™ä»·",
            "ä¿è¯é‡‘", "èµ„è´¨è¦æ±‚", "æŠ€æœ¯æ ‡å‡†", "è´¨é‡ç­‰çº§", "è¿çº¦è´£ä»»"
        ]
        
        medium_importance_keywords = [
            "æŠ€æœ¯è¦æ±‚", "æ–½å·¥æ–¹æ¡ˆ", "ææ–™è§„æ ¼", "äººå‘˜é…ç½®", "å®‰å…¨è¦æ±‚",
            "ç¯ä¿è¦æ±‚", "éªŒæ”¶æ ‡å‡†", "ä»˜æ¬¾æ–¹å¼"
        ]
        
        keyword_count = sum(1 for keyword in high_importance_keywords if keyword in text)
        score += keyword_count * 0.1
        
        keyword_count = sum(1 for keyword in medium_importance_keywords if keyword in text)
        score += keyword_count * 0.05
        
        # 3ï¸âƒ£ æ–‡æœ¬é•¿åº¦è°ƒæ•´ï¼ˆé€‚ä¸­é•¿åº¦æ›´é‡è¦ï¼‰
        text_length = len(text)
        if 200 <= text_length <= 1000:
            score += 0.1
        elif text_length > 2000:
            score -= 0.05
        
        return min(1.0, score)
    
    def _generate_chunking_report(self, chunks: List[Dict], file_id: str):
        """ğŸ“Š ç”Ÿæˆåˆ†å—è´¨é‡æŠ¥å‘Š"""
        total_chunks = len(chunks)
        table_chunks = sum(1 for c in chunks if c.get('block_type') == 'table')
        key_info_chunks = sum(1 for c in chunks if c.get('block_type', '').startswith('key_info'))
        
        avg_importance = sum(c.get('tender_info', {}).get('importance_score', 0) for c in chunks) / total_chunks if total_chunks > 0 else 0
        
        high_importance_chunks = sum(1 for c in chunks if c.get('tender_info', {}).get('importance_score', 0) > 0.8)
        
        structured_data_chunks = sum(1 for c in chunks if c.get('tender_info', {}).get('structured_data'))
        
        logger.info(f"ğŸ“Š æ‹›æ ‡ä¹¦åˆ†å—è´¨é‡æŠ¥å‘Š - {file_id}:")
        logger.info(f"   ğŸ“„ æ€»å—æ•°: {total_chunks}")
        logger.info(f"   ğŸ“Š è¡¨æ ¼å—: {table_chunks} ({table_chunks/total_chunks*100:.1f}%)")
        logger.info(f"   ğŸ”‘ å…³é”®ä¿¡æ¯å—: {key_info_chunks} ({key_info_chunks/total_chunks*100:.1f}%)")
        logger.info(f"   â­ é«˜é‡è¦æ€§å—: {high_importance_chunks} ({high_importance_chunks/total_chunks*100:.1f}%)")
        logger.info(f"   ğŸ—ï¸ ç»“æ„åŒ–æ•°æ®å—: {structured_data_chunks} ({structured_data_chunks/total_chunks*100:.1f}%)")
        logger.info(f"   ğŸ“ˆ å¹³å‡é‡è¦æ€§åˆ†æ•°: {avg_importance:.3f}")
    
    # ğŸ”§ è¾…åŠ©æ–¹æ³•
    def _get_section_level(self, section_type: str) -> int:
        """è·å–ç« èŠ‚å±‚çº§"""
        level_map = {
            'chapter': 1,
            'key_chapter': 1,
            'major_section': 2,
            'important_section': 2,
            'technical_section': 2,
            'commercial_section': 2,
            'subsection': 3
        }
        return level_map.get(section_type, 3)
    
    def _merge_overlapping_ranges(self, ranges: List[tuple]) -> List[tuple]:
        """åˆå¹¶é‡å çš„èŒƒå›´"""
        if not ranges:
            return []
        
        # æŒ‰å¼€å§‹ä½ç½®æ’åº
        sorted_ranges = sorted(ranges, key=lambda x: x[0])
        merged = [sorted_ranges[0]]
        
        for current in sorted_ranges[1:]:
            last = merged[-1]
            # å¦‚æœé‡å ï¼Œåˆå¹¶
            if current[0] <= last[1]:
                merged[-1] = (last[0], max(last[1], current[1]), last[2])
            else:
                merged.append(current)
        
        return merged
    
    def _extract_dates(self, text: str) -> List[str]:
        """æå–æ—¥æœŸä¿¡æ¯"""
        import re
        dates = []
        
        date_patterns = [
            r'[0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥',
            r'[0-9]{4}-[0-9]{1,2}-[0-9]{1,2}',
            r'[0-9]{4}\.[0-9]{1,2}\.[0-9]{1,2}'
        ]
        
        for pattern in date_patterns:
            dates.extend(re.findall(pattern, text))
        
        return list(set(dates))  # å»é‡
    
    def _extract_amounts(self, text: str) -> List[str]:
        """æå–é‡‘é¢ä¿¡æ¯"""
        import re
        amounts = []
        
        amount_patterns = [
            r'[0-9,]+\.?[0-9]*\s*ä¸‡å…ƒ',
            r'[0-9,]+\.?[0-9]*\s*å…ƒ',
            r'äººæ°‘å¸[^ï¼›ã€‚\n]*å…ƒ'
        ]
        
        for pattern in amount_patterns:
            amounts.extend(re.findall(pattern, text))
        
        return list(set(amounts))
    
    def _extract_requirements(self, text: str) -> List[str]:
        """æå–è¦æ±‚ä¿¡æ¯"""
        import re
        requirements = []
        
        req_patterns = [
            r'[^ï¼›ã€‚]*è¦æ±‚[ï¼š:]?[^ï¼›ã€‚\n]+',
            r'[^ï¼›ã€‚]*æ ‡å‡†[ï¼š:]?[^ï¼›ã€‚\n]+',
            r'[^ï¼›ã€‚]*è§„èŒƒ[ï¼š:]?[^ï¼›ã€‚\n]+'
        ]
        
        for pattern in req_patterns:
            requirements.extend(re.findall(pattern, text))
        
        return requirements[:5]  # é™åˆ¶æ•°é‡
    
    def _extract_deadlines(self, text: str) -> List[str]:
        """æå–æˆªæ­¢æ—¶é—´ä¿¡æ¯"""
        import re
        deadlines = []
        
        deadline_patterns = [
            r'æˆªæ ‡æ—¶é—´[ï¼š:]?[^ï¼›ã€‚\n]*',
            r'æŠ•æ ‡æˆªæ­¢[ï¼š:]?[^ï¼›ã€‚\n]*',
            r'å¼€æ ‡æ—¶é—´[ï¼š:]?[^ï¼›ã€‚\n]*'
        ]
        
        for pattern in deadline_patterns:
            deadlines.extend(re.findall(pattern, text))
        
        return list(set(deadlines))
    
    def _extract_specifications(self, text: str) -> List[str]:
        """æå–è§„æ ¼å‚æ•°ä¿¡æ¯"""
        import re
        specs = []
        
        spec_patterns = [
            r'[^ï¼›ã€‚]*è§„æ ¼[ï¼š:]?[^ï¼›ã€‚\n]+',
            r'[^ï¼›ã€‚]*å‹å·[ï¼š:]?[^ï¼›ã€‚\n]+',
            r'[^ï¼›ã€‚]*å‚æ•°[ï¼š:]?[^ï¼›ã€‚\n]+'
        ]
        
        for pattern in spec_patterns:
            specs.extend(re.findall(pattern, text))
        
        return specs[:3]  # é™åˆ¶æ•°é‡
    
    async def vectorize_document(self, file_id: str) -> Dict[str, Any]:
        """å°†æ–‡æ¡£å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“"""
        await self._get_services()
        
        if not self.rag_processor:
            raise create_service_exception(
                ErrorCode.INTERNAL_SERVER_ERROR,
                "RAGAnything å¤„ç†å™¨æœªåˆå§‹åŒ–"
            )
        
        try:
            # è·å–æ–‡ä»¶å…ƒæ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦å±äºçŸ¥è¯†åº“
            file_metadata = await self.get_file_info(file_id)
            if not file_metadata:
                raise create_service_exception(
                    ErrorCode.FILE_NOT_FOUND,
                    f"æ–‡ä»¶ä¸å­˜åœ¨: {file_id}"
                )
            
            # ç¡®å®šå‘é‡é›†åˆåç§°
            collection_name = None
            kb_id = file_metadata.get("kb_id")
            
            if kb_id:
                # æ–‡ä»¶å±äºçŸ¥è¯†åº“ï¼Œè·å–çŸ¥è¯†åº“çš„é›†åˆåç§°
                from app.services.knowledge_base_service import get_knowledge_base_service
                kb_service = await get_knowledge_base_service()
                knowledge_base = await kb_service.get_knowledge_base(kb_id)
                if knowledge_base:
                    collection_name = knowledge_base.qdrant_config.collection_name
                    logger.info(f"æ–‡ä»¶ {file_id} å±äºçŸ¥è¯†åº“ {kb_id}ï¼Œä½¿ç”¨é›†åˆ: {collection_name}")
                else:
                    logger.warning(f"æ–‡ä»¶ {file_id} å…³è”çš„çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é›†åˆ")
            
            # æå–æ–‡æœ¬å—
            chunks = await self.extract_text_chunks(file_id)
            
            if not chunks:
                raise create_service_exception(
                    ErrorCode.FILE_PARSE_FAILED,
                    f"æ–‡æ¡£æ²¡æœ‰å¯æå–çš„å†…å®¹: {file_id}"
                )
            
            # ç”Ÿæˆå‘é‡
            texts = [chunk["text"] for chunk in chunks]
            
            # ä½¿ç”¨RAGAnythingç”Ÿæˆembeddings
            embeddings = []
            for text in texts:
                # _get_embeddingæ–¹æ³•å†…éƒ¨å·²ç»åŒ…å«äº†fallbacké€»è¾‘
                embedding = await self._get_embedding(text)
                embeddings.append(embedding)
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨çŸ¥è¯†åº“çš„é›†åˆæˆ–é»˜è®¤é›†åˆï¼‰
            point_ids = await self.vector_service.add_document_chunks(
                file_id=file_id,
                chunks=chunks,
                vectors=embeddings,
                collection_name=collection_name  # ğŸ”§ ä½¿ç”¨çŸ¥è¯†åº“çš„é›†åˆåç§°
            )
            
            # æ›´æ–°æ–‡ä»¶å…ƒæ•°æ®
            if file_metadata:
                updated_metadata = {
                    **file_metadata,
                    "vector_status": "completed",
                    "vectorized_at": datetime.now().isoformat(),
                    "chunk_count": len(chunks),
                    "vector_point_ids": point_ids,
                    "vector_collection": collection_name or "rag_documents"  # è®°å½•å‘é‡é›†åˆåç§°
                }
                await self.cache_service.save_file_metadata(file_id, updated_metadata)
            
            result = {
                "file_id": file_id,
                "chunk_count": len(chunks),
                "vector_count": len(embeddings),
                "point_ids": point_ids
            }
            
            logger.info(f"æ–‡æ¡£å‘é‡åŒ–å®Œæˆ: {file_id} - {len(chunks)}ä¸ªå—")
            return result
            
        except Exception as e:
            # æ›´æ–°å‘é‡åŒ–çŠ¶æ€ä¸ºå¤±è´¥
            try:
                current_metadata = await self.get_file_info(file_id)
                if current_metadata:
                    updated_metadata = {
                        **current_metadata,
                        "vector_status": "failed",
                        "vector_error": str(e)
                    }
                    await self.cache_service.save_file_metadata(file_id, updated_metadata)
            except Exception as meta_error:
                logger.error(f"æ›´æ–°å¤±è´¥çŠ¶æ€å…ƒæ•°æ®å¤±è´¥: {file_id} - {meta_error}")
            
            logger.error(f"æ–‡æ¡£å‘é‡åŒ–å¤±è´¥: {file_id} - {e}")
            raise create_service_exception(
                ErrorCode.INTERNAL_SERVER_ERROR,
                f"æ–‡æ¡£å‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    async def _get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„embeddingå‘é‡"""
        try:
            # æ£€æŸ¥embedding APIé…ç½®
            if not settings.EMBEDDING_API_BASE or not settings.EMBEDDING_API_KEY:
                logger.warning("Embedding APIé…ç½®ä¸å®Œæ•´ï¼Œä½¿ç”¨æœ¬åœ°fallbackæ–¹æ¡ˆ")
                raise ValueError("Embedding APIé…ç½®ä¸å®Œæ•´")
            
            # ä½¿ç”¨å¤–éƒ¨embedding API
            import httpx
            
            logger.info(f"DocumentServiceä½¿ç”¨embedding API: {settings.EMBEDDING_API_BASE}")
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{settings.EMBEDDING_API_BASE}/embeddings",
                    json={
                        "model": settings.EMBEDDING_MODEL_NAME,
                        "input": text
                    },
                    headers={
                        "Authorization": f"Bearer {settings.EMBEDDING_API_KEY}",
                        "Content-Type": "application/json"
                    },
                    timeout=30
                )
                response.raise_for_status()
                
                data = response.json()
                embedding = data["data"][0]["embedding"]
                
                logger.info(f"DocumentServiceè·å–embeddingæˆåŠŸ: {len(embedding)}ç»´")
                return embedding
                
        except Exception as e:
            logger.warning(f"DocumentService Embedding APIå¤±è´¥: {e}")
            # ä½¿ç”¨æœ¬åœ°embeddingä½œä¸ºfallback
            from app.services.search_service import SearchService
            search_service = SearchService()
            fallback_embedding = await search_service._get_local_embedding(text)
            logger.info(f"DocumentServiceä½¿ç”¨æœ¬åœ°embeddingç”Ÿæˆå‘é‡: {len(fallback_embedding)}ç»´")
            return fallback_embedding
    
    async def list_files(
        self,
        limit: int = 20,
        offset: int = 0,
        status_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ–‡ä»¶"""
        await self._get_services()
        
        try:
            # è·å–æ‰€æœ‰æ–‡ä»¶keys
            keys = []
            cursor = 0
            while True:
                cursor, new_keys = await self.cache_service.redis.scan(
                    cursor, match="file:*", count=100
                )
                keys.extend(new_keys)
                if cursor == 0:
                    break
            
            files = []
            for key in keys:
                file_data = await self.cache_service.hgetall(key)
                if file_data:
                    # åº”ç”¨çŠ¶æ€è¿‡æ»¤
                    if status_filter and file_data.get("status") != status_filter:
                        continue
                    files.append(file_data)
            
            # æŒ‰ä¸Šä¼ æ—¶é—´æ’åº
            files.sort(key=lambda x: x.get("upload_date", ""), reverse=True)
            
            # åº”ç”¨åˆ†é¡µ
            start = offset
            end = offset + limit
            return files[start:end]
            
        except Exception as e:
            logger.error(f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    async def start_vectorize_task(self, file_id: str, priority: int = 0) -> str:
        """å¯åŠ¨æ–‡æ¡£å‘é‡åŒ–ä»»åŠ¡ - å‚è€ƒmineru-webçš„åå°å¤„ç†æœºåˆ¶"""
        try:
            await self._get_services()
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            file_info = await self.get_file_info(file_id)
            if not file_info:
                raise create_service_exception(
                    ErrorCode.FILE_NOT_FOUND,
                    f"æ–‡ä»¶ä¸å­˜åœ¨: {file_id}"
                )
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²è§£æ - ğŸ”§ ä¿®å¤ï¼šå…¼å®¹ä¸¤ç§çŠ¶æ€å­—æ®µ
            status = file_info.get("status")
            parse_status = file_info.get("parse_status")
            
            # æ£€æŸ¥APIå±‚çŠ¶æ€å­—æ®µæˆ–Serviceå±‚çŠ¶æ€å­—æ®µ
            is_parsed = (status == "parsed") or (parse_status == "completed")
            
            if not is_parsed:
                current_status = status or parse_status or "unknown"
                raise create_service_exception(
                    ErrorCode.INVALID_REQUEST,
                    f"æ–‡ä»¶å°šæœªè§£æå®Œæˆï¼Œå½“å‰çŠ¶æ€: {current_status}"
                )
            
            # ç”Ÿæˆä»»åŠ¡ID
            task_id = f"vectorize_{file_id}_{uuid.uuid4().hex[:8]}"
            current_time = datetime.utcnow().isoformat()
            
            # å‡†å¤‡ä»»åŠ¡æ•°æ®
            task_data = {
                "task_id": task_id,
                "task_type": "vectorize",
                "file_id": file_id,
                "filename": file_info.get("filename"),
                "status": "pending",
                "created_at": current_time,
                "priority": priority,
                "metadata": {
                    "file_size": file_info.get("file_size"),
                    "content_type": file_info.get("content_type"),
                    "parse_result": file_info.get("parse_result")
                }
            }
            
            # ä¿å­˜ä»»åŠ¡ä¿¡æ¯
            await self.cache_service.set_task_info(task_id, task_data)
            
            # æ ¹æ®ä¼˜å…ˆçº§æ·»åŠ åˆ°ä¸åŒé˜Ÿåˆ—
            if priority > 0:
                await self.cache_service.add_priority_task("document_vectorize", task_data, priority)
            else:
                await self.cache_service.add_to_queue("document_vectorize", task_data)
            
            # æ›´æ–°æ–‡ä»¶çŠ¶æ€
            await self.cache_service.hset_field(f"file:{file_id}", "vectorize_status", "pending")
            await self.cache_service.hset_field(f"file:{file_id}", "vectorize_task_id", task_id)
            
            logger.info(f"å‘é‡åŒ–ä»»åŠ¡å·²åˆ›å»º: {task_id} - æ–‡ä»¶: {file_id}")
            return task_id
            
        except Exception as e:
            logger.error(f"åˆ›å»ºå‘é‡åŒ–ä»»åŠ¡å¤±è´¥: {file_id} - {e}")
            if hasattr(e, 'code'):
                raise e
            else:
                raise create_service_exception(
                    ErrorCode.TASK_CREATION_FAILED,
                    f"åˆ›å»ºå‘é‡åŒ–ä»»åŠ¡å¤±è´¥: {str(e)}"
                )
    
    async def batch_process_files(self, file_ids: List[str], operations: List[str], priority: int = 0) -> Dict[str, List[str]]:
        """æ‰¹é‡å¤„ç†æ–‡ä»¶ - ç±»ä¼¼mineru-webçš„æ‰¹é‡æ“ä½œ"""
        try:
            results = {
                "parse_tasks": [],
                "vectorize_tasks": [],
                "failed_operations": []
            }
            
            for file_id in file_ids:
                try:
                    if "parse" in operations:
                        task_id = await self.start_parse_task(file_id, priority)
                        results["parse_tasks"].append(task_id)
                    
                    if "vectorize" in operations:
                        # å¦‚æœåŒæ—¶æœ‰è§£æä»»åŠ¡ï¼Œç­‰è§£æå®Œæˆåå†è¿›è¡Œå‘é‡åŒ–
                        # è¿™é‡Œå¯ä»¥è®¾ç½®ä¾èµ–å…³ç³»æˆ–å»¶è¿Ÿå¤„ç†
                        task_id = await self.start_vectorize_task(file_id, priority)
                        results["vectorize_tasks"].append(task_id)
                        
                except Exception as e:
                    logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {file_id} - {e}")
                    results["failed_operations"].append(f"{file_id}: {str(e)}")
            
            logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ - è§£æä»»åŠ¡: {len(results['parse_tasks'])}, å‘é‡åŒ–ä»»åŠ¡: {len(results['vectorize_tasks'])}")
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            raise create_service_exception(
                ErrorCode.INTERNAL_SERVER_ERROR,
                f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}"
            )
    
    async def get_file_processing_status(self, file_id: str) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶å®Œæ•´å¤„ç†çŠ¶æ€ - ç±»ä¼¼mineru-webçš„çŠ¶æ€ç›‘æ§"""
        try:
            await self._get_services()
            
            # è·å–åŸºæœ¬æ–‡ä»¶ä¿¡æ¯
            file_info = await self.get_file_info(file_id)
            if not file_info:
                return {"exists": False}
            
            # è·å–è§£æçŠ¶æ€
            parse_status = file_info.get("parse_status", "pending")
            parse_task_id = file_info.get("parse_task_id")
            
            # è·å–å‘é‡åŒ–çŠ¶æ€
            vectorize_status = file_info.get("vectorize_status", "pending")
            vectorize_task_id = file_info.get("vectorize_task_id")
            
            # è·å–ä»»åŠ¡è¯¦ç»†ä¿¡æ¯
            parse_task_info = None
            vectorize_task_info = None
            
            if parse_task_id:
                parse_task_info = await self.cache_service.get_task_info(parse_task_id)
            
            if vectorize_task_id:
                vectorize_task_info = await self.cache_service.get_task_info(vectorize_task_id)
            
            # è®¡ç®—æ€»ä½“è¿›åº¦
            progress = 0
            if parse_status == "completed":
                progress += 50
            elif parse_status == "running":
                progress += 25
            
            if vectorize_status == "completed":
                progress += 50
            elif vectorize_status == "running":
                progress += 25
            
            status_data = {
                "exists": True,
                "file_id": file_id,
                "filename": file_info.get("filename"),
                "upload_date": file_info.get("upload_date"),
                "file_size": file_info.get("file_size"),
                "total_progress": progress,
                "parse": {
                    "status": parse_status,
                    "task_id": parse_task_id,
                    "task_info": parse_task_info,
                    "result": file_info.get("parse_result")
                },
                "vectorize": {
                    "status": vectorize_status,
                    "task_id": vectorize_task_id,
                    "task_info": vectorize_task_info,
                    "vector_count": file_info.get("vector_count", 0)
                },
                "last_updated": file_info.get("updated_at", file_info.get("upload_date"))
            }
            
            return status_data
            
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶å¤„ç†çŠ¶æ€å¤±è´¥: {file_id} - {e}")
            raise create_service_exception(
                ErrorCode.INTERNAL_SERVER_ERROR,
                f"è·å–å¤„ç†çŠ¶æ€å¤±è´¥: {str(e)}"
            )
    
    async def get_processing_statistics(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯ - å‚è€ƒmineru-webçš„ç›‘æ§é¢æ¿"""
        try:
            await self._get_services()
            
            # è·å–é˜Ÿåˆ—ç»Ÿè®¡
            parse_stats = await self.cache_service.get_queue_stats("document_parse")
            vectorize_stats = await self.cache_service.get_queue_stats("document_vectorize")
            
            # è·å–æ–‡ä»¶ç»Ÿè®¡
            files = await self.list_files(limit=1000)
            
            # ç»Ÿè®¡å„ç§çŠ¶æ€çš„æ–‡ä»¶
            status_counts = {
                "total_files": len(files),
                "parsed_files": 0,
                "vectorized_files": 0,
                "processing_files": 0,
                "failed_files": 0
            }
            
            file_sizes = []
            processing_times = []
            
            for file in files:
                parse_status = file.get("parse_status", "pending")
                vectorize_status = file.get("vectorize_status", "pending")
                
                if parse_status == "completed":
                    status_counts["parsed_files"] += 1
                
                if vectorize_status == "completed":
                    status_counts["vectorized_files"] += 1
                
                if parse_status == "running" or vectorize_status == "running":
                    status_counts["processing_files"] += 1
                
                if parse_status == "failed" or vectorize_status == "failed":
                    status_counts["failed_files"] += 1
                
                # æ”¶é›†æ–‡ä»¶å¤§å°ç”¨äºç»Ÿè®¡
                if file.get("file_size"):
                    file_sizes.append(file["file_size"])
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            avg_file_size = sum(file_sizes) / len(file_sizes) if file_sizes else 0
            success_rate = (status_counts["parsed_files"] / status_counts["total_files"] * 100) if status_counts["total_files"] > 0 else 0
            
            statistics = {
                "file_statistics": status_counts,
                "queue_statistics": {
                    "parse_queue": parse_stats,
                    "vectorize_queue": vectorize_stats
                },
                "performance_metrics": {
                    "average_file_size_mb": round(avg_file_size / (1024 * 1024), 2),
                    "success_rate_percent": round(success_rate, 1),
                    "total_storage_mb": round(sum(file_sizes) / (1024 * 1024), 2)
                },
                "system_health": {
                    "active_tasks": parse_stats["running_tasks"] + vectorize_stats["running_tasks"],
                    "pending_tasks": parse_stats["pending_tasks"] + vectorize_stats["pending_tasks"],
                    "failed_tasks": parse_stats["failed_tasks"] + vectorize_stats["failed_tasks"]
                },
                "updated_at": datetime.utcnow().isoformat()
            }
            
            return statistics
            
        except Exception as e:
            logger.error(f"è·å–å¤„ç†ç»Ÿè®¡å¤±è´¥: {e}")
            raise create_service_exception(
                ErrorCode.INTERNAL_SERVER_ERROR,
                f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"
            )


# å…¨å±€æ–‡æ¡£æœåŠ¡å®ä¾‹
document_service = DocumentService()


async def get_document_service() -> DocumentService:
    """è·å–æ–‡æ¡£æœåŠ¡å®ä¾‹"""
    return document_service 