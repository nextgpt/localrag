"""
æ£€ç´¢å’Œæœç´¢APIç«¯ç‚¹
å¤„ç†å‘é‡æ£€ç´¢ã€è¯­ä¹‰æ£€ç´¢å’Œé—®ç­”ç”Ÿæˆ
"""

from fastapi import APIRouter, Depends, HTTPException
from typing import Optional, List, Dict, Any
import logging
from pydantic import BaseModel

from app.models.responses import SuccessResponse, PaginatedResponse, PaginationInfo, ErrorCode
from app.models.requests import SearchRequest, SearchType
from app.core.exceptions import create_search_exception
from app.services.search_service import get_search_service, SearchService

router = APIRouter(tags=["æ£€ç´¢æœç´¢"])  # ğŸ”§ ç§»é™¤é‡å¤çš„prefix
logger = logging.getLogger("rag-anything")


# ğŸ¯ æ–°å¢ï¼šæ‹›æ ‡ä¹¦åˆ†æè¯·æ±‚æ¨¡å‹
class TenderAnalysisRequest(BaseModel):
    query: str = "é¡¹ç›®åç§°"
    file_ids: Optional[List[str]] = None
    analysis_type: str = "general"  # general/project_info/technical_specs/commercial_terms/risks
    limit: int = 20
    score_threshold: float = 0.1  # ğŸ”§ é™ä½é»˜è®¤é˜ˆå€¼æé«˜å¬å›ç‡
    collection_name: Optional[str] = None
    
    class Config:
        schema_extra = {
            "example": {
                "query": "é¡¹ç›®åç§°",
                "file_ids": ["your-uploaded-file-id"],  # ä½¿ç”¨å®é™…ä¸Šä¼ çš„æ–‡ä»¶ID
                "analysis_type": "project_info",
                "limit": 20,
                "score_threshold": 0.1
            }
        }


@router.post("/", response_model=PaginatedResponse, summary="ç»Ÿä¸€æ£€ç´¢æ¥å£")
async def search_documents(
    request: SearchRequest,
    search_service: SearchService = Depends(get_search_service)
):
    """
    ç»Ÿä¸€æ£€ç´¢æ¥å£
    
    **åŠŸèƒ½è¯´æ˜:**
    - æ”¯æŒä¸‰ç§æ£€ç´¢ç±»å‹ï¼šå‘é‡æ£€ç´¢ã€è¯­ä¹‰æ£€ç´¢ã€æ··åˆæ£€ç´¢
    - æ”¯æŒåˆ†é¡µæŸ¥è¯¢
    - æ”¯æŒæŒ‰æ–‡ä»¶IDè¿‡æ»¤
    - è¿”å›ç›¸å…³æ€§åˆ†æ•°å’Œå…ƒæ•°æ®
    
    **è¯·æ±‚å‚æ•°:**
    - query: æ£€ç´¢æŸ¥è¯¢ï¼ˆå¿…å¡«ï¼‰
    - search_type: æ£€ç´¢ç±»å‹ï¼ˆvector/semantic/hybridï¼Œé»˜è®¤hybridï¼‰
    - limit: è¿”å›ç»“æœæ•°é‡ï¼ˆ1-100ï¼Œé»˜è®¤10ï¼‰
    - offset: ç»“æœåç§»é‡ï¼ˆé»˜è®¤0ï¼‰
    - file_ids: é™åˆ¶æ£€ç´¢çš„æ–‡ä»¶IDåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
    
    **å“åº”æ•°æ®:**
    - results: æ£€ç´¢ç»“æœåˆ—è¡¨
    - pagination: åˆ†é¡µä¿¡æ¯
    - search_metadata: æ£€ç´¢å…ƒæ•°æ®
    """
    
    try:
        # æ‰§è¡Œæ£€ç´¢ï¼Œè·å–æ›´å¤šç»“æœç”¨äºåˆ†é¡µ
        extended_limit = request.limit + request.offset
        results = await search_service.search(
            query=request.query,
            search_type=request.search_type,
            limit=extended_limit,
            score_threshold=0.1,  # ğŸ”§ å¤§å¹…é™ä½é˜ˆå€¼ç¡®ä¿èƒ½æ‰¾åˆ°ç»“æœ
            file_ids=request.file_ids
        )
        
        # åº”ç”¨åˆ†é¡µé€»è¾‘
        total_count = len(results)
        start_idx = request.offset
        end_idx = request.offset + request.limit
        result_data = results[start_idx:end_idx]
        
        # è®¡ç®—åˆ†é¡µä¿¡æ¯
        pages = (total_count + request.limit - 1) // request.limit
        pagination = PaginationInfo(
            page=(request.offset // request.limit) + 1,
            size=request.limit,
            total=total_count,
            pages=pages
        )
        
        # æ£€ç´¢å…ƒæ•°æ®
        search_metadata = {
            "search_type": request.search_type,
            "query_length": len(request.query),
            "result_count": len(results),
            "file_filter_count": len(request.file_ids) if request.file_ids else 0
        }
        
        logger.info(f"æ£€ç´¢å®Œæˆ: æŸ¥è¯¢='{request.query}', ç±»å‹={request.search_type}, ç»“æœæ•°={len(results)}")
        
        return PaginatedResponse(
            data={
                "results": result_data,
                "search_metadata": search_metadata
            },
            pagination=pagination,
            message=f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ°{total_count}ä¸ªç›¸å…³ç»“æœ"
        )
        
    except Exception as e:
        logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
        if hasattr(e, 'code'):
            raise e
        else:
            raise create_search_exception(
                ErrorCode.SEARCH_FAILED,
                f"æ£€ç´¢å¤±è´¥: {str(e)}"
            )


@router.post("/vector", response_model=SuccessResponse, summary="å‘é‡æ£€ç´¢")
async def vector_search(
    request: SearchRequest,
    search_service: SearchService = Depends(get_search_service)
):
    """
    å‘é‡æ£€ç´¢æ¥å£
    
    **åŠŸèƒ½è¯´æ˜:**
    - åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„å‘é‡æ£€ç´¢
    - ä½¿ç”¨åµŒå…¥æ¨¡å‹è®¡ç®—æŸ¥è¯¢å‘é‡
    - åœ¨å‘é‡æ•°æ®åº“ä¸­æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„å†…å®¹
    - è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°å’Œè·ç¦»ä¿¡æ¯
    
    **é€‚ç”¨åœºæ™¯:**
    - è¯­ä¹‰ç›¸ä¼¼å†…å®¹æŸ¥æ‰¾
    - æ¦‚å¿µåŒ¹é…
    - ä¸»é¢˜ç›¸å…³æ€§æ£€ç´¢
    """
    
    try:
        # å¼ºåˆ¶ä½¿ç”¨å‘é‡æ£€ç´¢
        request.search_type = SearchType.VECTOR
        
        results = await search_service.vector_search(
            query=request.query,
            limit=request.limit,
            score_threshold=request.score_threshold,
            file_ids=request.file_ids
        )
        
        # ç»“æœå·²ç»æ˜¯Dictæ ¼å¼
        result_data = results
        
        return SuccessResponse(
            data={
                "results": result_data,
                "search_type": "vector",
                "total_results": len(results)
            },
            message=f"å‘é‡æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ°{len(results)}ä¸ªç»“æœ"
        )
        
    except Exception as e:
        logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {e}")
        if hasattr(e, 'code'):
            raise e
        else:
            raise create_search_exception(
                ErrorCode.SEARCH_FAILED,
                f"å‘é‡æ£€ç´¢å¤±è´¥: {str(e)}"
            )


@router.post("/semantic", response_model=SuccessResponse, summary="è¯­ä¹‰æ£€ç´¢")
async def semantic_search(
    request: SearchRequest,
    search_service: SearchService = Depends(get_search_service)
):
    """
    è¯­ä¹‰æ£€ç´¢æ¥å£
    
    **åŠŸèƒ½è¯´æ˜:**
    - åŸºäºçŸ¥è¯†å›¾è°±çš„è¯­ä¹‰æ£€ç´¢
    - ç†è§£å®ä½“å…³ç³»å’Œè¯­ä¹‰ç»“æ„
    - è¿›è¡Œæ¨ç†å’ŒçŸ¥è¯†å…³è”
    - è¿”å›è¯­ä¹‰ç›¸å…³æ€§åˆ†æ•°
    
    **é€‚ç”¨åœºæ™¯:**
    - å¤æ‚é—®é¢˜å›ç­”
    - çŸ¥è¯†æ¨ç†
    - å…³ç³»æŸ¥è¯¢
    """
    
    try:
        # å¼ºåˆ¶ä½¿ç”¨è¯­ä¹‰æ£€ç´¢
        request.search_type = SearchType.SEMANTIC
        
        results = await search_service.semantic_search(
            query=request.query,
            limit=request.limit,
            score_threshold=request.score_threshold,
            file_ids=request.file_ids
        )
        
        # ç»“æœå·²ç»æ˜¯Dictæ ¼å¼
        result_data = results
        
        return SuccessResponse(
            data={
                "results": result_data,
                "search_type": "semantic",
                "total_results": len(results)
            },
            message=f"è¯­ä¹‰æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ°{len(results)}ä¸ªç»“æœ"
        )
        
    except Exception as e:
        logger.error(f"è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
        if hasattr(e, 'code'):
            raise e
        else:
            raise create_search_exception(
                ErrorCode.SEARCH_FAILED,
                f"è¯­ä¹‰æ£€ç´¢å¤±è´¥: {str(e)}"
            )


@router.post("/hybrid", response_model=SuccessResponse, summary="æ··åˆæ£€ç´¢")
async def hybrid_search(
    request: SearchRequest,
    vector_weight: float = 0.6,
    semantic_weight: float = 0.4,
    search_service: SearchService = Depends(get_search_service)
):
    """
    æ··åˆæ£€ç´¢æ¥å£
    
    **åŠŸèƒ½è¯´æ˜:**
    - ç»“åˆå‘é‡æ£€ç´¢å’Œè¯­ä¹‰æ£€ç´¢çš„ä¼˜åŠ¿
    - å¯è°ƒæ•´ä¸¤ç§æ£€ç´¢æ–¹æ³•çš„æƒé‡
    - æ™ºèƒ½åˆå¹¶å’Œæ’åºç»“æœ
    - æä¾›æœ€ä½³çš„æ£€ç´¢æ•ˆæœ
    
    **æŸ¥è¯¢å‚æ•°:**
    - vector_weight: å‘é‡æ£€ç´¢æƒé‡ï¼ˆ0-1ï¼Œé»˜è®¤0.6ï¼‰
    - semantic_weight: è¯­ä¹‰æ£€ç´¢æƒé‡ï¼ˆ0-1ï¼Œé»˜è®¤0.4ï¼‰
    
    **é€‚ç”¨åœºæ™¯:**
    - ç»¼åˆæ€§æŸ¥è¯¢
    - å¹³è¡¡ç²¾ç¡®æ€§å’Œå¬å›ç‡
    - å¤æ‚ä¿¡æ¯æ£€ç´¢
    """
    
    # éªŒè¯æƒé‡å‚æ•°
    if vector_weight + semantic_weight != 1.0:
        raise create_search_exception(
            ErrorCode.INVALID_SEARCH_PARAMS,
            f"æƒé‡ä¹‹å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º{vector_weight + semantic_weight}"
        )
    
    try:
        # å¼ºåˆ¶ä½¿ç”¨æ··åˆæ£€ç´¢
        request.search_type = SearchType.HYBRID
        
        results = await search_service.hybrid_search(
            query=request.query,
            limit=request.limit,
            file_ids=request.file_ids,
            vector_weight=vector_weight,
            text_weight=semantic_weight  # ä¿®å¤å‚æ•°åæ˜ å°„
        )
        
        # ç»“æœå·²ç»æ˜¯Dictæ ¼å¼
        result_data = results
        
        return SuccessResponse(
            data={
                "results": result_data,
                "search_type": "hybrid",
                "vector_weight": vector_weight,
                "semantic_weight": semantic_weight,
                "total_results": len(results)
            },
            message=f"æ··åˆæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ°{len(results)}ä¸ªç»“æœ"
        )
        
    except Exception as e:
        logger.error(f"æ··åˆæ£€ç´¢å¤±è´¥: {e}")
        if hasattr(e, 'code'):
            raise e
        else:
            raise create_search_exception(
                ErrorCode.SEARCH_FAILED,
                f"æ··åˆæ£€ç´¢å¤±è´¥: {str(e)}"
            )


@router.post("/answer", response_model=SuccessResponse, summary="é—®ç­”ç”Ÿæˆ")
async def generate_answer(
    request: SearchRequest,
    include_sources: bool = True,
    search_service: SearchService = Depends(get_search_service)
):
    """
    é—®ç­”ç”Ÿæˆæ¥å£
    
    **åŠŸèƒ½è¯´æ˜:**
    - åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆè‡ªç„¶è¯­è¨€ç­”æ¡ˆ
    - ç»“åˆå¤šæ¨¡æ€å†…å®¹ï¼ˆæ–‡æœ¬ã€å›¾åƒã€è¡¨æ ¼ï¼‰
    - æ”¯æŒä¸Šä¸‹æ–‡å¼•ç”¨å’Œæ¥æºæ ‡æ³¨
    - ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†å’Œç”Ÿæˆ
    
    **æŸ¥è¯¢å‚æ•°:**
    - include_sources: æ˜¯å¦åŒ…å«æ¥æºä¿¡æ¯ï¼ˆé»˜è®¤trueï¼‰
    
    **é€‚ç”¨åœºæ™¯:**
    - é—®ç­”ç³»ç»Ÿ
    - æ™ºèƒ½å®¢æœ
    - çŸ¥è¯†æŸ¥è¯¢
    """
    
    try:
        # é¦–å…ˆæ‰§è¡Œæ£€ç´¢
        results = await search_service.search(
            query=request.query,
            search_type=request.search_type,
            limit=request.limit,
            score_threshold=0.1,  # ğŸ”§ å¤§å¹…é™ä½é˜ˆå€¼ç¡®ä¿èƒ½æ‰¾åˆ°ç»“æœ
            file_ids=request.file_ids
        )
        
        if not results:
            return SuccessResponse(
                data={
                    "query": request.query,
                    "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    "context_count": 0,
                    "sources": [] if include_sources else None
                },
                message="æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
            )
        
        # ç”Ÿæˆç­”æ¡ˆ
        answer_data = await search_service.generate_answer(
            query=request.query,
            search_results=results,
            include_sources=include_sources
        )
        
        logger.info(f"é—®ç­”ç”Ÿæˆå®Œæˆ: æŸ¥è¯¢='{request.query}', ä¸Šä¸‹æ–‡æ•°={answer_data['context_count']}")
        
        return SuccessResponse(
            data=answer_data,
            message="ç­”æ¡ˆç”Ÿæˆå®Œæˆ"
        )
        
    except Exception as e:
        logger.error(f"é—®ç­”ç”Ÿæˆå¤±è´¥: {e}")
        if hasattr(e, 'code'):
            raise e
        else:
            raise create_search_exception(
                ErrorCode.SEARCH_FAILED,
                f"é—®ç­”ç”Ÿæˆå¤±è´¥: {str(e)}"
            )


@router.get("/stats", response_model=SuccessResponse, summary="æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯")
async def get_search_stats(
    search_service: SearchService = Depends(get_search_service)
):
    """
    æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯æ¥å£
    
    **åŠŸèƒ½è¯´æ˜:**
    - è·å–ç³»ç»Ÿçš„æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯
    - åŒ…æ‹¬æ–‡æ¡£æ•°é‡ã€ç´¢å¼•çŠ¶æ€ç­‰
    - æä¾›ç³»ç»Ÿæ€§èƒ½å’Œå®¹é‡ä¿¡æ¯
    
    **å“åº”æ•°æ®:**
    - total_documents: æ€»æ–‡æ¡£æ•°
    - indexed_documents: å·²ç´¢å¼•æ–‡æ¡£æ•°
    - total_chunks: æ€»å†…å®¹å—æ•°
    - vector_dimensions: å‘é‡ç»´åº¦
    - search_types: æ”¯æŒçš„æ£€ç´¢ç±»å‹
    """
    
    try:
        stats = await search_service.get_search_statistics()
        
        return SuccessResponse(
            data=stats,
            message="è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯æˆåŠŸ"
        )
        
    except Exception as e:
        logger.error(f"è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        raise create_search_exception(
            ErrorCode.SEARCH_FAILED,
            f"è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"
        ) 

@router.post("/tender", summary="ğŸ¯ æ‹›æ ‡ä¹¦ä¸“ç”¨æœç´¢åˆ†æ")
async def search_tender_documents(
    request: TenderAnalysisRequest,
    search_service = Depends(get_search_service)
) -> Dict[str, Any]:
    """
    ğŸ¯ æ‹›æ ‡ä¹¦ä¸“ç”¨æœç´¢åˆ†æ - 99%ç²¾å‡†åº¦ä¸“ä¸šè§£è¯»
    
    ## åˆ†æç±»å‹è¯´æ˜ï¼š
    - **general**: ç»¼åˆåˆ†æï¼ˆé»˜è®¤ï¼‰
    - **project_info**: é¡¹ç›®æ€§è´¨ã€å·¥æœŸè¦æ±‚ã€èŠ‚ç‚¹é‡Œç¨‹ç¢‘ã€æˆªæ ‡æ—¥æœŸ
    - **technical_specs**: æŠ€æœ¯è¦æ±‚ã€æ–½å·¥æ–¹æ¡ˆã€ææ–™è®¾å¤‡è¦æ±‚
    - **commercial_terms**: æŠ•æ ‡äººè´£ä»»ã€å·¥ä½œèŒƒå›´ã€æŠ¥ä»·è¦æ±‚ã€æŠ•æ ‡ä¹¦ç¼–åˆ¶
    - **risks**: é£é™©è¯†åˆ«ã€é‡éš¾ç‚¹åˆ†æã€çŸ›ç›¾æ£€æµ‹
    
    ## è¿”å›ç»“æ„åŒ–åˆ†æï¼š
    - å…³é”®ä¿¡æ¯æå–ï¼ˆé¡¹ç›®åç§°ã€å·¥æœŸã€é¢„ç®—ç­‰ï¼‰
    - æ—¶é—´çº¿åˆ†æï¼ˆæˆªæ ‡ã€å¼€æ ‡ã€é‡Œç¨‹ç¢‘ï¼‰
    - è´¢åŠ¡ä¿¡æ¯ï¼ˆé¢„ç®—ã€ä¿è¯é‡‘ã€ä»˜æ¬¾æ¡ä»¶ï¼‰
    - æŠ€æœ¯è¦æ±‚ï¼ˆè´¨é‡æ ‡å‡†ã€ææ–™è®¾å¤‡ï¼‰
    - èµ„æ ¼è¦æ±‚ï¼ˆä¼ä¸šèµ„è´¨ã€äººå‘˜é…ç½®ï¼‰
    - é£é™©è¯†åˆ«ï¼ˆæ½œåœ¨é£é™©ã€çŸ›ç›¾æ£€æµ‹ï¼‰
    - ä¸“ä¸šæŠ¥å‘Šï¼ˆæ‰§è¡Œæ‘˜è¦ã€å»ºè®®ã€è¡ŒåŠ¨é¡¹ï¼‰
    - ç½®ä¿¡åº¦è¯„ä¼°ï¼ˆæ•´ä½“ç½®ä¿¡åº¦ã€å®Œæ•´æ€§åˆ†æï¼‰
    """
    try:
        logger.info(f"ğŸ¯ æ‹›æ ‡ä¹¦ä¸“ç”¨æœç´¢: {request.query} - ç±»å‹: {request.analysis_type}")
        
        # éªŒè¯åˆ†æç±»å‹
        valid_analysis_types = ["general", "project_info", "technical_specs", "commercial_terms", "risks"]
        if request.analysis_type not in valid_analysis_types:
            raise HTTPException(
                status_code=400,
                detail=f"æ— æ•ˆçš„åˆ†æç±»å‹ã€‚æ”¯æŒçš„ç±»å‹: {', '.join(valid_analysis_types)}"
            )
        
        # ğŸ”§ éªŒè¯å’Œä¿®å¤collection_nameå‚æ•°
        actual_collection_name = None
        if request.collection_name is not None:
            if isinstance(request.collection_name, str):
                actual_collection_name = request.collection_name
            else:
                logger.warning(f"æ‹›æ ‡ä¹¦æœç´¢APIæ”¶åˆ°éå­—ç¬¦ä¸²collection_name: {type(request.collection_name)}, å€¼: {request.collection_name}")
                actual_collection_name = None
        
        # æ‰§è¡Œæ‹›æ ‡ä¹¦ä¸“ç”¨æœç´¢
        result = await search_service.search_tender_documents(
            query=request.query,
            file_ids=request.file_ids,
            analysis_type=request.analysis_type,
            limit=request.limit,
            score_threshold=request.score_threshold,
            collection_name=actual_collection_name
        )
        
        # æ·»åŠ APIå“åº”å…ƒæ•°æ®
        result["api_metadata"] = {
            "endpoint": "/search/tender",
            "analysis_type_description": {
                "general": "ç»¼åˆåˆ†ææ‰€æœ‰ç±»å‹ä¿¡æ¯",
                "project_info": "é¡¹ç›®æ€§è´¨ã€å·¥æœŸè¦æ±‚ã€èŠ‚ç‚¹é‡Œç¨‹ç¢‘ã€æˆªæ ‡æ—¥æœŸç­‰å…³é”®äº‹é¡¹",
                "technical_specs": "æŠ€æœ¯è¦æ±‚ã€åˆ¶å®šåˆç†çš„æ–½å·¥æ–¹æ¡ˆã€ææ–™å’Œè®¾å¤‡è¦æ±‚",
                "commercial_terms": "æŠ•æ ‡äººè´£ä»»ã€å·¥ä½œèŒƒå›´ã€æŠ¥ä»·è¦æ±‚ã€æŠ•æ ‡ä¹¦ç¼–åˆ¶å†…å®¹",
                "risks": "å·¥ç¨‹é£é™©ã€é‡éš¾ç‚¹ã€é”™è¯¯çŸ›ç›¾æ£€æµ‹"
            }.get(request.analysis_type, "æœªçŸ¥åˆ†æç±»å‹"),
            "precision_target": "99%",
            "specialized_features": [
                "æ™ºèƒ½ç»“æ„è¯†åˆ«",
                "å…³é”®ä¿¡æ¯æå–", 
                "å¤šå±‚æ¬¡æ£€ç´¢",
                "çŸ›ç›¾æ£€æµ‹",
                "é£é™©è¯†åˆ«",
                "ç½®ä¿¡åº¦è¯„ä¼°"
            ]
        }
        
        logger.info(f"âœ… æ‹›æ ‡ä¹¦åˆ†æå®Œæˆ: {result['total_results']}ä¸ªç»“æœ")
        return result
        
    except Exception as e:
        logger.error(f"âŒ æ‹›æ ‡ä¹¦æœç´¢å¤±è´¥: {request.query} - {e}")
        raise HTTPException(status_code=500, detail=f"æ‹›æ ‡ä¹¦æœç´¢å¤±è´¥: {str(e)}")

@router.post("/tender/batch", summary="ğŸ¯ æ‰¹é‡æ‹›æ ‡ä¹¦åˆ†æ")
async def batch_tender_analysis(
    queries: List[str],
    file_ids: Optional[List[str]] = None,
    analysis_type: str = "general",
    limit: int = 10,
    score_threshold: float = 0.1,  # ğŸ”§ é™ä½é»˜è®¤é˜ˆå€¼æé«˜å¬å›ç‡
    collection_name: Optional[str] = None,
    search_service = Depends(get_search_service)
) -> Dict[str, Any]:
    """
    ğŸ¯ æ‰¹é‡æ‹›æ ‡ä¹¦åˆ†æ - ä¸€æ¬¡æ€§åˆ†æå¤šä¸ªæŸ¥è¯¢
    
    é€‚ç”¨åœºæ™¯ï¼š
    - å…¨é¢è§£è¯»ä¸€ä»½æ‹›æ ‡ä¹¦çš„æ‰€æœ‰è¦æ±‚
    - åŒæ—¶æ£€æŸ¥å¤šä¸ªå…³é”®ä¿¡æ¯ç‚¹
    - æ‰¹é‡é£é™©è¯†åˆ«å’ŒçŸ›ç›¾æ£€æµ‹
    """
    try:
        logger.info(f"ğŸ¯ æ‰¹é‡æ‹›æ ‡ä¹¦åˆ†æ: {len(queries)}ä¸ªæŸ¥è¯¢")
        
        results = {}
        for i, query in enumerate(queries):
            try:
                result = await search_service.search_tender_documents(
                    query=query,
                    file_ids=file_ids,
                    analysis_type=analysis_type,
                    limit=limit,
                    score_threshold=score_threshold,
                    collection_name=collection_name
                )
                results[f"query_{i+1}_{query[:20]}"] = result
                
            except Exception as e:
                logger.error(f"æŸ¥è¯¢å¤±è´¥: {query} - {e}")
                results[f"query_{i+1}_{query[:20]}"] = {
                    "error": str(e),
                    "query": query
                }
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        comprehensive_analysis = _generate_comprehensive_analysis(results)
        
        return {
            "batch_analysis": results,
            "comprehensive_analysis": comprehensive_analysis,
            "summary": {
                "total_queries": len(queries),
                "successful_queries": len([r for r in results.values() if "error" not in r]),
                "failed_queries": len([r for r in results.values() if "error" in r])
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")

def _generate_comprehensive_analysis(batch_results: Dict[str, Any]) -> Dict[str, Any]:
    """ç”Ÿæˆæ‰¹é‡åˆ†æçš„ç»¼åˆæŠ¥å‘Š"""
    
    all_risks = []
    all_contradictions = []
    overall_completeness = []
    key_findings = []
    
    for query_key, result in batch_results.items():
        if "error" in result:
            continue
            
        # æ”¶é›†é£é™©
        risks = result.get("structured_analysis", {}).get("risks_and_issues", [])
        all_risks.extend(risks)
        
        # æ”¶é›†çŸ›ç›¾
        contradictions = result.get("structured_analysis", {}).get("contradictions", [])
        all_contradictions.extend(contradictions)
        
        # æ”¶é›†å®Œæ•´æ€§åˆ†æ•°
        completeness = result.get("structured_analysis", {}).get("completeness_analysis", {})
        if completeness.get("completeness_score"):
            overall_completeness.append(completeness["completeness_score"])
        
        # æ”¶é›†å…³é”®å‘ç°
        findings = result.get("tender_report", {}).get("detailed_findings", {})
        key_findings.extend(findings.get("positive_findings", []))
    
    # è®¡ç®—ç»¼åˆæŒ‡æ ‡
    avg_completeness = sum(overall_completeness) / len(overall_completeness) if overall_completeness else 0
    total_risks = len(all_risks)
    total_contradictions = len(all_contradictions)
    
    # ç”Ÿæˆæ•´ä½“é£é™©è¯„ä¼°
    high_risk_count = len([r for r in all_risks if r.get("risk_score", 0) >= 3])
    overall_risk_level = "é«˜" if high_risk_count > 0 else ("ä¸­" if total_risks > 5 else "ä½")
    
    return {
        "overall_completeness": avg_completeness,
        "risk_summary": {
            "total_risks": total_risks,
            "high_risk_count": high_risk_count,
            "overall_risk_level": overall_risk_level,
            "top_risks": sorted(all_risks, key=lambda x: x.get("risk_score", 0), reverse=True)[:5]
        },
        "consistency_check": {
            "total_contradictions": total_contradictions,
            "contradiction_details": all_contradictions[:3]
        },
        "key_achievements": list(set(key_findings))[:10],
        "recommendations": [
            "ğŸ” é‡ç‚¹å…³æ³¨é«˜é£é™©é¡¹ç›®" if high_risk_count > 0 else "âœ… é£é™©æ°´å¹³å¯æ§",
            "ğŸ“ è”ç³»æ‹›æ ‡æ–¹æ¾„æ¸…çŸ›ç›¾" if total_contradictions > 0 else "âœ… ä¿¡æ¯ä¸€è‡´æ€§è‰¯å¥½",
            "ğŸ“‹ è¡¥å……ç¼ºå¤±ä¿¡æ¯" if avg_completeness < 0.8 else "âœ… ä¿¡æ¯å®Œæ•´æ€§è‰¯å¥½"
        ]
    } 